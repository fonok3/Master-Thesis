\chapter{Fazit und Ausblick}

\section{Fazit}

Ziel dieser Arbeit war es, ein Modell zur Unterstützung des Designs von Erklärungen in erklärbaren Systemen zu konzipieren und im Anschluss zu evaluieren. Ein Ergebnis dieser Arbeit ist folglich ein Modell, welches in einen Leitfaden integriert ist. Der Leitfaden enthält darüber hinaus einen Katalog über bestehende und verallgemeinerbare Zusammenhänge zwischen den äußeren Abhängigkeiten für Erklärungen, den Eigenschaften und Einflüssen auf die Softwarequalität. Abschließend werden im Leitfaden Heuristiken für das Design von Erklärungen zusammengefasst.

Der Leitfaden ist auf Basis einer Literaturrecherche entwickelt worden. Ziel dieser war es, externe Abhängigkeiten, Eigenschaften und Bewertungsmethoden von Erklärungen zu identifizieren, welche einen Einfluss auf ausgewählte Qualitätsaspekte haben. Diese Ergebnisse sind in dem im Leitfaden enthaltenen Modell zusammengefasst.

\smallskip

Ein weiteres Resultat dieser Arbeit ist die erfolgreiche Anwendung des vorgestellten Leitfadens in der Wirtschaft. Mithilfe des Leitfadens konnten im Rahmen eines Workshops zusammen mit der Firma \textit{Graphmasters GmbH} aus Hannover Erklärungen in ein Navigationssystem integriert werden. Die Rohanforderungen, Umsetzungsideen und Ansätze zur Evaluation, welche das Ergebnis eines Workshops waren, wurden dazu mithilfe eines Qualitätsmodells in konkrete Anforderungen überführt. Final wurden diese in eine Produktivanwendung integriert und mit über 4~000 \textit{End Usern} der Smartphone-Anwendung evaluiert. Zusammen mit einem anschließenden Quasi-Experiment mit vier Teilnehmern konnte geschlussfolgert werden, dass der Leitfaden bei der Integration von Erklärungen in ein bestehendes System zum einen die Entwicklung unterstützt hat. Zum anderen konnten positive Auswirkungen der Erklärungen im Rahmen der aufgestellten Ziele erreicht werden. Ferner konnte anhand der qualitativen Evaluation Verbesserungspotenzial der Erklärungen für weitere Iterationen aufgedeckt werden.

\smallskip

Zusammenfassend liefert diese Arbeit einen Leitfaden zur Integration von Erklärungen, welcher die Möglichkeit bietet, Erklärungen in ein System zu integrieren. Der Leitfaden hat während seiner Anwendung im Unternehmen bei allen wichtige Schritten der Integration von Erklärungen unterstützt (Anforderungserhebung, Umsetzung und Evaluation). Mit der Verwendung des Leitfadens konnten die Erklärungen erfolgreich zum Erreichen von aufgestellten Qualitätszielen genutzt werden. Damit ist der Leitfaden ein erster Schritt bei der Entwicklung von Artefakten zur Vereinheitlichung der Entwicklung und Evaluation von Erklärungen in Wirtschaft und Wissenschaft \cite{kohl_explainability_2019,lim_2009_assessing,sokol_explainability_2020}.

Final erfüllt der in dieser Arbeit entwickelt Leitfaden folglich die Anforderung, eine Unterstützung bei der Gestaltung von Erklärungen in erklärbaren Systemen zu bieten.

\section{Ausblick}

Mit der Anwendung des entwickelten Leitfadens in der Wirtschaft konnte gezeigt werden, dass dieser das Ziel der Arbeit erfüllt. Die qualitative Evaluation ist zunächst lediglich mit vier Teilnehmern erfolgt, welche bereits verwendbare Rückmeldungen gegeben haben. Eine Ausweitung des Quasi-Experiments würde vermutlich zu weiteren und ggf. statistisch signifikanten Ergebnissen führen. Zudem sollte bei einer erneuten \textit{Case Study} der Zeitraum verlängert werden, um mehr Studiengruppen mit gleichbleibender Nutzerzahl und zusätzliche Langzeitmetriken wie den Nutzerzuwachs zu ermöglichen.

Außerdem kann der einzelne Einsatz des Leitfadens nicht ausreichend belegen, dass der Leitfaden allgemeingültig anwendbar ist. Um eine erfolgreiche Verwendung in der Wirtschaft zu gewährleisten, sollte folglich im Anschluss an diese Arbeit ein strukturierter Technologietransfer, wie zum Beispiel von \citeauthor{4012630} vorgestellt, durchgeführt werden \cite{4012630}. Insbesondere könnte dieser Transfer weitere Verständnisprobleme des Leitfadens aufdecken. Je nach Anwendungsfall sollte außerdem in ein finales Artefakt mit dem Leitfaden eine Einführung in das Thema Erklärbarkeit integriert werden.

Zusätzlich sollte die Vollständigkeit des Modells für Erklärungen evaluiert und durch weitere unabhängige Arbeiten zu dem Thema bestätigt werden.

Ein Thema, welches im entwickelten Leitfaden wenig behandelt wird, sind mögliche negative Einflüsse, durch die Integration von Erklärungen. Es gibt einige Autoren, die unter anderem die Gefahr der Beeinträchtigung der \textit{Usability} von Systemen durch Erklärungen diskutieren \cite{chazette_knowledge_nodate,koo_understanding_2016,kohl_explainability_2019}. Eine Evaluation erfolgt aber in der Regel in Bezug auf positive Einflüsse von Erklärungen. Folglich sollten in zukünftigen Arbeiten negative Einflüsse näher  betrachtet werden.

In diesem Zusammenhang fehlt außerdem ein Katalog über die genauen Einflüsse zwischen den verschiedenen Qualitätsaspekten, welche mit \textit{Explainability} in Verbindung stehen. \citeauthor{chazette_knowledge_nodate} haben dafür bereits eine Grundlage mit einem Katalog der Aspekte, welche beeinflusst werden vorgestellt \cite{chazette_knowledge_nodate}. Auf Basis dessen sollte ein \textit{Explainability Softgoal Interdependence Graph (SIG)} entwickelt werden. Dieser sollte über \textit{Explainability} einen ähnlichen Überblick liefern wie bereits existierende SIGs über andere Qualitätsaspekte \cite[vgl.][]{do2010software, carvalho2020developers}.

Folglich müssen in Zukunft weitere Artefakte entwickelt werden, welche die Möglichkeit bieten, die neue NFR \textit{Explainability} in der Wirtschaft einfach anwendbar zu machen und die Betrachtung in der Wissenschaft zu vereinheitlichen \cite{sokol_explainability_2020}. Außerdem wird ein Prozess benötigt, der die entwickelten Methoden und Artefakte in bestehende Software-Entwicklungszyklen integriert \cite{kohl_explainability_2019, cassens_ambient_2019}.
