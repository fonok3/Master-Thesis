
@inproceedings{chazette_end-users_nodate,
	title    = {Do {End}-{Users} {Want} {Explanations}? {Analyzing} the {Role} of {Explainability} as an {Emerging} {Aspect} of {Non}-{Functional} {Requirements}},
	abstract = {Software systems are getting more and more complex. Their ubiquitous presence makes users more dependent on them and their correctness in many aspects of daily life. Thus, there is a rising need to make software systems and their decisions more comprehensible. This seems to call for more transparency in software-supported decisions. Therefore, transparency is gaining importance as a non-functional requirement. However, the abstract quality aspect of transparency needs to be better understood and related to mechanisms that can foster it. Integrating explanations in software to leverage systems’ opacity has been discussed often. Yet, an important ﬁrst step is to understand user requirements with respect to explainable software behavior: Are users really interested in transparency, and are explanations considered an adequate mechanism to achieve it? We conducted a survey with 107 end-users to assess their opinion on the current status of transparency in software systems, and what they consider main advantages and disadvantages of explanations embedded in software. The overall attitude towards embedded explanations was positive. However, we also identiﬁed potential disadvantages. We assess the relation between explanations and transparency and analyze its possible impact on software quality.},
	language = {en},
	author   = {Chazette, Larissa and Karras, Oliver and Schneider, Kurt},
	keywords = {Decision making, Explanations, Interpretability, Measurement, Non-Functional Requirements, Privacy, Qualitative Evaluation, Software quality, Software systems, Software Transparency, Stakeholders},
	pages    = {11},
	year     = {2019}
}

@article{waa_evaluating_2021,
	title    = {Evaluating {XAI}: {A} comparison of rule-based and example-based explanations},
	volume   = {291},
	issn     = {0004-3702},
	url      = {https://www.sciencedirect.com/science/article/pii/S0004370220301533},
	doi      = {https://doi.org/10.1016/j.artint.2020.103404},
	abstract = {Current developments in Artificial Intelligence (AI) led to a resurgence of Explainable AI (XAI). New methods are being researched to obtain information from AI systems in order to generate explanations for their output. However, there is an overall lack of valid and reliable evaluations of the effects on users' experience of, and behavior in response to explanations. New XAI methods are often based on an intuitive notion what an effective explanation should be. Rule- and example-based contrastive explanations are two exemplary explanation styles. In this study we evaluate the effects of these two explanation styles on system understanding, persuasive power and task performance in the context of decision support in diabetes self-management. Furthermore, we provide three sets of recommendations based on our experience designing this evaluation to help improve future evaluations. Our results show that rule-based explanations have a small positive effect on system understanding, whereas both rule- and example-based explanations seem to persuade users in following the advice even when incorrect. Neither explanation improves task performance compared to no explanation. This can be explained by the fact that both explanation styles only provide details relevant for a single decision, not the underlying rational or causality. These results show the importance of user evaluations in assessing the current assumptions and intuitions on effective explanations.},
	journal  = {Artificial Intelligence},
	author   = {Waa, Jasper van der and Nieuwburg, Elisabeth and Cremers, Anita and Neerincx, Mark},
	year     = {2021},
	keywords = {Artificial Intelligence (AI), Contrastive explanations, Decision support systems, Explainable Artificial Intelligence (XAI), Explanation: Kind, Machine learning, User evaluations},
	pages    = {103404}
}

@inproceedings{sokol_explainability_2020,
	address   = {New York, NY, USA},
	series    = {{FAT}* '20},
	title     = {Explainability {Fact} {Sheets}: {A} {Framework} for {Systematic} {Assessment} of {Explainable} {Approaches}},
	isbn      = {978-1-4503-6936-7},
	url       = {https://doi.org/10.1145/3351095.3372870},
	doi       = {10.1145/3351095.3372870},
	abstract  = {Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author    = {Sokol, Kacper and Flach, Peter},
	year      = {2020},
	note      = {event-place: Barcelona, Spain},
	keywords  = {AI, desiderata, explainability, fact sheet, interpretability, ML, taxonomy, transparency, work sheet},
	pages     = {56--67},
	file      = {Submitted Version:/Users/fonok3/Zotero/storage/SUPGLGHM/Sokol and Flach - 2020 - Explainability Fact Sheets A Framework for System.pdf:application/pdf}
}

@inproceedings{balog_measuring_2020,
	address   = {New York, NY, USA},
	series    = {{SIGIR} '20},
	title     = {Measuring {Recommendation} {Explanation} {Quality}: {The} {Conflicting} {Goals} of {Explanations}},
	isbn      = {978-1-4503-8016-4},
	url       = {https://doi.org/10.1145/3397271.3401032},
	doi       = {10.1145/3397271.3401032},
	abstract  = {Explanations have a large effect on how people respond to recommendations. However, there are many possible intentions a system may have in generating explanations for a given recommendation -from increasing transparency, to enabling a faster decision, to persuading the recipient. As a good explanation for one goal may not be good for others, we address the questions of (1) how to robustly measure if an explanation meets a given goal and (2) how the different goals interact with each other. Specifically, this paper presents a first proposal of how to measure the quality of explanations along seven common goal dimensions catalogued in the literature. We find that the seven goals are not independent, but rather exhibit strong structure. Proposing two novel explanation evaluation designs, we identify challenges in evaluation, and provide more efficient measurement approaches of explanation quality.},
	booktitle = {Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author    = {Balog, Krisztian and Radlinski, Filip},
	year      = {2020},
	note      = {event-place: Virtual Event, China},
	keywords  = {evaluation, explanations, recommendations},
	pages     = {329--338},
	file      = {Full Text:/Users/fonok3/Zotero/storage/5XA46XKX/Balog and Radlinski - 2020 - Measuring Recommendation Explanation Quality The .pdf:application/pdf}
}

@inproceedings{eiband_impact_2019,
	address   = {New York, NY, USA},
	series    = {{CHI} {EA} '19},
	title     = {The {Impact} of {Placebic} {Explanations} on {Trust} in {Intelligent} {Systems}},
	isbn      = {978-1-4503-5971-9},
	url       = {https://doi.org/10.1145/3290607.3312787},
	doi       = {10.1145/3290607.3312787},
	abstract  = {Work in social psychology on interpersonal interaction has demonstrated that people are more likely to comply to a request if they are presented with a justification - even if this justification conveys no information. In the light of the many calls for explaining reasoning of interactive intelligent systems to users, we investigate whether this effect holds true for human-computer interaction. Using a prototype of a nutrition recommender, we conducted a lab study (N=30) between three groups (no explanation, placebic explanation, and real explanation). Our results indicate that placebic explanations for algorithmic decision-making may indeed invoke perceived levels of trust similar to real explanations. We discuss how placebic explanations could be considered in future work.},
	booktitle = {Extended {Abstracts} of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author    = {Eiband, Malin and Buschek, Daniel and Kremer, Alexander and Hussmann, Heinrich},
	year      = {2019},
	note      = {event-place: Glasgow, Scotland Uk},
	keywords  = {explainability, explanations, intelligent systems, transparency, XAI},
	pages     = {1--6},
	file      = {Eiband et al. - 2019 - The Impact of Placebic Explanations on Trust in In.pdf:/Users/fonok3/Zotero/storage/2J5AXHJQ/Eiband et al. - 2019 - The Impact of Placebic Explanations on Trust in In.pdf:application/pdf}
}

@inproceedings{mohseni_quantitative_2021,
	address   = {New York, NY, USA},
	series    = {{IUI} '21},
	title     = {Quantitative {Evaluation} of {Machine} {Learning} {Explanations}: {A} {Human}-{Grounded} {Benchmark}},
	isbn      = {978-1-4503-8017-1},
	url       = {https://doi.org/10.1145/3397481.3450689},
	doi       = {10.1145/3397481.3450689},
	abstract  = {Research in interpretable machine learning proposes different computational and human subject approaches to evaluate model saliency explanations. These approaches measure different qualities of explanations to achieve diverse goals in designing interpretable machine learning systems. In this paper, we propose a benchmark for image and text domains using multi-layer human attention masks aggregated from multiple human annotators. We then present an evaluation study to compare model saliency explanations obtained using Grad-cam and LIME techniques to human understanding and acceptance. We demonstrate our benchmark’s utility for quantitative evaluation of model explanations by comparing it with human subjective ratings and ground-truth single-layer segmentation masks evaluations. Our study results show that our threshold agnostic evaluation method with the human attention baseline is more effective than single-layer object segmentation masks to ground truth. Our experiments also reveal user biases in the subjective rating of model saliency explanations.},
	booktitle = {26th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author    = {Mohseni, Sina and Block, Jeremy E and Ragan, Eric},
	year      = {2021},
	note      = {event-place: College Station, TX, USA},
	keywords  = {data annotation, explanation benchmark, explanation evaluation, Explanation: Evaluation Method, machine learning explanations},
	pages     = {22--31},
	file      = {Mohseni et al. - 2021 - Quantitative Evaluation of Machine Learning Explan.pdf:/Users/fonok3/Zotero/storage/PI8TI5VK/Mohseni et al. - 2021 - Quantitative Evaluation of Machine Learning Explan.pdf:application/pdf}
}

@inproceedings{ehsan_operationalizing_2021,
	address   = {New York, NY, USA},
	series    = {{CHI} {EA} '21},
	title     = {Operationalizing {Human}-{Centered} {Perspectives} in {Explainable} {AI}},
	isbn      = {978-1-4503-8095-9},
	url       = {https://doi.org/10.1145/3411763.3441342},
	doi       = {10.1145/3411763.3441342},
	abstract  = {The realm of Artificial Intelligence (AI)’s impact on our lives is far reaching – with AI systems proliferating high-stakes domains such as healthcare, finance, mobility, law, etc., these systems must be able to explain their decision to diverse end-users comprehensibly. Yet the discourse of Explainable AI (XAI) has been predominantly focused on algorithm-centered approaches, suffering from gaps in meeting user needs and exacerbating issues of algorithmic opacity. To address these issues, researchers have called for human-centered approaches to XAI. There is a need to chart the domain and shape the discourse of XAI with reflective discussions from diverse stakeholders. The goal of this workshop is to examine how human-centered perspectives in XAI can be operationalized at the conceptual, methodological, and technical levels. Encouraging holistic (historical, sociological, and technical) approaches, we put an emphasis on “operationalizing”, aiming to produce actionable frameworks, transferable evaluation methods, concrete design guidelines, and articulate a coordinated research agenda for XAI.},
	booktitle = {Extended {Abstracts} of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author    = {Ehsan, Upol and Wintersberger, Philipp and Liao, Q. Vera and Mara, Martina and Streit, Marc and Wachter, Sandra and Riener, Andreas and Riedl, Mark O.},
	year      = {2021},
	note      = {event-place: Yokohama, Japan},
	keywords  = {Algorithmic Fairness, Artificial Intelligence, Critical Technical Practice, Explainable Artificial Intelligence, Human-centered Computing, Interpretability, Interpretable Machine Learning, Trust in Automation},
	file      = {Full Text:/Users/fonok3/Zotero/storage/EUKX4QSB/Ehsan et al. - 2021 - Operationalizing Human-Centered Perspectives in Ex.pdf:application/pdf}
}

@inproceedings{mucha_interfaces_2021,
	address   = {New York, NY, USA},
	series    = {{CHI} {EA} '21},
	title     = {Interfaces for {Explanations} in {Human}-{AI} {Interaction}: {Proposing} a {Design} {Evaluation} {Approach}},
	isbn      = {978-1-4503-8095-9},
	url       = {https://doi.org/10.1145/3411763.3451759},
	doi       = {10.1145/3411763.3451759},
	abstract  = {Explanations in Human-AI Interaction are communicated to human decision makers through interfaces. Yet, it is not clear what consequences the exact representation of such explanations as part of decision support systems (DSS) and working on machine learning (ML) models has on human decision making. We observe a need for research methods that allow for measuring the effect different eXplainable AI (XAI) interface designs have on people’s decision making. In this paper, we argue for adopting research approaches from decision theory for HCI research on XAI interface design. We outline how we used estimation tasks in human-grounded design research in order to introduce a method and measurement for collecting evidence on XAI interface effects. To this end, we investigated representations of LIME explanations in an estimation task online study as proof-of-concept for our proposal.},
	booktitle = {Extended {Abstracts} of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author    = {Mucha, Henrik and Robert, Sebastian and Breitschwerdt, Ruediger and Fellmann, Michael},
	year      = {2021},
	note      = {event-place: Yokohama, Japan},
	keywords  = {Explainable Artificial Intelligence (XAI), Explanation: Design, Explanatory User Interfaces, Human-AI Interaction},
	file      = {Full Text:/Users/fonok3/Zotero/storage/PIK5UJ8Y/Mucha et al. - 2021 - Interfaces for Explanations in Human-AI Interactio.pdf:application/pdf}
}

@inproceedings{kouki_user_2017,
	address   = {New York, NY, USA},
	series    = {{RecSys} '17},
	title     = {User {Preferences} for {Hybrid} {Explanations}},
	isbn      = {978-1-4503-4652-8},
	url       = {https://doi.org/10.1145/3109859.3109915},
	doi       = {10.1145/3109859.3109915},
	abstract  = {Hybrid recommender systems combine several different sources of information to generate recommendations. These systems demonstrate improved accuracy compared to single-source recommendation strategies. However, hybrid recommendation strategies are inherently more complex than those that use a single source of information, and thus the process of explaining recommendations to users becomes more challenging. In this paper we describe a hybrid recommender system built on a probabilistic programming language, and discuss the benefits and challenges of explaining its recommendations to users. We perform a mixed model statistical analysis of user preferences for explanations in this system. Through an online user survey, we evaluate explanations for hybrid algorithms in a variety of text and visual, graph-based formats, that are either novel designs or derived from existing hybrid recommender systems.},
	booktitle = {Proceedings of the {Eleventh} {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {Association for Computing Machinery},
	author    = {Kouki, Pigi and Schaffer, James and Pujara, Jay and O'Donovan, John and Getoor, Lise},
	year      = {2017},
	note      = {event-place: Como, Italy},
	keywords  = {explanations, hybrid explanations, hybrid recommendations},
	pages     = {84--88},
	file      = {Kouki et al. - 2017 - User Preferences for Hybrid Explanations.pdf:/Users/fonok3/Zotero/storage/MBF869FZ/Kouki et al. - 2017 - User Preferences for Hybrid Explanations.pdf:application/pdf}
}

@inproceedings{abdulrahman_belief-based_2019,
	address   = {New York, NY, USA},
	series    = {{IVA} '19},
	title     = {Belief-{Based} {Agent} {Explanations} to {Encourage} {Behaviour} {Change}},
	isbn      = {978-1-4503-6672-4},
	url       = {https://doi.org/10.1145/3308532.3329444},
	doi       = {10.1145/3308532.3329444},
	abstract  = {Explainable? virtual agents provide insight into the agent's decision-making process, which aims to improve the user's acceptance of the agent's actions or recommendations. However, explainable agents commonly rely on their own knowledge and goals in providing explanations, rather than the beliefs, plans or goals of the user. Little is known about the user perception of such tailored explanations and their impact on their behaviour change. In this paper, we explore the role of belief-based explanation by proposing a user-aware explainable agent by embedding the cognitive agent architecture with a user model and explanation engine to provide a tailored explanation. To make a clear conclusion on the role of explanation in behaviour change intentions, we investigated whether the level of behaviour change intentions is due to building agent-user rapport through the use of empathic language or due to trusting the agent's understanding through providing explanation. Hence, we designed two versions of a virtual advisor agent, empathic and neutral, to reduce study stress among university students and measured students' rapport levels and intentions to change their behaviour. Our results showed that the agent could build a trusted relationship with the user with the help of the explanation regardless of the level of rapport. The results, further, showed that nearly all the recommendations provided by the agent highly significantly increased the intention of the user to change their behavior.},
	booktitle = {Proceedings of the 19th {ACM} {International} {Conference} on {Intelligent} {Virtual} {Agents}},
	publisher = {Association for Computing Machinery},
	author    = {Abdulrahman, Amal and Richards, Deborah and Ranjbartabar, Hedieh and Mascarenhas, Samuel},
	year      = {2019},
	note      = {event-place: Paris, France},
	keywords  = {behaviour change, explainable ai, Explanation: Kind, intelligent virtual agents},
	pages     = {176--178},
	file      = {Abdulrahman et al. - 2019 - Belief-Based Agent Explanations to Encourage Behav.pdf:/Users/fonok3/Zotero/storage/V985K8KC/Abdulrahman et al. - 2019 - Belief-Based Agent Explanations to Encourage Behav.pdf:application/pdf}
}

@inproceedings{tsai_evaluating_2019,
	address   = {New York, NY, USA},
	series    = {{UMAP} '19},
	title     = {Evaluating {Visual} {Explanations} for {Similarity}-{Based} {Recommendations}: {User} {Perception} and {Performance}},
	isbn      = {978-1-4503-6021-0},
	url       = {https://doi.org/10.1145/3320435.3320465},
	doi       = {10.1145/3320435.3320465},
	abstract  = {Recommender system helps users to reduce information overload. In recent years, enhancing explainability in recommender systems has drawn more and more attention in the field of Human-Computer Interaction (HCI). However, it is not clear whether a user-preferred explanation interface can maintain the same level of performance while the users are exploring or comparing the recommendations. In this paper, we introduced a participatory process of designing explanation interfaces with multiple explanatory goals for three similarity-based recommendation models. We investigate the relations of user perception and performance with two user studies. In the first study (N=15), we conducted card-sorting and semi-interview to identify the user preferred interfaces. In the second study (N=18), we carry out a performance-focused evaluation of six explanation interfaces. The result suggests that the user-preferred interface may not guarantee the same level of performance.},
	booktitle = {Proceedings of the 27th {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author    = {Tsai, Chun-Hua and Brusilovsky, Peter},
	year      = {2019},
	note      = {event-place: Larnaca, Cyprus},
	keywords  = {Explanation: Design, Explanation: Evaluation Method, recommendation, similarity-based, visual explanation},
	pages     = {22--30},
	file      = {Tsai and Brusilovsky - 2019 - Evaluating Visual Explanations for Similarity-Base.pdf:/Users/fonok3/Zotero/storage/74BTHNGP/Tsai and Brusilovsky - 2019 - Evaluating Visual Explanations for Similarity-Base.pdf:application/pdf;Tsai and Brusilovsky - 2019 - Evaluating Visual Explanations for Similarity-Base.pdf:/Users/fonok3/Zotero/storage/WZ7SRVYM/Tsai and Brusilovsky - 2019 - Evaluating Visual Explanations for Similarity-Base.pdf:application/pdf}
}

@inproceedings{hernandez-bocanegra_effects_2020,
	address   = {New York, NY, USA},
	series    = {{UMAP} '20 {Adjunct}},
	title     = {Effects of {Argumentative} {Explanation} {Types} on the {Perception} of {Review}-{Based} {Recommendations}},
	isbn      = {978-1-4503-7950-2},
	url       = {https://doi.org/10.1145/3386392.3399302},
	doi       = {10.1145/3386392.3399302},
	abstract  = {Recommender systems have achieved considerable maturity and accuracy in recent years. However, the rationale behind recommendations mostly remains opaque. Providing textual explanations based on user reviews may increase users' perception of transparency and, by that, overall system satisfaction. However, little is known about how these explanations can be effectively and efficiently presented to the user. In the following paper, we present an empirical study conducted in the domain of hotels to investigate the effect of different textual explanation types on, among others, perceived system transparency and trustworthiness, as well as the overall assessment of explanation quality. The explanations presented to participants follow an argument-based design, which we propose to provide a rationale to support a recommendation in a structured way. Our results show that people prefer explanations that include an aggregation using percentages of other users' opinions, over explanations that only include a brief summary of opinions. The results additionally indicate that user characteristics such as social awareness may influence the perception of explanation quality.},
	booktitle = {Adjunct {Publication} of the 28th {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author    = {Hernandez-Bocanegra, Diana C. and Donkers, Tim and Ziegler, Jürgen},
	year      = {2020},
	note      = {event-place: Genoa, Italy},
	keywords  = {Explanation: Evaluation Method, Explanation: Kind, explanations, recommender systems, user study},
	pages     = {219--225},
	file      = {Hernandez-Bocanegra et al. - 2020 - Effects of Argumentative Explanation Types on the .pdf:/Users/fonok3/Zotero/storage/8TAAMDPI/Hernandez-Bocanegra et al. - 2020 - Effects of Argumentative Explanation Types on the .pdf:application/pdf}
}

@inproceedings{brennen_what_2020,
	address   = {New York, NY, USA},
	series    = {{CHI} {EA} '20},
	title     = {What {Do} {People} {Really} {Want} {When} {They} {Say} {They} {Want} Explainable AI? We {Asked} 60 {Stakeholders}.},
	isbn      = {978-1-4503-6819-3},
	url       = {https://doi.org/10.1145/3334480.3383047},
	doi       = {10.1145/3334480.3383047},
	abstract  = {This paper summarizes findings from a qualitative research effort aimed at understanding how various stakeholders characterize the problem of Explainable Artificial Intelligence (Explainable AI or XAI). During a nine-month period, the author conducted 40 interviews and 2 focus groups. An analysis of data gathered led to two significant initial findings: (1) current discourse on Explainable AI is hindered by a lack of consistent terminology; and (2) there are multiple distinct use cases for Explainable AI, including: debugging models, understanding bias, and building trust. These uses cases assume different user personas, will likely require different explanation strategies, and are not evenly addressed by current XAI tools. This stakeholder research supports a broad characterization of the problem of Explainable AI and can provide important context to inform the design of future capabilities.},
	booktitle = {Extended {Abstracts} of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author    = {Brennen, Andrea},
	year      = {2020},
	note      = {event-place: Honolulu, HI, USA},
	keywords  = {data science, explainable AI, interface design, machine learning, UI/UX design, user research},
	pages     = {1--7},
	file      = {Brennen - 2020 - What Do People Really Want When They Say They Want.pdf:/Users/fonok3/Zotero/storage/FYBVL2YF/Brennen - 2020 - What Do People Really Want When They Say They Want.pdf:application/pdf}
}

@inproceedings{stange_effects_2021,
	address   = {New York, NY, USA},
	series    = {{HRI} '21 {Companion}},
	title     = {Effects of {Referring} to {Robot} vs. {User} {Needs} in {Self}-{Explanations} of {Undesirable} {Robot} {Behavior}},
	isbn      = {978-1-4503-8290-8},
	url       = {https://doi.org/10.1145/3434074.3447174},
	doi       = {10.1145/3434074.3447174},
	abstract  = {Autonomous or lively social robots will often exhibit behavior that is surprising to users and calls for explanation. However, it is not clear how such robot behavior should be explained best. Our previous work showed that different types of a robot's self-explanations, citing its actions, intentions, or needs - alone or in causal relations - have different effects on users (Stange \&amp; Kopp, 2020). Further analysis of the data from the cited study implies that explanations in terms of robot needs (e.g. for energy or social contact) did not adequately justify the robot's behavior. In this paper we study the effects of a robot citing the user's needs to explain its behavior. Our study is based on the assumption that users may feel more connected to a robot that aims to recognize and incorporate the users' needs in its decision-making, even when the resulting behavior turns out to be undesirable. Results show that explaining robot behavior with user needs generally did neither lead to higher gains in understanding or desirability of the behaviors, nor did it help to justify them better than explaining it with robot needs. Further, a robot referring to user needs was not perceived as more likable, trustworthy or mindful, nor were users' contact intentions increased. However, an in-depth analysis showed different effects of explanations for different behaviors. We discuss these differences in order to clarify which factors should inform content and form of a robot's behavioral self-explanations.},
	booktitle = {Companion of the 2021 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {Association for Computing Machinery},
	author    = {Stange, Sonja and Kopp, Stefan},
	year      = {2021},
	note      = {event-place: Boulder, CO, USA},
	keywords  = {empirical study, explainability, human-robot interaction},
	pages     = {271--275},
	file      = {Stange and Kopp - 2021 - Effects of Referring to Robot vs. User Needs in Se.pdf:/Users/fonok3/Zotero/storage/KPYLP8WP/Stange and Kopp - 2021 - Effects of Referring to Robot vs. User Needs in Se.pdf:application/pdf;Stange and Kopp - 2021 - Effects of Referring to Robot vs. User Needs in Se.pdf:/Users/fonok3/Zotero/storage/A4Q45T79/Stange and Kopp - 2021 - Effects of Referring to Robot vs. User Needs in Se.pdf:application/pdf}
}

@inproceedings{kunkel_let_2019,
	address   = {New York, NY, USA},
	series    = {{CHI} '19},
	title     = {Let {Me} {Explain}: {Impact} of {Personal} and {Impersonal} {Explanations} on {Trust} in {Recommender} {Systems}},
	isbn      = {978-1-4503-5970-2},
	url       = {https://doi.org/10.1145/3290605.3300717},
	doi       = {10.1145/3290605.3300717},
	abstract  = {Trust in a Recommender System (RS) is crucial for its overall success. However, it remains underexplored whether users trust personal recommendation sources (i.e. other humans) more than impersonal sources (i.e. conventional RS), and, if they do, whether the perceived quality of explanation provided account for the difference. We conducted an empirical study in which we compared these two sources of recommendations and explanations. Human advisors were asked to explain movies they recommended in short texts while the RS created explanations based on item similarity. Our experiment comprised two rounds of recommending. Over both rounds the quality of explanations provided by users was assessed higher than the quality of the system's explanations. Moreover, explanation quality significantly influenced perceived recommendation quality as well as trust in the recommendation source. Consequently, we suggest that RS should provide richer explanations in order to increase their perceived recommendation quality and trustworthiness.},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author    = {Kunkel, Johannes and Donkers, Tim and Michael, Lisa and Barbu, Catalin-Mihai and Ziegler, Jürgen},
	year      = {2019},
	note      = {event-place: Glasgow, Scotland Uk},
	keywords  = {counterfactual analysis, explanations, recommender systems, structural equation modelling, trust, user study},
	pages     = {1--12},
	file      = {Kunkel et al. - 2019 - Let Me Explain Impact of Personal and Impersonal .pdf:/Users/fonok3/Zotero/storage/55I9MSJW/Kunkel et al. - 2019 - Let Me Explain Impact of Personal and Impersonal .pdf:application/pdf}
}

@inproceedings{schaffer_i_2019,
	address   = {New York, NY, USA},
	series    = {{IUI} '19},
	title     = {I {Can} {Do} {Better} than {Your} {AI}: {Expertise} and {Explanations}},
	isbn      = {978-1-4503-6272-6},
	url       = {https://doi.org/10.1145/3301275.3302308},
	doi       = {10.1145/3301275.3302308},
	abstract  = {Intelligent assistants, such as navigation, recommender, and expert systems, are most helpful in situations where users lack domain knowledge. Despite this, recent research in cognitive psychology has revealed that lower-skilled individuals may maintain a sense of illusory superiority, which might suggest that users with the highest need for advice may be the least likely to defer judgment. Explanation interfaces - a method for persuading users to take a system's advice - are thought by many to be the solution for instilling trust, but do their effects hold for self-assured users? To address this knowledge gap, we conducted a quantitative study (N=529) wherein participants played a binary decision-making game with help from an intelligent assistant. Participants were profiled in terms of both actual (measured) expertise and reported familiarity with the task concept. The presence of explanations, level of automation, and number of errors made by the intelligent assistant were manipulated while observing changes in user acceptance of advice. An analysis of cognitive metrics lead to three findings for research in intelligent assistants: 1) higher reported familiarity with the task simultaneously predicted more reported trust but less adherence, 2) explanations only swayed people who reported very low task familiarity, and 3) showing explanations to people who reported more task familiarity led to automation bias.},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author    = {Schaffer, James and O'Donovan, John and Michaelis, James and Raglin, Adrienne and Höllerer, Tobias},
	year      = {2019},
	note      = {event-place: Marina del Ray, California},
	keywords  = {cognitive modeling, decision support systems, human-computer interaction, information systems, intelligent assistants, user interfaces},
	pages     = {240--251},
	file      = {Schaffer et al. - 2019 - I can do better than your AI expertise and explan.pdf:/Users/fonok3/Zotero/storage/VITFKZEX/Schaffer et al. - 2019 - I can do better than your AI expertise and explan.pdf:application/pdf}
}

@inproceedings{weitz_you_2019,
	address   = {New York, NY, USA},
	series    = {{IVA} '19},
	title     = {Do You Trust Me?: Increasing {User}-{Trust} by {Integrating} {Virtual} {Agents} in {Explainable} {AI} {Interaction} {Design}},
	isbn      = {978-1-4503-6672-4},
	url       = {https://doi.org/10.1145/3308532.3329441},
	doi       = {10.1145/3308532.3329441},
	abstract  = {While the research area of artificial intelligence benefited from increasingly sophisticated machine learning techniques in recent years, the resulting systems suffer from a loss of transparency and comprehensibility. This development led to an on-going resurgence of the research area of explainable artificial intelligence (XAI) which aims to reduce the opaqueness of those black-box-models. However, much of the current XAI-Research is focused on machine learning practitioners and engineers while omitting the specific needs of end-users. In this paper, we examine the impact of virtual agents within the field of XAI on the perceived trustworthiness of autonomous intelligent systems. To assess the practicality of this concept, we conducted a user study based on a simple speech recognition task. As a result of this experiment, we found significant evidence suggesting that the integration of virtual agents into XAI interaction design leads to an increase of trust in the autonomous intelligent system.},
	booktitle = {Proceedings of the 19th {ACM} {International} {Conference} on {Intelligent} {Virtual} {Agents}},
	publisher = {Association for Computing Machinery},
	author    = {Weitz, Katharina and Schiller, Dominik and Schlagowski, Ruben and Huber, Tobias and André, Elisabeth},
	year      = {2019},
	note      = {event-place: Paris, France},
	keywords  = {deep learning, explainable artificial intelligence, Explanation: Design, human-agent interaction, interpretability, trust, virtual agents},
	pages     = {7--9},
	file      = {Full Text:/Users/fonok3/Zotero/storage/I8D9B48K/Weitz et al. - 2019 - Do You Trust Me Increasing User-Trust by Integ.pdf:application/pdf}
}

@inproceedings{sato_action-triggering_2019,
	title     = {Action-{Triggering} {Recommenders}: {Uplift} {Optimization} and {Persuasive} {Explanation}},
	doi       = {10.1109/ICDMW.2019.00155},
	abstract  = {A principal purpose of recommender systems is to induce positive user actions such as clicks and purchases. The performance of a recommender is typically evaluated in terms of prediction accuracy; a recommender is considered to be better if a larger number of its recommended items are purchased. However, accurate prediction is not enough for increasing user actions. The items might have been purchased even without recommendations, that is, the purchases are not triggered by the recommendations. Conversely, a user might be reluctant to purchase any items and a mere recommendation alone cannot motivate the user to purchase an item even if the recommended item is the best among all the items. In this work, we pursue recommender systems for triggering user actions. For this purpose, we tackle two issues: 1) uplift optimization and 2) persuasive explanation. For the first issue, we propose uplift-based evaluation and optimization methods for recommenders. Uplift, which is defined as an increase in user actions caused by recommendations, cannot be observed directly. We apply a causal inference framework to estimate the average uplift for the offline evaluation of recommenders. For optimization, we derive positive and negative samples specific to uplift and construct pointwise and pairwise optimization methods. Through experiments with three public datasets, we demonstrate the effectiveness of our optimization methods in improving uplift. For the second issue, we propose a new explanation style using context. The context style explanation presents contexts suitable for consuming the recommended items. The exhibited contexts make users imagine situations of items' usage and motivate them to purchase the items. Via a crowdsourcing-based user study, we confirm that the persuasiveness of our explanation style is better than conventional styles. The hybridization of context style with other styles further improves the persuasiveness.},
	booktitle = {2019 {International} {Conference} on {Data} {Mining} {Workshops} ({ICDMW})},
	author    = {Sato, Masahiro and Kawai, Shin and Nobuhara, Hajime},
	month     = nov,
	year      = {2019},
	note      = {ISSN: 2375-9259},
	keywords  = {causal inference, context-awareness, explainable recommendation, Explanation: Design, Measurement, Optimization methods, Predictive models, Protocols, recommendation effect, Recommender systems, Training, uplift modeling},
	pages     = {1060--1069},
	file      = {Sato et al. - 2019 - Action-Triggering Recommenders Uplift Optimizatio.pdf:/Users/fonok3/Zotero/storage/6MLRUT55/Sato et al. - 2019 - Action-Triggering Recommenders Uplift Optimizatio.pdf:application/pdf}
}

@inproceedings{li_reasoning_2020,
	title     = {Reasoning about {When} to {Provide} {Explanation} for {Human}-involved {Self}-{Adaptive} {Systems}},
	doi       = {10.1109/ACSOS49614.2020.00042},
	abstract  = {Many self-adaptive systems benefit from human involvement, where a human operator can provide expertise not available to the system and perform adaptations involving physical changes that cannot be automated. However, a lack of transparency and intelligibility of system goals and the autonomous behaviors enacted to achieve them may hinder a human operator's effort to make such involvement effective. Explanation is sometimes helpful to allow the human to understand why the system is making certain decisions. However, explanations come with costs in terms of, e.g., delayed actions. Hence, it is not always obvious whether explanations will improve the satisfaction of system goals and, if so, when to provide them to the operator. In this work, we define a formal framework for reasoning about explanations of adaptive system behaviors and the conditions under which they are warranted. Specifically, we characterize explanations in terms of their impact on a human operator's ability to effectively engage in adaptive actions. We then present a decision-making approach for planning in self-adaptation that leverages a probabilistic reasoning tool to determine when the explanation should be used in an adaptation strategy in order to improve overall system utility. We illustrate our approach in a representative scenario for the application of an adaptive news website in the context of potential denial-of-service attacks.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Autonomic} {Computing} and {Self}-{Organizing} {Systems} ({ACSOS})},
	author    = {Li, Nianyu and Cámara, Javier and Garlan, David and Schmerl, Bradley},
	month     = aug,
	year      = {2020},
	keywords  = {Adaptation models, Adaptive systems, Analytical models, Cognition, Model checking, Probabilistic logic, Time factors},
	pages     = {195--204},
	file      = {Li et al. - 2020 - Reasoning about When to Provide Explanation for Hu.pdf:/Users/fonok3/Zotero/storage/HPUNCK8V/Li et al. - 2020 - Reasoning about When to Provide Explanation for Hu.pdf:application/pdf}
}

@inproceedings{kohl_explainability_2019,
	title     = {Explainability as a {Non}-{Functional} {Requirement}},
	doi       = {10.1109/RE.2019.00046},
	abstract  = {Recent research efforts strive to aid in designing explainable systems. Nevertheless, a systematic and overarching approach to ensure explainability by design is still missing. Often it is not even clear what precisely is meant when demanding explainability. To address this challenge, we investigate the elicitation, specification, and verification of explainablity as a Non-Functional Requirement (NFR) with the long-term vision of establishing a standardized certification process for the explainability of software-driven systems in tandem with appropriate development techniques. In this work, we carve out different notions of explainability and high-level requirements people have in mind when demanding explainability, and sketch how explainability concerns may be approached in a hypothetical hiring scenario. We provide a conceptual analysis which unifies the different notions of explainability and the corresponding explainability demands.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author    = {Köhl, Maximilian A. and Baum, Kevin and Langer, Markus and Oster, Daniel and Speith, Timo and Bohlender, Dimitri},
	month     = sep,
	year      = {2019},
	note      = {ISSN: 2332-6441},
	keywords  = {Certification, certified explainability, Decision making, explainable systems, Organizational aspects, Personnel, requirements elicitation, requirements specification, Stakeholders, Standardization, terminology, Terminology},
	pages     = {363--368},
	file      = {Kohl et al. - 2019 - Explainability as a Non-Functional Requirement.pdf:/Users/fonok3/Zotero/storage/IUBTPCIE/Kohl et al. - 2019 - Explainability as a Non-Functional Requirement.pdf:application/pdf}
}

@inproceedings{meteier_workshop_2019,
	address   = {New York, NY, USA},
	series    = {{AutomotiveUI} '19},
	title     = {Workshop on {Explainable} {AI} in {Automated} {Driving}: {A} {User}-{Centered} {Interaction} {Approach}},
	isbn      = {978-1-4503-6920-6},
	url       = {https://doi.org/10.1145/3349263.3350762},
	doi       = {10.1145/3349263.3350762},
	abstract  = {With the increasing use of automation, users tend to delegate more tasks to the machines. Such complex systems are usually developed with "black box" Artificial Intelligence (AI), which makes these systems difficult to understand for the user. This assumption is particularly true in the field of automated driving since the level of automation is constantly increasing via the use of state-of-the-art AI solutions. We believe it is important to investigate the field of Explainable AI (XAI) in the context of automated driving since interpretability and transparency are key factors for increasing trust and security. In this workshop, we aim at gathering researchers and industry practitioners from different fields to brainstorm about XAI with a special focus on human-vehicle interaction. Questions like "what kind of explanation do we need", "which is the best trade-off between performance and explainability" and "how granular should the explanations be" will be addressed in this workshop.},
	booktitle = {Proceedings of the 11th {International} {Conference} on {Automotive} {User} {Interfaces} and {Interactive} {Vehicular} {Applications}: {Adjunct} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author    = {Meteier, Quentin and Capallera, Marine and Angelini, Leonardo and Mugellini, Elena and Khaled, Omar Abou and Carrino, Stefano and De Salis, Emmanuel and Galland, Stéphane and Boll, Susanne},
	year      = {2019},
	note      = {event-place: Utrecht, Netherlands},
	keywords  = {automated driving, explainable artificial intelligence, trust in automation, user interface design, vehicle environment},
	pages     = {32--37},
	file      = {Meteier et al. - 2019 - Workshop on explainable AI in automated driving a.pdf:/Users/fonok3/Zotero/storage/WKX27Z8M/Meteier et al. - 2019 - Workshop on explainable AI in automated driving a.pdf:application/pdf}
}

@inproceedings{haspiel_explanations_2018,
	address   = {New York, NY, USA},
	series    = {{HRI} '18},
	title     = {Explanations and {Expectations}: {Trust} {Building} in {Automated} {Vehicles}},
	isbn      = {978-1-4503-5615-2},
	url       = {https://doi.org/10.1145/3173386.3177057},
	doi       = {10.1145/3173386.3177057},
	abstract  = {Trust is a vital determinant of acceptance of automated vehicles (AVs) and expectations and explanations are often at the heart of any trusting relationship. Once expectations have been violated, explanations are needed to mitigate the damage. This study introduces the importance of timing of explanations in promoting trust in AVs. We present the preliminary results of a within-subjects experimental study involving eight participants exposed to four AV driving conditions (i.e. 32 data points). Preliminary results show a pattern that suggests that explanations provided before the AV takes actions promote more trust than explanations provided afterward.},
	booktitle = {Companion of the 2018 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {Association for Computing Machinery},
	author    = {Haspiel, Jacob and Du, Na and Meyerson, Jill and Robert Jr., Lionel P. and Tilbury, Dawn and Yang, X. Jessie and Pradhan, Anuj K.},
	year      = {2018},
	note      = {event-place: Chicago, IL, USA},
	keywords  = {human-machine interface, transparency, trust in avs},
	pages     = {119--120},
	file      = {Full Text:/Users/fonok3/Zotero/storage/TRNIKWJ9/Haspiel et al. - 2018 - Explanations and Expectations Trust Building in A.pdf:application/pdf}
}

@inproceedings{de_almeida_investigating_2019,
	address   = {New York, NY, USA},
	series    = {{IHC} '19},
	title     = {Investigating {Google} {Dashboard}'s {Explainability} to {Support} {Individual} {Privacy} {Decision} {Making}},
	isbn      = {978-1-4503-6971-8},
	url       = {https://doi.org/10.1145/3357155.3358438},
	doi       = {10.1145/3357155.3358438},
	abstract  = {Advances in information technology often overwhelm users with complex privacy and security decisions. They make the collection and use of personal data quite invisible. In the current scenario, this data collection can introduce risks, manipulate and influence the decision making process. This research is based on concepts from an emerging field of study called Human Data Interaction (HDI), which proposes to include the human at the center of the data stream, providing mechanisms for citizens to interact explicitly with the collected data. We explored the explanation as a promising mechanism for transparency in automated systems. In the first step, we apply the Semiotic Inspection Method (SIM) longitudinally to investigate how using explanations as an interactive feature can help or prevent users from making privacy decisions on Google services. In the second step, we conducted an empirical study in which users are able to analyze whether these explanations are satisfactory and feel (un) secure in the decision making process. And by comparing the results of the two steps, we find that even in a large company like Google, the right to explanation is not guaranteed. Google does not make its data processing transparent to users, nor does it provide satisfactory explanations of how its services use individual data. Consequently, the lack of coherent, detailed and transparent explanations hamper users to make good and safe decisions.},
	booktitle = {Proceedings of the 18th {Brazilian} {Symposium} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author    = {de Almeida, Maria Clara G. and de Castro Salgado, Luciana C.},
	year      = {2019},
	note      = {event-place: Vitória, Espírito Santo, Brazil},
	keywords  = {data control, decision making, explanation, privacy},
	file      = {de Almeida and de Castro Salgado - 2019 - Investigating Google dashboard's explainability to.pdf:/Users/fonok3/Zotero/storage/6YLUAL5K/de Almeida and de Castro Salgado - 2019 - Investigating Google dashboard's explainability to.pdf:application/pdf}
}

@inproceedings{barik_improving_2015,
	title     = {Improving error notification comprehension in {IDEs} by supporting developer self-explanations},
	doi       = {10.1109/VLHCC.2015.7357233},
	abstract  = {Despite the advanced static analysis techniques available to compilers, error notifications as presented by modern IDEs remain perplexing for developers to resolve. My thesis postulates that tools fail to adequately support self-explanation, a core metacognitive process necessary to comprehend notifications. The contribution of my work will bridge the gap between the presentation of tools and interpretation by developers by enabling IDEs to present the information they compute in a way that supports developer self-explanation.},
	booktitle = {2015 {IEEE} {Symposium} on {Visual} {Languages} and {Human}-{Centric} {Computing} ({VL}/{HCC})},
	author    = {Barik, Titus},
	month     = oct,
	year      = {2015},
	keywords  = {Cognition, Probabilistic logic},
	pages     = {293--294},
	file      = {Barik - 2015 - Improving error notification comprehension in IDEs.pdf:/Users/fonok3/Zotero/storage/YUNRKTW5/Barik - 2015 - Improving error notification comprehension in IDEs.pdf:application/pdf}
}

@inproceedings{kaptein_personalised_2017,
	title     = {Personalised self-explanation by robots: {The} role of goals versus beliefs in robot-action explanation for children and adults},
	doi       = {10.1109/ROMAN.2017.8172376},
	abstract  = {A good explanation takes the user who is receiving the explanation into account. We aim to get a better understanding of user preferences and the differences between children and adults who receive explanations from a robot. We implemented a Nao-robot as a belief-desire-intention (BDI)-based agent and explained its actions using two different explanation styles. Both are based on how humans explain and justify their actions to each other. One explanation style communicates the beliefs that give context information on why the agent performed the action. The other explanation style communicates the goals that inform the user of the agent's desired state when performing the action. We conducted a user study (19 children, 19 adults) in which a Nao-robot performed actions to support type 1 diabetes mellitus management. We investigated the preference of children and adults for goalversus belief-based action explanations. From this, we learned that adults have a significantly higher tendency to prefer goal-based action explanations. This work is a necessary step in addressing the challenge of providing personalised explanations in human-robot and human-agent interaction.},
	booktitle = {2017 26th {IEEE} {International} {Symposium} on {Robot} and {Human} {Interactive} {Communication} ({RO}-{MAN})},
	author    = {Kaptein, Frank and Broekens, Joost and Hindriks, Koen and Neerincx, Mark},
	month     = aug,
	year      = {2017},
	note      = {ISSN: 1944-9437},
	keywords  = {Aging, Diabetes, Games, Intelligent systems, Pediatrics, Psychology, Robots},
	pages     = {676--682},
	file      = {Kaptein et al. - 2017 - Personalised self-explanation by robots The role .pdf:/Users/fonok3/Zotero/storage/T7G4EPIW/Kaptein et al. - 2017 - Personalised self-explanation by robots The role .pdf:application/pdf}
}

@inproceedings{zolotas_towards_2019,
	title     = {Towards {Explainable} {Shared} {Control} using {Augmented} {Reality}},
	doi       = {10.1109/IROS40897.2019.8968117},
	abstract  = {Shared control plays a pivotal role in establishing effective human-robot interactions. Traditional control-sharing methods strive to complement a human's capabilities at safely completing a task, and thereby rely on users forming a mental model of the expected robot behaviour. However, these methods can often bewilder or frustrate users whenever their actions do not elicit the intended system response, forming a misalignment between the respective internal models of the robot and human. To resolve this model misalignment, we introduce Explainable Shared Control as a paradigm in which assistance and information feedback are jointly considered. Augmented reality is presented as an integral component of this paradigm, by visually unveiling the robot's inner workings to human operators. Explainable Shared Control is instantiated and tested for assistive navigation in a setup involving a robotic wheelchair and a Microsoft HoloLens with add-on eye tracking. Experimental results indicate that the introduced paradigm facilitates transparent assistance by improving recovery times from adverse events associated with model misalignment.},
	booktitle = {2019 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author    = {Zolotas, Mark and Demiris, Yiannis},
	month     = nov,
	year      = {2019},
	note      = {ISSN: 2153-0866},
	pages     = {3020--3026},
	file      = {Submitted Version:/Users/fonok3/Zotero/storage/KVMWC2M8/Zolotas and Demiris - 2019 - Towards Explainable Shared Control using Augmented.pdf:application/pdf}
}

@article{riveiro_thats_2021,
	title    = {“{That}'s (not) the output {I} expected!” {On} the role of end user expectations in creating explanations of {AI} systems},
	volume   = {298},
	issn     = {0004-3702},
	url      = {https://www.sciencedirect.com/science/article/pii/S0004370221000588},
	doi      = {https://doi.org/10.1016/j.artint.2021.103507},
	abstract = {Research in the social sciences has shown that expectations are an important factor in explanations as used between humans: rather than explaining the cause of an event per se, the explainer will often address another event that did not occur but that the explainee might have expected. For AI-powered systems, this finding suggests that explanation-generating systems may need to identify such end user expectations. In general, this is a challenging task, not the least because users often keep them implicit; there is thus a need to investigate the importance of such an ability. In this paper, we report an empirical study with 181 participants who were shown outputs from a text classifier system along with an explanation of why the system chose a particular class for each text. Explanations were both factual, explaining why the system produced a certain output or counterfactual, explaining why the system produced one output instead of another. Our main hypothesis was explanations should align with end user expectations; that is, a factual explanation should be given when the system's output is in line with end user expectations, and a counterfactual explanation when it is not. We find that factual explanations are indeed appropriate when expectations and output match. When they do not, neither factual nor counterfactual explanations appear appropriate, although we do find indications that our counterfactual explanations contained at least some necessary elements. Overall, this suggests that it is important for systems that create explanations of AI systems to infer what outputs the end user expected so that factual explanations can be generated at the appropriate moments. At the same time, this information is, by itself, not sufficient to also create appropriate explanations when the output and user expectations do not match. This is somewhat surprising given investigations of explanations in the social sciences, and will need more scrutiny in future studies.},
	journal  = {Artificial Intelligence},
	author   = {Riveiro, Maria and Thill, Serge},
	year     = {2021},
	keywords = {Contrastive, Counterfactual, Expectations, Explainable AI, Explanation: Kind, Explanations, Factual, Human-AI interaction, Machine behaviour, Mental models},
	pages    = {103507},
	file     = {Riveiro and Thill - 2021 - “That's (not) the output I expected!” On the role .pdf:/Users/fonok3/Zotero/storage/67DCW548/Riveiro and Thill - 2021 - “That's (not) the output I expected!” On the role .pdf:application/pdf;Riveiro and Thill - 2021 - “That's (not) the output I expected!” On the role .pdf:/Users/fonok3/Zotero/storage/M2RYKNUG/Riveiro and Thill - 2021 - “That's (not) the output I expected!” On the role .pdf:application/pdf}
}

@article{martin_evaluating_2021,
	title    = {Evaluating {Explainability} {Methods} {Intended} for {Multiple} {Stakeholders}},
	issn     = {1610-1987},
	url      = {https://doi.org/10.1007/s13218-020-00702-6},
	doi      = {10.1007/s13218-020-00702-6},
	abstract = {Explanation mechanisms for intelligent systems are typically designed to respond to specific user needs, yet in practice these systems tend to have a wide variety of users. This can present a challenge to organisations looking to satisfy the explanation needs of different groups using an individual system. In this paper we present an explainability framework formed of a catalogue of explanation methods, and designed to integrate with a range of projects within a telecommunications organisation. Explainability methods are split into low-level explanations and high-level explanations for increasing levels of contextual support in their explanations. We motivate this framework using the specific case-study of explaining the conclusions of field network engineering experts to non-technical planning staff and evaluate our results using feedback from two distinct user groups; domain-expert telecommunication engineers and non-expert desk agent staff. We also present and investigate two metrics designed to model the quality of explanations - Meet-In-The-Middle (MITM) and Trust-Your-Neighbours (TYN). Our analysis of these metrics offers new insights into the use of similarity knowledge for the evaluation of explanations.},
	language = {en},
	urldate  = {2021-05-24},
	journal  = {KI - Künstliche Intelligenz},
	author   = {Martin, Kyle and Liret, Anne and Wiratunga, Nirmalie and Owusu, Gilbert and Kern, Mathias},
	month    = feb,
	year     = {2021},
	keywords = {Explanation: Evaluation Method},
	file     = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/DWIJHANY/Martin et al. - 2021 - Evaluating Explainability Methods Intended for Mul.pdf:application/pdf}
}

@inproceedings{martin_developing_2019,
	address   = {Cham},
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {Developing a {Catalogue} of {Explainability} {Methods} to {Support} {Expert} and {Non}-expert {Users}},
	isbn      = {978-3-030-34885-4},
	doi       = {10.1007/978-3-030-34885-4_24},
	abstract  = {Organisations face growing legal requirements and ethical responsibilities to ensure that decisions made by their intelligent systems are explainable. However, provisioning of an explanation is often application dependent, causing an extended design phase and delayed deployment. In this paper we present an explainability framework formed of a catalogue of explanation methods, allowing integration to a range of projects within a telecommunications organisation. These methods are split into low-level explanations, high-level explanations and co-created explanations. We motivate and evaluate this framework using the specific case-study of explaining the conclusions of field engineering experts to non-technical planning staff. Feedback from an iterative co-creation process and a qualitative evaluation is indicative that this is a valuable development tool for use in future company projects.},
	language  = {en},
	booktitle = {Artificial {Intelligence} {XXXVI}},
	publisher = {Springer International Publishing},
	author    = {Martin, Kyle and Liret, Anne and Wiratunga, Nirmalie and Owusu, Gilbert and Kern, Mathias},
	editor    = {Bramer, Max and Petridis, Miltos},
	year      = {2019},
	keywords  = {Explainability, Information retrieval, Machine learning, Similarity modeling},
	pages     = {309--324},
	file      = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/D2BLE55F/Martin et al. - 2019 - Developing a Catalogue of Explainability Methods t.pdf:application/pdf}
}

@article{tsai_effects_2020,
	title    = {The effects of controllability and explainability in a social recommender system},
	issn     = {1573-1391},
	url      = {https://doi.org/10.1007/s11257-020-09281-5},
	doi      = {10.1007/s11257-020-09281-5},
	abstract = {In recent years, researchers in the field of recommender systems have explored a range of advanced interfaces to improve user interactions with recommender systems. Some of the major research ideas explored in this new area include the explainability and controllability of recommendations. Controllability enables end users to participate in the recommendation process by providing various kinds of input. Explainability focuses on making the recommendation process and the reasons behind specific recommendation more clear to the users. While each of these approaches contributes to making traditional “black-box” recommendation more attractive and acceptable to end users, little is known about how these approaches work together. In this paper, we investigate the effects of adding user control and visual explanations in a specific context of an interactive hybrid social recommender system. We present Relevance Tuner+, a hybrid recommender system that allows the users to control the fusion of multiple recommender sources while also offering explanations of both the fusion process and each of the source recommendations. We also report the results of a controlled study (N = 50) that explores the impact of controllability and explainability in this context.},
	language = {en},
	urldate  = {2021-05-24},
	journal  = {User Modeling and User-Adapted Interaction},
	author   = {Tsai, Chun-Hua and Brusilovsky, Peter},
	month    = oct,
	year     = {2020},
	file     = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/QUW5Y23M/Tsai and Brusilovsky - 2020 - The effects of controllability and explainability .pdf:application/pdf}
}

@article{rosenfeld_explainability_2019,
	title    = {Explainability in human–agent systems},
	volume   = {33},
	issn     = {1573-7454},
	url      = {https://doi.org/10.1007/s10458-019-09408-y},
	doi      = {10.1007/s10458-019-09408-y},
	abstract = {This paper presents a taxonomy of explainability in human–agent systems. We consider fundamental questions about the Why, Who, What, When and How of explainability. First, we define explainability, and its relationship to the related terms of interpretability, transparency, explicitness, and faithfulness. These definitions allow us to answer why explainability is needed in the system, whom it is geared to and what explanations can be generated to meet this need. We then consider when the user should be presented with this information. Last, we consider how objective and subjective measures can be used to evaluate the entire system. This last question is the most encompassing as it will need to evaluate all other issues regarding explainability.},
	language = {en},
	number   = {6},
	urldate  = {2021-05-24},
	journal  = {Autonomous Agents and Multi-Agent Systems},
	author   = {Rosenfeld, Avi and Richardson, Ariella},
	month    = nov,
	year     = {2019},
	pages    = {673--705},
	file     = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/F6PLU5CM/Rosenfeld and Richardson - 2019 - Explainability in human–agent systems.pdf:application/pdf}
}

@article{weitz_let_2021,
	title      = {“{Let} me explain!”: exploring the potential of virtual agents in explainable {AI} interaction design},
	volume     = {15},
	issn       = {1783-8738},
	shorttitle = {“{Let} me explain!”},
	url        = {https://doi.org/10.1007/s12193-020-00332-0},
	doi        = {10.1007/s12193-020-00332-0},
	abstract   = {While the research area of artificial intelligence benefited from increasingly sophisticated machine learning techniques in recent years, the resulting systems suffer from a loss of transparency and comprehensibility, especially for end-users. In this paper, we explore the effects of incorporating virtual agents into explainable artificial intelligence (XAI) designs on the perceived trust of end-users. For this purpose, we conducted a user study based on a simple speech recognition system for keyword classification. As a result of this experiment, we found that the integration of virtual agents leads to increased user trust in the XAI system. Furthermore, we found that the user’s trust significantly depends on the modalities that are used within the user-agent interface design. The results of our study show a linear trend where the visual presence of an agent combined with a voice output resulted in greater trust than the output of text or the voice output alone. Additionally, we analysed the participants’ feedback regarding the presented XAI visualisations. We found that increased human-likeness of and interaction with the virtual agent are the two most common mention points on how to improve the proposed XAI interaction design. Based on these results, we discuss current limitations and interesting topics for further research in the field of XAI. Moreover, we present design recommendations for virtual agents in XAI systems for future projects.},
	language   = {en},
	number     = {2},
	urldate    = {2021-05-24},
	journal    = {Journal on Multimodal User Interfaces},
	author     = {Weitz, Katharina and Schiller, Dominik and Schlagowski, Ruben and Huber, Tobias and André, Elisabeth},
	month      = jun,
	year       = {2021},
	keywords   = {Explanation: Design},
	pages      = {87--98},
	file       = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/BVRC6YPP/Weitz et al. - 2021 - “Let me explain!” exploring the potential of virt.pdf:application/pdf}
}

@article{hacker_explainable_2020,
	title      = {Explainable {AI} under contract and tort law: legal incentives and technical challenges},
	volume     = {28},
	issn       = {1572-8382},
	shorttitle = {Explainable {AI} under contract and tort law},
	url        = {https://doi.org/10.1007/s10506-020-09260-6},
	doi        = {10.1007/s10506-020-09260-6},
	abstract   = {This paper shows that the law, in subtle ways, may set hitherto unrecognized incentives for the adoption of explainable machine learning applications. In doing so, we make two novel contributions. First, on the legal side, we show that to avoid liability, professional actors, such as doctors and managers, may soon be legally compelled to use explainable ML models. We argue that the importance of explainability reaches far beyond data protection law, and crucially influences questions of contractual and tort liability for the use of ML models. To this effect, we conduct two legal case studies, in medical and corporate merger applications of ML. As a second contribution, we discuss the (legally required) trade-off between accuracy and explainability and demonstrate the effect in a technical case study in the context of spam classification.},
	language   = {en},
	number     = {4},
	urldate    = {2021-05-24},
	journal    = {Artificial Intelligence and Law},
	author     = {Hacker, Philipp and Krestel, Ralf and Grundmann, Stefan and Naumann, Felix},
	month      = dec,
	year       = {2020},
	pages      = {415--439},
	file       = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/I494CPG8/Hacker et al. - 2020 - Explainable AI under contract and tort law legal .pdf:application/pdf}
}

@inproceedings{rjoob_towards_2021,
	address   = {Cham},
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {Towards {Explainable} {Artificial} {Intelligence} and {Explanation} {User} {Interfaces} to {Open} the ‘{Black} {Box}’ of {Automated} {ECG} {Interpretation}},
	isbn      = {978-3-030-68007-7},
	doi       = {10.1007/978-3-030-68007-7_6},
	abstract  = {This an exploratory paper that discusses the use of artificial intelligence (AI) in ECG interpretation and opportunities for improving the explainability of the AI (XAI) when reading 12-lead ECGs. To develop AI systems, many principles (human rights, well-being, data agency, effectiveness, transparency, accountability, awareness of misuse and competence) must be considered to ensure that the AI is trustworthy and applicable. The current computerised ECG interpretation algorithms can detect different types of heart diseases. However, there are some challenges and shortcomings that need to be addressed, such as the explainability issue and the interaction between the human and the AI for clinical decision making. These challenges create opportunities to develop a trustworthy XAI for automated ECG interpretation with a high performance and a high confidence level. This study reports a proposed XAI interface design in automatic ECG interpretation based on suggestions from previous studies and based on standard guidelines that were developed by the human computer interaction (HCI) community. New XAI interfaces should be developed in the future that facilitate more transparency of the decision logic of the algorithm which may allow users to calibrate their trust and use of the AI system.},
	language  = {en},
	booktitle = {Advanced {Visual} {Interfaces}. {Supporting} {Artificial} {Intelligence} and {Big} {Data} {Applications}},
	publisher = {Springer International Publishing},
	author    = {Rjoob, Khaled and Bond, Raymond and Finlay, Dewar and McGilligan, Victoria and Leslie, Stephen J. and Rababah, Ali and Iftikhar, Aleeha and Guldenring, Daniel and Knoery, Charles and McShane, Anne and Peace, Aaron},
	editor    = {Reis, Thoralf and Bornschlegl, Marco X. and Angelini, Marco and Hemmje, Matthias L.},
	year      = {2021},
	keywords  = {Artificial intelligence (AI), ECG interpretation, Explainable AI (XAI)},
	pages     = {96--108},
	file      = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/M9UI7MV5/Rjoob et al. - 2021 - Towards Explainable Artificial Intelligence and Ex.pdf:application/pdf}
}

@inproceedings{ehsan_human-centered_2020,
	address    = {Cham},
	series     = {Lecture {Notes} in {Computer} {Science}},
	title      = {Human-{Centered} {Explainable} {AI}: {Towards} a {Reflective} {Sociotechnical} {Approach}},
	isbn       = {978-3-030-60117-1},
	shorttitle = {Human-{Centered} {Explainable} {AI}},
	doi        = {10.1007/978-3-030-60117-1_33},
	abstract   = {Explanations—a form of post-hoc interpretability—play an instrumental role in making systems accessible as AI continues to proliferate complex and sensitive sociotechnical systems. In this paper, we introduce Human-centered Explainable AI (HCXAI) as an approach that puts the human at the center of technology design. It develops a holistic understanding of “who” the human is by considering the interplay of values, interpersonal dynamics, and the socially situated nature of AI systems. In particular, we advocate for a reflective sociotechnical approach. We illustrate HCXAI through a case study of an explanation system for non-technical end-users that shows how technical advancements and the understanding of human factors co-evolve. Building on the case study, we lay out open research questions pertaining to further refining our understanding of “who” the human is and extending beyond 1-to-1 human-computer interactions. Finally, we propose that a reflective HCXAI paradigm—mediated through the perspective of Critical Technical Practice and supplemented with strategies from HCI, such as value-sensitive design and participatory design—not only helps us understand our intellectual blind spots, but it can also open up new design and research spaces.},
	language   = {en},
	booktitle  = {{HCI} {International} 2020 - {Late} {Breaking} {Papers}: {Multimodality} and {Intelligence}},
	publisher  = {Springer International Publishing},
	author     = {Ehsan, Upol and Riedl, Mark O.},
	editor     = {Stephanidis, Constantine and Kurosu, Masaaki and Degen, Helmut and Reinerman-Jones, Lauren},
	year       = {2020},
	keywords   = {Artificial intelligence, Critical technical practice, Explainable AI, Human-centered computing, Interpretability, Machine learning, Rationale generation, Sociotechnical, User perception},
	pages      = {449--466},
	file       = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/WPF6T4X6/Ehsan and Riedl - 2020 - Human-Centered Explainable AI Towards a Reflectiv.pdf:application/pdf}
}

@inproceedings{cassens_ambient_2019,
	address    = {Cham},
	series     = {Lecture {Notes} in {Computer} {Science}},
	title      = {Ambient {Explanations}: {Ambient} {Intelligence} and {Explainable} {AI}},
	isbn       = {978-3-030-34255-5},
	shorttitle = {Ambient {Explanations}},
	doi        = {10.1007/978-3-030-34255-5_30},
	abstract   = {With renewed prominence of Explainable AI (XAI), many areas are revisiting early work on Explainability. Within the broader field of Artificial Intelligence (AI), Ambient Intelligence (AmI) has an advantage in the development of transparent and ethical systems because such work has long been an integral part of research, development and operations in AmI. In this paper we argue that, because of the paradigm requirements of system intelligence, social intelligence and embeddedness, AmI is uniquely prepared to support the push for ethical and transparent technology development. We argue that Ambient Intelligent Systems are well suited to infer when an explanation might be needed (and of what kind), and the form that it should take. We further propose AmI devices as mediators between humans and machines because they can combine social and technical systems in a fully embedded way.},
	language   = {en},
	booktitle  = {Ambient {Intelligence}},
	publisher  = {Springer International Publishing},
	author     = {Cassens, Jörg and Wegener, Rebekah},
	editor     = {Chatzigiannakis, Ioannis and De Ruyter, Boris and Mavrommati, Irene},
	year       = {2019},
	keywords   = {Ambient Intelligence, Context, Explanation: Evaluation Method, Explanation: When?, Explanations, Semiotics},
	pages      = {370--376},
	file       = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/9LQSDE24/Cassens and Wegener - 2019 - Ambient Explanations Ambient Intelligence and Exp.pdf:application/pdf}
}

@inproceedings{thomson_knowledge--information_2020,
	address    = {Cham},
	series     = {Lecture {Notes} in {Computer} {Science}},
	title      = {Knowledge-to-{Information} {Translation} {Training} ({KITT}): {An} {Adaptive} {Approach} to {Explainable} {Artificial} {Intelligence}},
	isbn       = {978-3-030-50788-6},
	shorttitle = {Knowledge-to-{Information} {Translation} {Training} ({KITT})},
	doi        = {10.1007/978-3-030-50788-6_14},
	abstract   = {Modern black-box artificial intelligence algorithms are computationally powerful yet fallible in unpredictable ways. While much research has gone into developing techniques to interpret these algorithms, less have also integrated the requirement to understand the algorithm as a function of their training data. In addition, few have examined the human requirements for explainability, so these interpretations provide the right quantity and quality of information to each user. We argue that Explainable Artificial Intelligence (XAI) frameworks need to account the expertise and goals of the user in order to gain widespread adoptance. We describe the Knowledge-to-Information Translation Training (KITT) framework, an approach to XAI that considers a number of possible explanatory models that can be used to facilitate users’ understanding of artificial intelligence. Following a review of algorithms, we provide a taxonomy of explanation types and outline how adaptive instructional systems can facilitate knowledge translation between developers and users. Finally, we describe limitations of our approach and paths for future research opportunities.},
	language   = {en},
	booktitle  = {Adaptive {Instructional} {Systems}},
	publisher  = {Springer International Publishing},
	author     = {Thomson, Robert and Schoenherr, Jordan Richard},
	editor     = {Sottilare, Robert A. and Schwarz, Jessica},
	year       = {2020},
	keywords   = {Adaptive instructional systems, Explainable AI, Knowledge translation},
	pages      = {187--204},
	file       = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/DD94REWA/Thomson and Schoenherr - 2020 - Knowledge-to-Information Translation Training (KIT.pdf:application/pdf}
}

@inproceedings{cirqueira_scenario-based_2020,
	address   = {Cham},
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {Scenario-{Based} {Requirements} {Elicitation} for {User}-{Centric} {Explainable} {AI}},
	isbn      = {978-3-030-57321-8},
	doi       = {10.1007/978-3-030-57321-8_18},
	abstract  = {Explainable Artificial Intelligence (XAI) develops technical explanation methods and enable interpretability for human stakeholders on why Artificial Intelligence (AI) and machine learning (ML) models provide certain predictions. However, the trust of those stakeholders into AI models and explanations is still an issue, especially domain experts, who are knowledgeable about their domain but not AI inner workings. Social and user-centric XAI research states it is essential to understand the stakeholder’s requirements to provide explanations tailored to their needs, and enhance their trust in working with AI models. Scenario-based design and requirements elicitation can help bridge the gap between social and operational aspects of a stakeholder early before the adoption of information systems and identify its real problem and practices generating user requirements. Nevertheless, it is still rarely explored the adoption of scenarios in XAI, especially in the domain of fraud detection to supporting experts who are about to work with AI models. We demonstrate the usage of scenario-based requirements elicitation for XAI in a fraud detection context, and develop scenarios derived with experts in banking fraud. We discuss how those scenarios can be adopted to identify user or expert requirements for appropriate explanations in his daily operations and to make decisions on reviewing fraudulent cases in banking. The generalizability of the scenarios for further adoption is validated through a systematic literature review in domains of XAI and visual analytics for fraud detection.},
	language  = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Extraction}},
	publisher = {Springer International Publishing},
	author    = {Cirqueira, Douglas and Nedbal, Dietmar and Helfert, Markus and Bezbradica, Marija},
	editor    = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
	year      = {2020},
	keywords  = {Domain expert, Explainable artificial intelligence, Fraud detection, Requirements elicitation},
	pages     = {321--341},
	file      = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/PTVNGSPL/Cirqueira et al. - 2020 - Scenario-Based Requirements Elicitation for User-C.pdf:application/pdf}
}

@article{sokol_one_2020,
	title    = {One {Explanation} {Does} {Not} {Fit} {All}},
	volume   = {34},
	issn     = {1610-1987},
	url      = {https://doi.org/10.1007/s13218-020-00637-y},
	doi      = {10.1007/s13218-020-00637-y},
	abstract = {The need for transparency of predictive systems based on Machine Learning algorithms arises as a consequence of their ever-increasing proliferation in the industry. Whenever black-box algorithmic predictions influence human affairs, the inner workings of these algorithms should be scrutinised and their decisions explained to the relevant stakeholders, including the system engineers, the system’s operators and the individuals whose case is being decided. While a variety of interpretability and explainability methods is available, none of them is a panacea that can satisfy all diverse expectations and competing objectives that might be required by the parties involved. We address this challenge in this paper by discussing the promises of Interactive Machine Learning for improved transparency of black-box systems using the example of contrastive explanations—a state-of-the-art approach to Interpretable Machine Learning. Specifically, we show how to personalise counterfactual explanations by interactively adjusting their conditional statements and extract additional explanations by asking follow-up “What if?” questions. Our experience in building, deploying and presenting this type of system allowed us to list desired properties as well as potential limitations, which can be used to guide the development of interactive explainers. While customising the medium of interaction, i.e., the user interface comprising of various communication channels, may give an impression of personalisation, we argue that adjusting the explanation itself and its content is more important. To this end, properties such as breadth, scope, context, purpose and target of the explanation have to be considered, in addition to explicitly informing the explainee about its limitations and caveats. Furthermore, we discuss the challenges of mirroring the explainee’s mental model, which is the main building block of intelligible human–machine interactions. We also deliberate on the risks of allowing the explainee to freely manipulate the explanations and thereby extracting information about the underlying predictive model, which might be leveraged by malicious actors to steal or game the model. Finally, building an end-to-end interactive explainability system is a challenging engineering task; unless the main goal is its deployment, we recommend “Wizard of Oz” studies as a proxy for testing and evaluating standalone interactive explainability algorithms.},
	language = {en},
	number   = {2},
	urldate  = {2021-05-24},
	journal  = {KI - Künstliche Intelligenz},
	author   = {Sokol, Kacper and Flach, Peter},
	month    = jun,
	year     = {2020},
	pages    = {235--250},
	file     = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/P72U5J56/Sokol and Flach - 2020 - One Explanation Does Not Fit All.pdf:application/pdf}
}

@inproceedings{lu_study_2017,
	address   = {Cham},
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {A {Study} on {Interactive} {Explanation} {Boards} {Design} and {Evaluation} for {Active} {Aging} {Ecotourism}},
	isbn      = {978-3-319-58530-7},
	doi       = {10.1007/978-3-319-58530-7_11},
	abstract  = {According to the advanced aging society and the change of travelstyle, the advanced age groups are active in joining ecotourism to increase knowledge and experiences that raise their qualities of living. Therefore a solution to enhance ecotourism experiences of the advanced age group would be a major consideration in the future. A tendency of elevating knowledge and experiences of the active aging group with assistance of technology is anticipated. As a consequence the study focus on literature analysis of active aging group, ecotourism, interactive design and related design principles. Through a demand investigation, design and building of principle-based interactive interpretive signs prototype, a revised proposal of the prototype design is come up based on user reports regarding evaluation and review of the interactive prototypes. The study results are as following: (1) A fulfillment of recording, knowledge, safety and convenience of ecotourism is brought to the active aging group through the assistance of technology. (2) From the investigation of technology demand, the study developed consistency, flexibility, efficiency, artistic and simplified design, visibility, feedback, attraction, instruction, sustainability, satisfaction, assistance and directions, user control and unrestrained 11 prototype design characteristics. The design and building of interactive interpretive sign prototypes are based on the above characteristics. (3) In accordance with the user report, an induction of “Hardware Performance Issue” of interactive prototypes are able to increase knowledge of the active aging group, so as to improve the height, layout, narration, icons and information guidance, etc. For the interactive narration time design in “Information Media”, long time consumption of active aging group in a fixed location should be avoided. It may cause inconvenience to other users. The guiding information should be more detailed and screen size should be increased. The “Interactive Operation” is smooth. Manipulating both graphics and words simultaneously, as well as the efficiency of operation system should improve the guidelines. The listening experience in “Sharing their Experience” is one of the most important elements to enhance the interactive narration quality. As for the vision experience, an advancement of the narration screen and the story content is required. The study results expect to enhance the ecotourism experience of the active aging group, also to provide references for related studies operations.},
	language  = {en},
	booktitle = {Human {Aspects} of {IT} for the {Aged} {Population}. {Aging}, {Design} and {User} {Experience}},
	publisher = {Springer International Publishing},
	author    = {Lu, Li-Shu},
	editor    = {Zhou, Jia and Salvendy, Gavriel},
	year      = {2017},
	keywords  = {Active aging, Ecotourism, Explanation boards, Explanation: Design, Interactive design, Usability},
	pages     = {160--172},
	file      = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/E2MS5QPT/Lu - 2017 - A Study on Interactive Explanation Boards Design a.pdf:application/pdf}
}

@inproceedings{chari_explanation_2020,
	address    = {Cham},
	series     = {Lecture {Notes} in {Computer} {Science}},
	title      = {Explanation {Ontology}: {A} {Model} of {Explanations} for {User}-{Centered} {AI}},
	isbn       = {978-3-030-62466-8},
	shorttitle = {Explanation {Ontology}},
	doi        = {10.1007/978-3-030-62466-8_15},
	abstract   = {Explainability has been a goal for Artificial Intelligence (AI) systems since their conception, with the need for explainability growing as more complex AI models are increasingly used in critical, high-stakes settings such as healthcare. Explanations have often added to an AI system in a non-principled, post-hoc manner. With greater adoption of these systems and emphasis on user-centric explainability, there is a need for a structured representation that treats explainability as a primary consideration, mapping end user needs to specific explanation types and the system’s AI capabilities. We design an explanation ontology to model both the role of explanations, accounting for the system and user attributes in the process, and the range of different literature-derived explanation types. We indicate how the ontology can support user requirements for explanations in the domain of healthcare. We evaluate our ontology with a set of competency questions geared towards a system designer who might use our ontology to decide which explanation types to include, given a combination of users’ needs and a system’s capabilities, both in system design settings and in real-time operations. Through the use of this ontology, system designers will be able to make informed choices on which explanations AI systems can and should provide.},
	language   = {en},
	booktitle  = {The {Semantic} {Web} – {ISWC} 2020},
	publisher  = {Springer International Publishing},
	author     = {Chari, Shruthi and Seneviratne, Oshani and Gruen, Daniel M. and Foreman, Morgan A. and Das, Amar K. and McGuinness, Deborah L.},
	editor     = {Pan, Jeff Z. and Tamma, Valentina and d’Amato, Claudia and Janowicz, Krzysztof and Fu, Bo and Polleres, Axel and Seneviratne, Oshani and Kagal, Lalana},
	year       = {2020},
	keywords   = {Explainable AI, Explanation ontology, Modeling of explanations and explanation types, Supporting explainable ai in clinical decision making and decision support},
	pages      = {228--243},
	file       = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/CP533N33/Chari et al. - 2020 - Explanation Ontology A Model of Explanations for .pdf:application/pdf}
}

@article{bharadhwaj_explanations_2018,
	title    = {Explanations for {Temporal} {Recommendations}},
	volume   = {32},
	issn     = {1610-1987},
	url      = {https://doi.org/10.1007/s13218-018-0560-x},
	doi      = {10.1007/s13218-018-0560-x},
	abstract = {Recommendation systems (RS) are an integral part of artificial intelligence (AI) and have become increasingly important in the growing age of commercialization in AI. Deep learning (DL) techniques for RS provide powerful latent-feature models for effective recommendation but suffer from the major drawback of being non-interpretable. In this paper we describe a framework for explainable temporal recommendations in a DL model. We consider an LSTM based Recurrent Neural Network architecture for recommendation and a neighbourhood based scheme for generating explanations in the model. We demonstrate the effectiveness of our approach through experiments on the Netflix dataset by jointly optimizing for both prediction accuracy and explainability.},
	language = {en},
	number   = {4},
	urldate  = {2021-05-24},
	journal  = {KI - Künstliche Intelligenz},
	author   = {Bharadhwaj, Homanga and Joshi, Shruti},
	month    = nov,
	year     = {2018},
	pages    = {267--272},
	file     = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/PZ7JLP3C/Bharadhwaj and Joshi - 2018 - Explanations for Temporal Recommendations.pdf:application/pdf}
}

@inproceedings{sovrano_modelling_2020,
	address   = {Cham},
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {Modelling {GDPR}-{Compliant} {Explanations} for {Trustworthy} {AI}},
	isbn      = {978-3-030-58957-8},
	doi       = {10.1007/978-3-030-58957-8_16},
	abstract  = {Through the General Data Protection Regulation (GDPR), the European Union has set out its vision for Automated Decision-Making (ADM) and AI, which must be reliable and human-centred. In particular we are interested on the Right to Explanation, that requires industry to produce explanations of ADM. The High-Level Expert Group on Artificial Intelligence (AI-HLEG), set up to support the implementation of this vision, has produced guidelines discussing the types of explanations that are appropriate for user-centred (interactive) Explanatory Tools. In this paper we propose our version of Explanatory Narratives (EN), based on user-centred concepts drawn from ISO 9241, as a model for user-centred explanations aligned with the GDPR and the AI-HLEG guidelines. Through the use of ENs we convert the problem of generating explanations for ADM into the identification of an appropriate path over an Explanatory Space, allowing explainees to interactively explore it and produce the explanation best suited to their needs. To this end we list suitable exploration heuristics, we study the properties and structure of explanations, and discuss the proposed model identifying its weaknesses and strengths.},
	language  = {en},
	booktitle = {Electronic {Government} and the {Information} {Systems} {Perspective}},
	publisher = {Springer International Publishing},
	author    = {Sovrano, Francesco and Vitali, Fabio and Palmirani, Monica},
	editor    = {Kő, Andrea and Francesconi, Enrico and Kotsis, Gabriele and Tjoa, A Min and Khalil, Ismail},
	year      = {2020},
	keywords  = {General Data Protection Regulation, Interactive explanatory tool, Trustworthy artificial intelligence},
	pages     = {219--233},
	file      = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/YCXVGH9Y/Sovrano et al. - 2020 - Modelling GDPR-Compliant Explanations for Trustwor.pdf:application/pdf}
}

@inproceedings{neerincx_using_2018,
	address   = {Cham},
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {Using {Perceptual} and {Cognitive} {Explanations} for {Enhanced} {Human}-{Agent} {Team} {Performance}},
	isbn      = {978-3-319-91122-9},
	doi       = {10.1007/978-3-319-91122-9_18},
	abstract  = {Most explainable AI (XAI) research projects focus on well-delineated topics, such as interpretability of machine learning outcomes, knowledge sharing in a multi-agent system or human trust in agent’s performance. For the development of explanations in human-agent teams, a more integrative approach is needed. This paper proposes a perceptual-cognitive explanation (PeCoX) framework for the development of explanations that address both the perceptual and cognitive foundations of an agent’s behavior, distinguishing between explanation generation, communication and reception. It is a generic framework (i.e., the core is domain-agnostic and the perceptual layer is model-agnostic), and being developed and tested in the domains of transport, health-care and defense. The perceptual level entails the provision of an Intuitive Confidence Measure and the identification of the “foil” in a contrastive explanation. The cognitive level entails the selection of the beliefs, goals and emotions for explanations. Ontology Design Patterns are being constructed for the reasoning and communication, whereas Interaction Design Patterns are being constructed for the shaping of the multimodal communication. First results show (1) positive effects on human’s understanding of the perceptual and cognitive foundation of agent’s behavior, and (2) the need for harmonizing the explanations to the context and human’s information processing capabilities.},
	language  = {en},
	booktitle = {Engineering {Psychology} and {Cognitive} {Ergonomics}},
	publisher = {Springer International Publishing},
	author    = {Neerincx, Mark A. and van der Waa, Jasper and Kaptein, Frank and van Diggelen, Jurriaan},
	editor    = {Harris, Don},
	year      = {2018},
	keywords  = {Cognitive engineering, Design patterns, Explainable AI, Human-agent teamwork, Ontologies},
	pages     = {204--214},
	file      = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/WAPJBCEF/Neerincx et al. - 2018 - Using Perceptual and Cognitive Explanations for En.pdf:application/pdf}
}

@article{nunes_systematic_2017,
	title    = {A systematic review and taxonomy of explanations in decision support and recommender systems},
	volume   = {27},
	issn     = {1573-1391},
	url      = {https://doi.org/10.1007/s11257-017-9195-0},
	doi      = {10.1007/s11257-017-9195-0},
	abstract = {With the recent advances in the field of artificial intelligence, an increasing number of decision-making tasks are delegated to software systems. A key requirement for the success and adoption of such systems is that users must trust system choices or even fully automated decisions. To achieve this, explanation facilities have been widely investigated as a means of establishing trust in these systems since the early years of expert systems. With today’s increasingly sophisticated machine learning algorithms, new challenges in the context of explanations, accountability, and trust towards such systems constantly arise. In this work, we systematically review the literature on explanations in advice-giving systems. This is a family of systems that includes recommender systems, which is one of the most successful classes of advice-giving software in practice. We investigate the purposes of explanations as well as how they are generated, presented to users, and evaluated. As a result, we derive a novel comprehensive taxonomy of aspects to be considered when designing explanation facilities for current and future decision support systems. The taxonomy includes a variety of different facets, such as explanation objective, responsiveness, content and presentation. Moreover, we identified several challenges that remain unaddressed so far, for example related to fine-grained issues associated with the presentation of explanations and how explanation facilities are evaluated.},
	language = {en},
	number   = {3},
	urldate  = {2021-05-24},
	journal  = {User Modeling and User-Adapted Interaction},
	author   = {Nunes, Ingrid and Jannach, Dietmar},
	month    = dec,
	year     = {2017},
	keywords = {Explanation: Design, Explanation: Purpose},
	pages    = {393--444},
	file     = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/L98FBGB5/Nunes and Jannach - 2017 - A systematic review and taxonomy of explanations i.pdf:application/pdf}
}

@inproceedings{schrills_color_2020,
	address   = {Cham},
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {Color for {Characters} - {Effects} of {Visual} {Explanations} of {AI} on {Trust} and {Observability}},
	isbn      = {978-3-030-50334-5},
	doi       = {10.1007/978-3-030-50334-5_8},
	abstract  = {The present study investigates the effects of prototypical visualization approaches aimed at increasing the explainability of machine learning systems in regard to perceived trustworthiness and observability. As the amount of processes automated by artificial intelligence (AI) increases, so does the need to investigate users’ perception. Previous research on explainable AI (XAI) tends to focus on technological optimization. The limited amount of empirical user research leaves key questions unanswered, such as which XAI designs actually improve perceived trustworthiness and observability. We assessed three different visual explanation approaches, consisting of either only a table with classification scores used for classification, or, additionally, one of two different backtraced visual explanations. In a within-subjects design with N = 83 we examined the effects on trust and observability in an online experiment. While observability benefitted from visual explanations, information-rich explanations also led to decreased trust. Explanations can support human-AI interaction, but differentiated effects on trust and observability have to be expected. The suitability of different explanatory approaches for individual AI applications should be further examined to ensure a high level of trust and observability in e.g. automated image processing.},
	language  = {en},
	booktitle = {Artificial {Intelligence} in {HCI}},
	publisher = {Springer International Publishing},
	author    = {Schrills, Tim and Franke, Thomas},
	editor    = {Degen, Helmut and Reinerman-Jones, Lauren},
	year      = {2020},
	keywords  = {Explainable AI, Explanation: Design, Human-AI interaction, Human-automation interaction, Machine learning, Trust in Automation},
	pages     = {121--135},
	file      = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/TFPHECEI/Schrills and Franke - 2020 - Color for Characters - Effects of Visual Explanati.pdf:application/pdf}
}

@inproceedings{sassoon_explainable_2019,
	address   = {Cham},
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {Explainable {Argumentation} for {Wellness} {Consultation}},
	isbn      = {978-3-030-30391-4},
	doi       = {10.1007/978-3-030-30391-4_11},
	abstract  = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers’ intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
	language  = {en},
	booktitle = {Explainable, {Transparent} {Autonomous} {Agents} and {Multi}-{Agent} {Systems}},
	publisher = {Springer International Publishing},
	author    = {Sassoon, Isabel and Kökciyan, Nadin and Sklar, Elizabeth and Parsons, Simon},
	editor    = {Calvaresi, Davide and Najjar, Amro and Schumacher, Michael and Främling, Kary},
	year      = {2019},
	keywords  = {Explainability, Explainable AI, Explanation, Interpretability, Transparency},
	pages     = {186--202},
	file      = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/VNGF5GRG/Sassoon et al. - 2019 - Explainable Argumentation for Wellness Consultatio.pdf:application/pdf;Springer Full Text PDF:/Users/fonok3/Zotero/storage/9U62JCSC/Sassoon et al. - 2019 - Explainable Argumentation for Wellness Consultatio.pdf:application/pdf}
}

@inproceedings{zhu_effects_2020,
	address   = {Cham},
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {Effects of {Proactive} {Explanations} by {Robots} on {Human}-{Robot} {Trust}},
	isbn      = {978-3-030-62056-1},
	doi       = {10.1007/978-3-030-62056-1_8},
	abstract  = {The performance of human-robot teams depends on human-robot trust, which in turn depends on appropriate robot-to-human transparency. A key way for robots to build trust through transparency is by providing appropriate explanations for their actions. While most previous work on robot explanation generation has focused on robots’ ability to provide post-hoc explanations upon request, in this paper we instead examine proactive explanations generated before actions are taken, and the effect this has on human-robot trust. Our results suggest a positive relationship between proactive explanations and human-robot trust, and reveal fundamental new questions into the effects of proactive explanations on the nature of humans’ mental models and the fundamental nature of human-robot trust.},
	language  = {en},
	booktitle = {Social {Robotics}},
	publisher = {Springer International Publishing},
	author    = {Zhu, Lixiao and Williams, Thomas},
	editor    = {Wagner, Alan R. and Feil-Seifer, David and Haring, Kerstin S. and Rossi, Silvia and Williams, Thomas and He, Hongsheng and Sam Ge, Shuzhi},
	year      = {2020},
	keywords  = {Explanation, Explanation: Kind, Human-robot interaction, Human-robot trust, Transparency},
	pages     = {85--95},
	file      = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/ECI97NTS/Zhu and Williams - 2020 - Effects of Proactive Explanations by Robots on Hum.pdf:application/pdf}
}

@inproceedings{goram_supporting_2019,
	address   = {Cham},
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {Supporting {Privacy} {Control} and {Personalized} {Data} {Usage} {Explanations} in a {Context}-{Based} {Adaptive} {Collaboration} {Environment}},
	isbn      = {978-3-030-34974-5},
	doi       = {10.1007/978-3-030-34974-5_8},
	abstract  = {The General Data Protection Regulation, e.g., provides the “right of access by the data subject” and demands explanations of data usages, i.e. explanations where and for what purpose personal data is being processed. Supporting this kind of privacy control and related personalized explanations of data usage in context-based adaptive collaboration environments are big challenges. Currently, users cannot retrace the usage and the storage of their personal data in context-based adaptive collaboration environments. We address the aforementioned challenges by developing a context-based adaptive collaboration platform, the CONTact platform, that can be linked to or integrated into different kinds of collaboration environments (e.g., meinDorf55+, a novel community support system for elderly). The CONTact platform supports users with privacy control and personalized explanations of data usages. In this paper we present an excerpt of our extended domain model and two sample situations when privacy control and personalized explanations get relevant. We use a sample ontology that is based on our domain model to illustrate the related processes and rules. Using our approach users can control their data usage and are able to get personalized explanations of their data usage in a context-based adaptive collaboration environment. This helps us observing legal regulations, e.g. privacy laws like the GDPR.},
	language  = {en},
	booktitle = {Modeling and {Using} {Context}},
	publisher = {Springer International Publishing},
	author    = {Goram, Mandy and Veiel, Dirk},
	editor    = {Bella, Gábor and Bouquet, Paolo},
	year      = {2019},
	keywords  = {Adaptive, Collaboration environment, Context-based, GDPR, Legal regulations, Personalized explanations, Privacy control},
	pages     = {84--97},
	file      = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/7A2YIFR9/Goram and Veiel - 2019 - Supporting Privacy Control and Personalized Data U.pdf:application/pdf}
}

@inproceedings{wang_is_2018,
	address    = {Cham},
	series     = {Lecture {Notes} in {Computer} {Science}},
	title      = {Is {It} {My} {Looks}? {Or} {Something} {I} {Said}? {The} {Impact} of {Explanations}, {Embodiment}, and {Expectations} on {Trust} and {Performance} in {Human}-{Robot} {Teams}},
	isbn       = {978-3-319-78978-1},
	shorttitle = {Is {It} {My} {Looks}?},
	doi        = {10.1007/978-3-319-78978-1_5},
	abstract   = {Trust is critical to the success of human-robot interaction. Research has shown that people will more accurately trust a robot if they have an accurate understanding of its decision-making process. The Partially Observable Markov Decision Process (POMDP) is one such decision-making process, but its quantitative reasoning is typically opaque to people. This lack of transparency is exacerbated when a robot can learn, making its decision making better, but also less predictable. Recent research has shown promise in calibrating human-robot trust by automatically generating explanations of POMDP-based decisions. In this work, we explore factors that can potentially interact with such explanations in influencing human decision-making in human-robot teams. We focus on explanations with quantitative expressions of uncertainty and experiment with common design factors of a robot: its embodiment and its communication strategy in case of an error. Results help us identify valuable properties and dynamics of the human-robot trust relationship.},
	language   = {en},
	booktitle  = {Persuasive {Technology}},
	publisher  = {Springer International Publishing},
	author     = {Wang, Ning and Pynadath, David V. and Rovira, Ericka and Barnes, Michael J. and Hill, Susan G.},
	editor     = {Ham, Jaap and Karapanos, Evangelos and Morita, Plinio P. and Burns, Catherine M.},
	year       = {2018},
	pages      = {56--69},
	file       = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/4LU3PHEF/Wang et al. - 2018 - Is It My Looks Or Something I Said The Impact of.pdf:application/pdf}
}

@inproceedings{vivacqua_explanations_2019,
	address   = {Panama City Panama},
	title     = {Explanations and sensemaking with {AI} and {HCI}},
	isbn      = {978-1-4503-7679-2},
	url       = {https://dl.acm.org/doi/10.1145/3358961.3359004},
	doi       = {10.1145/3358961.3359004},
	language  = {en},
	urldate   = {2021-05-24},
	booktitle = {Proceedings of the {IX} {Latin} {American} {Conference} on {Human} {Computer} {Interaction}},
	publisher = {ACM},
	author    = {Vivacqua, Adriana S and Stelling, Roberto and Garcia, Ana Cristina B and Gouvea, Livia C},
	month     = sep,
	year      = {2019},
	keywords  = {artificial intelligence, explainability, Explanation: Design, human computer interaction, human-centered AI, information visualization, machine learning},
	pages     = {1--4},
	file      = {Vivacqua et al. - 2019 - Explanations and sensemaking with AI and HCI.pdf:/Users/fonok3/Zotero/storage/LUJJAAYT/Vivacqua et al. - 2019 - Explanations and sensemaking with AI and HCI.pdf:application/pdf}
}

@inproceedings{wiegand_id_2020,
	address   = {Oldenburg Germany},
	title     = {“{I}’d like an {Explanation} for {That}!”{Exploring} {Reactions} to {Unexpected} {Autonomous} {Driving}},
	isbn      = {978-1-4503-7516-0},
	url       = {https://dl.acm.org/doi/10.1145/3379503.3403554},
	doi       = {10.1145/3379503.3403554},
	abstract  = {Autonomous vehicles are complex systems that may behave in unexpected ways. From the drivers’ perspective, this can cause stress and lower trust and acceptance of autonomous driving. Prior work has shown that explanation of system behavior can mitigate these negative effects. Nevertheless, it remains unclear in which situations drivers actually need an explanation and what kind of interaction is relevant to them. Using thematic analysis of real-world experience reports, we first identified 17 situations in which a vehicle behaved unexpectedly. We then conducted a think-aloud study (N = 26) in a driving simulator to validate these situations and enrich them with qualitative insights about drivers’ need for explanation. We identified six categories to describe the main concerns and topics during unexpected driving behavior (emotion and evaluation, interpretation and reason, vehicle capability, interaction, future driving prediction and explanation request times). Based on these categories, we suggest design implications for autonomous vehicles, in particular related to collaboration insights, user mental models and explanation requests.},
	language  = {en},
	urldate   = {2021-05-24},
	booktitle = {22nd {International} {Conference} on {Human}-{Computer} {Interaction} with {Mobile} {Devices} and {Services}},
	publisher = {ACM},
	author    = {Wiegand, Gesa and Eiband, Malin and Haubelt, Maximilian and Hussmann, Heinrich},
	month     = oct,
	year      = {2020},
	keywords  = {Autonomous driving., Explainable AI, Explanation: Interaction?, Explanation: When?, Unexpected driving behavior},
	pages     = {1--11},
	file      = {Wiegand et al. - 2020 - “I’d like an Explanation for That!”Exploring React.pdf:/Users/fonok3/Zotero/storage/L88J6WLD/Wiegand et al. - 2020 - “I’d like an Explanation for That!”Exploring React.pdf:application/pdf}
}

@inproceedings{zahedi_towards_2019,
	address   = {Daegu, Korea (South)},
	title     = {Towards {Understanding} {User} {Preferences} for {Explanation} {Types} in {Model} {Reconciliation}},
	isbn      = {978-1-5386-8555-6},
	url       = {https://ieeexplore.ieee.org/document/8673097/},
	doi       = {10.1109/HRI.2019.8673097},
	abstract  = {Recent work has formalized the explanation process in the context of automated planning as one of model reconciliation – i.e. a process by which the planning agent can bring the explainee’s (possibly faulty) model of a planning problem closer to its understanding of the ground truth until both agree that its plan is the best possible. The content of explanations can thus range from misunderstandings about the agent’s beliefs (state), desires (goals) and capabilities (action model). Though existing literature has considered different kinds of these model differences to be equivalent, literature on the explanations in social sciences has suggested that explanations with similar logical properties may often be perceived differently by humans. In this brief report, we explore to what extent humans attribute importance to different kinds of model differences that have been traditionally considered equivalent in the model reconciliation setting. Our results suggest that people prefer the explanations which are related to the effects of actions.},
	language  = {en},
	urldate   = {2021-05-24},
	booktitle = {2019 14th {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction} ({HRI})},
	publisher = {IEEE},
	author    = {Zahedi, Zahra and Olmo, Alberto and Chakraborti, Tathagata and Sreedharan, Sarath and Kambhampati, Subbarao},
	month     = mar,
	year      = {2019},
	keywords  = {Artificial intelligence, Cognitive science, Computational modeling, Context modeling, Logistics, Planning},
	pages     = {648--649},
	file      = {Zahedi et al. - 2019 - Towards Understanding User Preferences for Explana.pdf:/Users/fonok3/Zotero/storage/X97ZZ7S2/Zahedi et al. - 2019 - Towards Understanding User Preferences for Explana.pdf:application/pdf}
}

@inproceedings{yamada_evaluating_2016,
	address   = {Kumamoto, Japan},
	title     = {Evaluating {Explanation} {Function} in {Railway} {Crew} {Rescheduling} {System} by {Think}-{Aloud} {Test}},
	isbn      = {978-1-4673-8985-3},
	url       = {http://ieeexplore.ieee.org/document/7557757/},
	doi       = {10.1109/IIAI-AAI.2016.93},
	abstract  = {A previously developed interactive system with an explanation function generates crew schedules automatically by using a rule base in an ‘if-then-because’ format. Delivering ‘because’ information to the user is helpful for decision making because it visualizes the computing process. We evaluated the effectiveness of the explanation function by think-aloud testing in which the verbal comments of each participant were recorded while the participant was solving a pre-specified problem of crew rescheduling in our system. A cognitive model for using the system was obtained in a user state transition form by protocol analysis of the comments. The results revealed that users spent 28\% of their time for interpreting ‘because’ information and this information played an important role in the decision making. Interactive processing with the help of the explanation function is practical for real-time railway crew rescheduling.},
	language  = {en},
	urldate   = {2021-05-24},
	booktitle = {2016 5th {IIAI} {International} {Congress} on {Advanced} {Applied} {Informatics} ({IIAI}-{AAI})},
	publisher = {IEEE},
	author    = {Yamada, Takaaki and Sato, Tatsuhiro and Tomiyama, Tomoe and Ueki, Nobutaka},
	month     = jul,
	year      = {2016},
	keywords  = {Decision making, explanation function, interaction, Interactive systems, Rail transportation, railway crew, Railway engineering, Real-time systems, Schedules, scheduling, Testing, think-aloud},
	pages     = {991--994},
	file      = {Yamada et al. - 2016 - Evaluating Explanation Function in Railway Crew Re.pdf:/Users/fonok3/Zotero/storage/QMK7PELY/Yamada et al. - 2016 - Evaluating Explanation Function in Railway Crew Re.pdf:application/pdf}
}

@article{chi_three_nodate,
	title    = {Three {Types} of {Conceptual} {Change}: {Belief} {Revision}, {Mental} {Model} {Transformation}, and {Categorical} {Shift}},
	language = {en},
	author   = {Chi, Michelene T H},
	pages    = {22},
	file     = {Chi - Three Types of Conceptual Change Belief Revision,.pdf:/Users/fonok3/Zotero/storage/YKYZB7L4/Chi - Three Types of Conceptual Change Belief Revision,.pdf:application/pdf}
}

@inproceedings{ribera2019can,
	title     = {Can we do better explanations? A proposal of user-centered explainable AI.},
	author    = {Ribera, Mireia and Lapedriza, Agata},
	booktitle = {IUI Workshops},
	year      = {2019}
}

@inproceedings{wiegand2019drive,
	title     = {I drive-you trust: Explaining driving behavior of autonomous cars},
	author    = {Wiegand, Gesa and Schmidmaier, Matthias and Weber, Thomas and Liu, Yuanting and Hussmann, Heinrich},
	booktitle = {Extended abstracts of the 2019 chi conference on human factors in computing systems},
	pages     = {1--6},
	year      = {2019}
}

@article{miller2017explainable,
	title   = {Explainable AI: Beware of inmates running the asylum or: How I learnt to stop worrying and love the social and behavioural sciences},
	author  = {Miller, Tim and Howe, Piers and Sonenberg, Liz},
	journal = {arXiv preprint arXiv:1712.00547},
	year    = {2017}
}

@article{jaimes2007guest,
	title     = {HumanCentered Computing: Toward a Human Revolution},
	author    = {Jaimes, Alejandro and Gatica-Perez, Daniel and Sebe, Nicu and Huang, Thomas S},
	journal   = {Computer},
	volume    = {40},
	number    = {5},
	pages     = {30--34},
	year      = {2007},
	publisher = {IEEE}
}

@inproceedings{bilgic2005explaining,
	title     = {Explaining recommendations: Satisfaction vs. promotion},
	author    = {Bilgic, Mustafa and Mooney, Raymond J},
	booktitle = {Beyond Personalization Workshop, IUI},
	volume    = {5},
	pages     = {153},
	year      = {2005}
}

@article{chazette_knowledge_nodate,
	title    = {A {Knowledge} {Catalogue} for {Explainability}: {Deﬁnitions}, {Impacts}, and {Dimensions}},
	abstract = {The growing complexity of software systems and the inﬂuence of software-supported decisions in our society awoke the need for software that is transparent, accountable, and trustable. Explainability has been identiﬁed as a means to achieve these qualities. It is recognized as an emerging non-functional requirement (NFR) that has a signiﬁcant impact on system quality. However, in order to incorporate this NFR into systems, we need to understand what explainability means from a software engineering perspective and how it impacts other quality aspects in a system. This allows for an early analysis of the beneﬁts and possible design issues that arise from interrelationships between different quality aspects. Nevertheless, explainability is currently under-researched in the domain of requirements engineering and there is a lack of conceptual models and knowledge catalogues that support the requirements engineering process and system design. In this work, we bridge this gap by proposing a deﬁnition, a model, and a catalogue for explainability. They illustrate how explainability interacts with other quality aspects and how it may impact various system dimensions. For this purpose, we have conducted an interdisciplinary Systematic Literature Review and validated our ﬁndings with experts in workshops.},
	language = {en},
	year     = {2021},
	author   = {Chazette, Larissa and Brunotte, Wasja and Speith, Timo},
	keywords = {Explainability, Explanations, Interpretability, Non-Functional Requirements, Quality Aspects, Requirements Synergy, Software Transparency},
	pages    = {12}
}

@article{gunning2019darpa,
	title   = {DARPA’s explainable artificial intelligence (XAI) program},
	author  = {Gunning, David and Aha, David},
	journal = {AI Magazine},
	volume  = {40},
	number  = {2},
	pages   = {44--58},
	year    = {2019}
}

@article{doshi2017towards,
	title   = {Towards a rigorous science of interpretable machine learning},
	author  = {Doshi-Velez, Finale and Kim, Been},
	journal = {arXiv preprint arXiv:1702.08608},
	year    = {2017}
}

@article{miller2019explanation,
	title     = {Explanation in artificial intelligence: Insights from the social sciences},
	author    = {Miller, Tim},
	journal   = {Artificial intelligence},
	volume    = {267},
	pages     = {1--38},
	year      = {2019},
	publisher = {Elsevier}
}

@article{Pearl_causal_2009,
	author    = {Judea Pearl},
	title     = {{Causal inference in statistics: An overview}},
	volume    = {3},
	journal   = {Statistics Surveys},
	number    = {none},
	publisher = {Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the Statist. Soc. Canada},
	pages     = {96 -- 146},
	keywords  = {causal effects, causes of effects, confounding, counterfactuals, graphical methods, mediation, policy evaluation, potential-outcome, structural equation models},
	year      = {2009},
	doi       = {10.1214/09-SS057},
	url       = {https://doi.org/10.1214/09-SS057}
}

@inproceedings{lim_2009_assessing,
	title     = {Assessing demand for intelligibility in context-aware applications},
	author    = {Lim, Brian Y and Dey, Anind K},
	booktitle = {Proceedings of the 11th international conference on Ubiquitous computing},
	pages     = {195--204},
	year      = {2009}
}

@article{hoffman2018metrics,
	title   = {Metrics for explainable AI: Challenges and prospects},
	author  = {Hoffman, Robert R and Mueller, Shane T and Klein, Gary and Litman, Jordan},
	journal = {arXiv preprint arXiv:1812.04608},
	year    = {2018}
}

@inproceedings{cheng2019explaining,
	title     = {Explaining decision-making algorithms through UI: Strategies to help non-expert stakeholders},
	author    = {Cheng, Hao-Fei and Wang, Ruotong and Zhang, Zheng and O'Connell, Fiona and Gray, Terrance and Harper, F Maxwell and Zhu, Haiyi},
	booktitle = {Proceedings of the 2019 chi conference on human factors in computing systems},
	pages     = {1--12},
	year      = {2019}
}

@inproceedings{biran2014justification,
	title     = {Justification narratives for individual classifications},
	author    = {Biran, Or and McKeown, Kathleen},
	booktitle = {Proceedings of the AutoML workshop at ICML},
	volume    = {2014},
	pages     = {1--7},
	year      = {2014}
}

@article{chakraborti2017visualizations,
	title   = {Visualizations for an explainable planning agent},
	author  = {Chakraborti, Tathagata and Fadnis, Kshitij P and Talamadupula, Kartik and Dholakia, Mishal and Srivastava, Biplav and Kephart, Jeffrey O and Bellamy, Rachel KE},
	journal = {arXiv preprint arXiv:1709.04517},
	year    = {2017}
}

@article{koo_understanding_2016,
	title    = {Understanding driver responses to voice alerts of autonomous car operations},
	volume   = {70},
	issn     = {0143-3369, 1741-5314},
	url      = {http://www.inderscience.com/link.php?id=76740},
	doi      = {10.1504/IJVD.2016.076740},
	abstract = {This study explores, in the context of automated braking, how a voice alert accompanying the car’s autonomous action affects the driver’s attitude and driving behaviour. To examine the research question we designed an experimental setup in a simulator environment that (1) enabled automatic braking to perform as a vehicle’s autonomous longitudinal behaviour and (2) enacted a voice alert system in a timely way to notify the driver of pending brake actions. Subjective driving experience and driver responses towards the car were strongly affected by the voice alerts when the car made automated decisions. These results have important implications for the design of vehicle–user interfaces, suggesting that, rather than simply developing a car that executes autonomous decisions, car makers should also focus on the human–machine interaction, i.e., on how the car announces its ‘intentions’ to act.},
	language = {en},
	number   = {4},
	urldate  = {2021-05-26},
	journal  = {International Journal of Vehicle Design},
	author   = {Koo, Jeamin and Shin, Dongjun and Steinert, Martin and Leifer, Larry},
	year     = {2016},
	pages    = {377},
	file     = {Koo et al. - 2016 - Understanding driver responses to voice alerts of .pdf:/Users/fonok3/Zotero/storage/253NSTLP/Koo et al. - 2016 - Understanding driver responses to voice alerts of .pdf:application/pdf}
}

@article{koo_why_2015,
	title      = {Why did my car just do that? {Explaining} semi-autonomous driving actions to improve driver understanding, trust, and performance},
	volume     = {9},
	issn       = {1955-2513, 1955-2505},
	shorttitle = {Why did my car just do that?},
	url        = {http://link.springer.com/10.1007/s12008-014-0227-2},
	doi        = {10.1007/s12008-014-0227-2},
	abstract   = {This study explores, in the context of semiautonomous driving, how the content of the verbalized message accompanying the car’s autonomous action affects the driver’s attitude and safety performance. Using a driving simulator with an auto-braking function, we tested different messages that provided advance explanation of the car’s imminent autonomous action. Messages providing only “how” information describing actions (e.g., “The car is braking”) led to poor driving performance, whereas “why” information describing reasoning for actions (e.g., “Obstacle ahead”) was preferred by drivers and led to better driving performance. Providing both “how and why” resulted in the safest driving performance but increased negative feelings in drivers. These results suggest that, to increase overall safety, car makers need to attend not only to the design of autonomous actions but also to the right way to explain these actions to the drivers.},
	language   = {en},
	number     = {4},
	urldate    = {2021-05-26},
	journal    = {International Journal on Interactive Design and Manufacturing (IJIDeM)},
	author     = {Koo, Jeamin and Kwac, Jungsuk and Ju, Wendy and Steinert, Martin and Leifer, Larry and Nass, Clifford},
	month      = nov,
	year       = {2015},
	pages      = {269--275},
	file       = {Koo et al. - 2015 - Why did my car just do that Explaining semi-auton.pdf:/Users/fonok3/Zotero/storage/QJF4TM4F/Koo et al. - 2015 - Why did my car just do that Explaining semi-auton.pdf:application/pdf}
}

@inproceedings{eiband2018bringing,
	title     = {Bringing transparency design into practice},
	author    = {Eiband, Malin and Schneider, Hanna and Bilandzic, Mark and Fazekas-Con, Julian and Haug, Mareike and Hussmann, Heinrich},
	booktitle = {23rd international conference on intelligent user interfaces},
	pages     = {211--223},
	year      = {2018}
}


@article{hoffman_metrics_nodate,
	title    = {Metrics for {Explainable} {AI}: {Challenges} and {Prospects}},
	abstract = {The question addressed in this paper is: If we present to a user an AI system that explains how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? In other words, how do we know that an explanainable AI system (XAI) is any good? Our focus is on the key concepts of measurement. We discuss specific methods for evaluating: (1) the goodness of explanations, (2) whether users are satisfied by explanations, (3) how well users understand the AI systems, (4) how curiosity motivates the search for explanations, (5) whether the user's trust and reliance on the AI are appropriate, and finally, (6) how the human-XAI work system performs. The recommendations we present derive from our integration of extensive research literatures and our own psychometric evaluations.},
	language = {en},
	author   = {Hoffman, Robert R and Mueller, Shane T and Klein, Gary and Litman, Jordan},
	pages    = {50},
	file     = {Hoffman et al. - Metrics for Explainable AI Challenges and Prospec.pdf:/Users/fonok3/Zotero/storage/AIKTD4ZQ/Hoffman et al. - Metrics for Explainable AI Challenges and Prospec.pdf:application/pdf}
}

@article{tintarev_designing_nodate,
	title    = {Designing and {Evaluating} {Explanations} for {Recommender} {Systems}},
	abstract = {This chapter gives an overview of the area of explanations in recommender systems. We approach the literature from the angle of evaluation: that is, we are interested in what makes an explanation “good”, and suggest guidelines as how to best evaluate this. We identify seven beneﬁts that explanations may contribute to a recommender system, and relate them to criteria used in evaluations of explanations in existing systems, and how these relate to evaluations with live recommender systems. We also discuss how explanations can be affected by how recommendations are presented, and the role the interaction with the recommender system plays w.r.t. explanations. Finally, we describe a number of explanation styles, and how they may be related to the underlying algorithms. Examples of explanations in existing systems are mentioned throughout.},
	language = {en},
	author   = {Tintarev, Nava and Masthoff, Judith},
	pages    = {32},
	file     = {Designing and Evaluating Explanations for Recommender Systems.pdf:/Users/fonok3/Zotero/storage/24TQCDRK/Designing and Evaluating Explanations for Recommender Systems.pdf:application/pdf},
	year     = {2010}
}


@article{sato_context_nodate,
	title  = {Context {Style} {Explanation} for {Recommender} {Systems}},
	doi    = {10.2197/ipsjjip.27.720},
	author = {Sato, Masahiro and Nagatani, Koki and Sonoda, Takashi and Zhang, Qian and Ohkuma, Tomoko},
	file   = {Context Style Explanation for Recommender Systems.pdf:/Users/fonok3/Zotero/storage/4DRRL2YT/Context Style Explanation for Recommender Systems.pdf:application/pdf}
}

@inproceedings{tintarev2007survey,
	title        = {A survey of explanations in recommender systems},
	author       = {Tintarev, Nava and Masthoff, Judith},
	booktitle    = {2007 IEEE 23rd international conference on data engineering workshop},
	pages        = {801--810},
	year         = {2007},
	organization = {IEEE}
}

@book{wohlin2012experimentation,
	title     = {Experimentation in software engineering},
	author    = {Wohlin, Claes and Runeson, Per and H{\"o}st, Martin and Ohlsson, Magnus C and Regnell, Bj{\"o}rn and Wessl{\'e}n, Anders},
	year      = {2012},
	publisher = {Springer Science \& Business Media}
}

@inproceedings{ghazi2016exploratory,
	title        = {An exploratory study on user interaction challenges when handling interconnected requirements artifacts of various sizes},
	author       = {Ghazi, Parisa and Glinz, Martin},
	booktitle    = {2016 IEEE 24th International Requirements Engineering Conference (RE)},
	pages        = {76--85},
	year         = {2016},
	organization = {IEEE}
}

@inproceedings{carvalho2020developers,
	title        = {How developers believe Invisibility impacts NFRs related to User Interaction},
	author       = {Carvalho, Rainara Maia and Andrade, Rossana MC and Oliveira, K{\'a}thia M},
	booktitle    = {2020 IEEE 28th International Requirements Engineering Conference (RE)},
	pages        = {102--112},
	year         = {2020},
	organization = {IEEE}
}

@article{do2010software,
	title     = {Software transparency},
	author    = {do Prado Leite, Julio Cesar Sampaio and Cappelli, Claudia},
	journal   = {Business \& Information Systems Engineering},
	volume    = {2},
	number    = {3},
	pages     = {127--139},
	year      = {2010},
	publisher = {Springer}
}

@misc{international2011iso,
	title     = {ISO/IEC 25010: Systems and software engineering-systems and Software Quality Requirements and Evaluation (SQuaRE)},
	author    = {INTERNATIONAL ORGANIZATION FOR STANDARDIZATION},
	year      = {2011},
	publisher = {System and software quality models Geneva}
}

@article{chazette2020explainability,
	title     = {Explainability as a non-functional requirement: challenges and recommendations},
	author    = {Chazette, Larissa and Schneider, Kurt},
	journal   = {Requirements Engineering},
	volume    = {25},
	number    = {4},
	pages     = {493--514},
	year      = {2020},
	publisher = {Springer}
}

@phdthesis{wang_integration_2020,
	type     = {Bachelor},
	title    = {Integration and {Evaluation} of {Explanations} in the {Context} of a {Navigation} {App}},
	language = {en},
	school   = {Leibniz University Hanover},
	author   = {Wang, Zhongpin},
	month    = jul,
	year     = {2020}
}

@book{schneider2012abenteuer,
	title     = {Abenteuer Softwarequalit{\"a}t: Grundlagen und Verfahren f{\"u}r Qualit{\"a}tssicherung und Qualit{\"a}tsmanagement},
	author    = {Schneider, Kurt},
	year      = {2012},
	publisher = {dpunkt. verlag}
}

@article{briand1995goal,
	title  = {Goal-driven definition of product metrics based on properties},
	author = {Briand, Lionel and Morasca, Sandro and Basili, Victor R},
	year   = {1995}
}

@inproceedings{anjomshoae2019explainable,
	title        = {Explainable agents and robots: Results from a systematic literature review},
	author       = {Anjomshoae, Sule and Najjar, Amro and Calvaresi, Davide and Fr{\"a}mling, Kary},
	booktitle    = {18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019), Montreal, Canada, May 13--17, 2019},
	pages        = {1078--1088},
	year         = {2019},
	organization = {International Foundation for Autonomous Agents and Multiagent Systems}
}

@book{golledge1999wayfinding,
	title     = {Wayfinding behavior: Cognitive mapping and other spatial processes},
	author    = {Golledge, Reginald G and others},
	year      = {1999},
	publisher = {JHU press}
}

@book{bovy2012route,
	title     = {Route Choice: Wayfinding in Transport Networks: Wayfinding in Transport Networks},
	author    = {Bovy, Piet H and Stern, Eliahu},
	volume    = {9},
	year      = {2012},
	publisher = {Springer Science \& Business Media}
}

@article{rajnish2010quality,
	author  = {Rajnish, Ranjana and Dev, Prof and Rajnish, Vyas},
	year    = {2010},
	month   = {06},
	pages   = {},
	title   = {Writing Quality Requirements (SRS): An Approach To Manage Requirements Volatility},
	volume  = {1},
	journal = {Indian Journal of Computer Science and Engineering}
}

@book{alexander2002writing,
	title     = {Writing better requirements},
	author    = {Alexander, Ian F and Stevens, Richard},
	year      = {2002},
	publisher = {Pearson Education}
}

@article{wiegers1999writing,
	title     = {Writing quality requirements},
	author    = {Wiegers, Karl E},
	journal   = {Software Development},
	volume    = {7},
	number    = {5},
	pages     = {44--48},
	year      = {1999},
	publisher = {{Miller Freeman, Inc. Lawrence, KS, USA}}
}

@incollection{tintarev2015explaining,
	title     = {Explaining recommendations: Design and evaluation},
	author    = {Tintarev, Nava and Masthoff, Judith},
	booktitle = {Recommender systems handbook},
	pages     = {353--382},
	year      = {2015},
	publisher = {Springer}
}

@article{knijnenburg2012explaining,
	title     = {Explaining the user experience of recommender systems},
	author    = {Knijnenburg, Bart P and Willemsen, Martijn C and Gantner, Zeno and Soncu, Hakan and Newell, Chris},
	journal   = {User Modeling and User-Adapted Interaction},
	volume    = {22},
	number    = {4},
	pages     = {441--504},
	year      = {2012},
	publisher = {Springer}
}

@incollection{chung2009non,
	title     = {On non-functional requirements in software engineering},
	author    = {Chung, Lawrence and do Prado Leite, Julio Cesar Sampaio},
	booktitle = {Conceptual modeling: Foundations and applications},
	pages     = {363--379},
	year      = {2009},
	publisher = {Springer}
}

@book{hleg2019policy,
	author    = {{High-Level Expert Group on Artificial Intelligence}},
	title     = {Policy and investment recommendations for trustworthy AI},
	subtitle  = {A subtitle (optional)},
	year      = {2019},
	publisher = {The European Commission}
}

@article{mayer1999effect,
	title     = {The effect of the performance appraisal system on trust for management: A field quasi-experiment.},
	author    = {Mayer, Roger C and Davis, James H},
	journal   = {Journal of applied psychology},
	volume    = {84},
	number    = {1},
	pages     = {123},
	year      = {1999},
	publisher = {American Psychological Association}
}

@phdthesis{schaefer2013perception,
	title  = {The perception and measurement of human-robot trust},
	author = {Schaefer, Kristin},
	year   = {2013}
}

@book{norman1988psychology,
	title     = {The psychology of everyday things.},
	author    = {Norman, Donald A},
	year      = {1988},
	publisher = {Basic books}
}

@incollection{cypko2017guide,
	title     = {A guide for constructing bayesian network graphs of cancer treatment decisions},
	author    = {Cypko, Mario A and Stoehr, Matthaeus and Oeltze-Jafra, Steffen and Dietz, Andreas and Lemke, Heinz U},
	booktitle = {MEDINFO 2017: Precision Healthcare through Informatics},
	pages     = {1355--1355},
	year      = {2017},
	publisher = {IOS Press}
}

@inproceedings{cawsey1991generating,
	title        = {Generating Interactive Explanations.},
	author       = {Cawsey, Alison},
	booktitle    = {AAAI},
	pages        = {86--91},
	year         = {1991},
	organization = {Citeseer}
}

@incollection{byrne1991construction,
	title     = {The construction of explanations},
	author    = {Byrne, Ruth MJ},
	booktitle = {AI and Cognitive Science’90},
	pages     = {337--351},
	year      = {1991},
	publisher = {Springer}
}


@inproceedings{gilpin_explaining_2018,
	title     = {Explaining {Explanations}: {An} {Overview} of {Interpretability} of {Machine} {Learning}},
	doi       = {10.1109/DSAA.2018.00018},
	booktitle = {2018 {IEEE} 5th {International} {Conference} on {Data} {Science} and {Advanced} {Analytics} ({DSAA})},
	author    = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
	month     = oct,
	year      = {2018},
	keywords  = {Artificial intelligence, Taxonomy, Biological neural networks, Computational modeling, Decision trees, Complexity theory, Deep learning and deep analytics, Fairness and transparency in data science, Machine learning theories, Models and systems},
	pages     = {80--89},
	file      = {Gilpin et al. - 2018 - Explaining Explanations An Overview of Interpreta.pdf:/Users/fonok3/Zotero/storage/PVC4TBJS/Gilpin et al. - 2018 - Explaining Explanations An Overview of Interpreta.pdf:application/pdf}
}

@inproceedings{fong_interpretable_2017,
	title     = {Interpretable {Explanations} of {Black} {Boxes} by {Meaningful} {Perturbation}},
	doi       = {10.1109/ICCV.2017.371},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author    = {Fong, Ruth C. and Vedaldi, Andrea},
	month     = oct,
	year      = {2017},
	note      = {ISSN: 2380-7504},
	keywords  = {Neural networks, Visualization, Prediction algorithms, Perturbation methods, Machine learning algorithms, Gradient methods, Backpropagation},
	pages     = {3449--3457}
}

@incollection{samek_towards_2019,
	address   = {Cham},
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {Towards {Explainable} {Artificial} {Intelligence}},
	isbn      = {978-3-030-28954-6},
	url       = {https://doi.org/10.1007/978-3-030-28954-6_1},
	language  = {en},
	urldate   = {2021-05-24},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author    = {Samek, Wojciech and Müller, Klaus-Robert},
	editor    = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year      = {2019},
	doi       = {10.1007/978-3-030-28954-6_1},
	keywords  = {Interpretability, Explainable artificial intelligence, Deep learning, Neural networks, Model transparency},
	pages     = {5--22}
}
