
@inproceedings{chazette_end-users_nodate,
	title    = {Do {End}-{Users} {Want} {Explanations}? {Analyzing} the {Role} of {Explainability} as an {Emerging} {Aspect} of {Non}-{Functional} {Requirements}},
	language = {en},
	author   = {Chazette, Larissa and Karras, Oliver and Schneider, Kurt},
	keywords = {Decision making, Explanations, Interpretability, Measurement, Non-Functional Requirements, Privacy, Qualitative Evaluation, Software quality, Software systems, Software Transparency, Stakeholders},
	pages    = {11},
	year     = {2019}
}

@article{waa_evaluating_2021,
	title    = {Evaluating {XAI}: {A} comparison of rule-based and example-based explanations},
	volume   = {291},
	issn     = {0004-3702},
	url      = {https://www.sciencedirect.com/science/article/pii/S0004370220301533},
	doi      = {10.1016/j.artint.2020.103404},
	journal  = {Artificial Intelligence},
	author   = {Waa, Jasper van der and Nieuwburg, Elisabeth and Cremers, Anita and Neerincx, Mark},
	year     = {2021},
	keywords = {Artificial Intelligence (AI), Contrastive explanations, Decision support systems, Explainable Artificial Intelligence (XAI), Explanation: Kind, Machine learning, User evaluations},
	pages    = {103404}
}

@inproceedings{sokol_explainability_2020,
	address   = {New York, NY, USA},
	series    = {{FAT}* '20},
	title     = {Explainability {Fact} {Sheets}: {A} {Framework} for {Systematic} {Assessment} of {Explainable} {Approaches}},
	isbn      = {978-1-4503-6936-7},
	doi       = {10.1145/3351095.3372870},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author    = {Sokol, Kacper and Flach, Peter},
	year      = {2020},
	note      = {event-place: Barcelona, Spain},
	keywords  = {AI, desiderata, explainability, fact sheet, interpretability, ML, taxonomy, transparency, work sheet},
	pages     = {56--67},
	file      = {Submitted Version:/Users/fonok3/Zotero/storage/SUPGLGHM/Sokol and Flach - 2020 - Explainability Fact Sheets A Framework for System.pdf:application/pdf}
}

@inproceedings{balog_measuring_2020,
	address   = {New York, NY, USA},
	series    = {{SIGIR} '20},
	title     = {Measuring {Recommendation} {Explanation} {Quality}: {The} {Conflicting} {Goals} of {Explanations}},
	isbn      = {978-1-4503-8016-4},
	doi       = {10.1145/3397271.3401032},
	booktitle = {Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author    = {Balog, Krisztian and Radlinski, Filip},
	year      = {2020},
	note      = {event-place: Virtual Event, China},
	keywords  = {evaluation, explanations, recommendations},
	pages     = {329--338},
	file      = {Full Text:/Users/fonok3/Zotero/storage/5XA46XKX/Balog and Radlinski - 2020 - Measuring Recommendation Explanation Quality The .pdf:application/pdf}
}

@inproceedings{eiband_impact_2019,
	address   = {New York, NY, USA},
	series    = {{CHI} {EA} '19},
	title     = {The {Impact} of {Placebic} {Explanations} on {Trust} in {Intelligent} {Systems}},
	isbn      = {978-1-4503-5971-9},
	doi       = {10.1145/3290607.3312787},
	booktitle = {Extended {Abstracts} of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author    = {Eiband, Malin and Buschek, Daniel and Kremer, Alexander and Hussmann, Heinrich},
	year      = {2019},
	note      = {event-place: Glasgow, Scotland Uk},
	keywords  = {explainability, explanations, intelligent systems, transparency, XAI},
	pages     = {1--6},
	file      = {Eiband et al. - 2019 - The Impact of Placebic Explanations on Trust in In.pdf:/Users/fonok3/Zotero/storage/2J5AXHJQ/Eiband et al. - 2019 - The Impact of Placebic Explanations on Trust in In.pdf:application/pdf}
}

@inproceedings{mohseni_quantitative_2021,
	address   = {New York, NY, USA},
	series    = {{IUI} '21},
	title     = {Quantitative {Evaluation} of {Machine} {Learning} {Explanations}: {A} {Human}-{Grounded} {Benchmark}},
	isbn      = {978-1-4503-8017-1},
	doi       = {10.1145/3397481.3450689},
	booktitle = {26th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author    = {Mohseni, Sina and Block, Jeremy E and Ragan, Eric},
	year      = {2021},
	note      = {event-place: College Station, TX, USA},
	keywords  = {data annotation, explanation benchmark, explanation evaluation, Explanation: Evaluation Method, machine learning explanations},
	pages     = {22--31},
	file      = {Mohseni et al. - 2021 - Quantitative Evaluation of Machine Learning Explan.pdf:/Users/fonok3/Zotero/storage/PI8TI5VK/Mohseni et al. - 2021 - Quantitative Evaluation of Machine Learning Explan.pdf:application/pdf}
}

@inproceedings{ehsan_operationalizing_2021,
	address   = {New York, NY, USA},
	series    = {{CHI} {EA} '21},
	title     = {Operationalizing {Human}-{Centered} {Perspectives} in {Explainable} {AI}},
	isbn      = {978-1-4503-8095-9},
	doi       = {10.1145/3411763.3441342},
	booktitle = {Extended {Abstracts} of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author    = {Ehsan, Upol and Wintersberger, Philipp and Liao, Q. Vera and Mara, Martina and Streit, Marc and Wachter, Sandra and Riener, Andreas and Riedl, Mark O.},
	year      = {2021},
	note      = {event-place: Yokohama, Japan},
	keywords  = {Algorithmic Fairness, Artificial Intelligence, Critical Technical Practice, Explainable Artificial Intelligence, Human-centered Computing, Interpretability, Interpretable Machine Learning, Trust in Automation},
	file      = {Full Text:/Users/fonok3/Zotero/storage/EUKX4QSB/Ehsan et al. - 2021 - Operationalizing Human-Centered Perspectives in Ex.pdf:application/pdf}
}

@inproceedings{mucha_interfaces_2021,
	address   = {New York, NY, USA},
	series    = {{CHI} {EA} '21},
	title     = {Interfaces for {Explanations} in {Human}-{AI} {Interaction}: {Proposing} a {Design} {Evaluation} {Approach}},
	isbn      = {978-1-4503-8095-9},
	doi       = {10.1145/3411763.3451759},
	booktitle = {Extended {Abstracts} of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author    = {Mucha, Henrik and Robert, Sebastian and Breitschwerdt, Ruediger and Fellmann, Michael},
	year      = {2021},
	note      = {event-place: Yokohama, Japan},
	keywords  = {Explainable Artificial Intelligence (XAI), Explanation: Design, Explanatory User Interfaces, Human-AI Interaction},
	file      = {Full Text:/Users/fonok3/Zotero/storage/PIK5UJ8Y/Mucha et al. - 2021 - Interfaces for Explanations in Human-AI Interactio.pdf:application/pdf}
}

@inproceedings{kouki_user_2017,
	address   = {New York, NY, USA},
	series    = {{RecSys} '17},
	title     = {User {Preferences} for {Hybrid} {Explanations}},
	isbn      = {978-1-4503-4652-8},
	doi       = {10.1145/3109859.3109915},
	booktitle = {Proceedings of the {Eleventh} {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {Association for Computing Machinery},
	author    = {Kouki, Pigi and Schaffer, James and Pujara, Jay and O'Donovan, John and Getoor, Lise},
	year      = {2017},
	note      = {event-place: Como, Italy},
	keywords  = {explanations, hybrid explanations, hybrid recommendations},
	pages     = {84--88},
	file      = {Kouki et al. - 2017 - User Preferences for Hybrid Explanations.pdf:/Users/fonok3/Zotero/storage/MBF869FZ/Kouki et al. - 2017 - User Preferences for Hybrid Explanations.pdf:application/pdf}
}

@inproceedings{abdulrahman_belief-based_2019,
	address   = {New York, NY, USA},
	series    = {{IVA} '19},
	title     = {Belief-{Based} {Agent} {Explanations} to {Encourage} {Behaviour} {Change}},
	isbn      = {978-1-4503-6672-4},
	doi       = {10.1145/3308532.3329444},
	booktitle = {Proceedings of the 19th {ACM} {International} {Conference} on {Intelligent} {Virtual} {Agents}},
	publisher = {Association for Computing Machinery},
	author    = {Abdulrahman, Amal and Richards, Deborah and Ranjbartabar, Hedieh and Mascarenhas, Samuel},
	year      = {2019},
	note      = {event-place: Paris, France},
	keywords  = {behaviour change, explainable ai, Explanation: Kind, intelligent virtual agents},
	pages     = {176--178},
	file      = {Abdulrahman et al. - 2019 - Belief-Based Agent Explanations to Encourage Behav.pdf:/Users/fonok3/Zotero/storage/V985K8KC/Abdulrahman et al. - 2019 - Belief-Based Agent Explanations to Encourage Behav.pdf:application/pdf}
}

@inproceedings{tsai_evaluating_2019,
	address   = {New York, NY, USA},
	series    = {{UMAP} '19},
	title     = {Evaluating {Visual} {Explanations} for {Similarity}-{Based} {Recommendations}: {User} {Perception} and {Performance}},
	isbn      = {978-1-4503-6021-0},
	doi       = {10.1145/3320435.3320465},
	booktitle = {Proceedings of the 27th {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author    = {Tsai, Chun-Hua and Brusilovsky, Peter},
	year      = {2019},
	note      = {event-place: Larnaca, Cyprus},
	keywords  = {Explanation: Design, Explanation: Evaluation Method, recommendation, similarity-based, visual explanation},
	pages     = {22--30},
	file      = {Tsai and Brusilovsky - 2019 - Evaluating Visual Explanations for Similarity-Base.pdf:/Users/fonok3/Zotero/storage/74BTHNGP/Tsai and Brusilovsky - 2019 - Evaluating Visual Explanations for Similarity-Base.pdf:application/pdf;Tsai and Brusilovsky - 2019 - Evaluating Visual Explanations for Similarity-Base.pdf:/Users/fonok3/Zotero/storage/WZ7SRVYM/Tsai and Brusilovsky - 2019 - Evaluating Visual Explanations for Similarity-Base.pdf:application/pdf}
}

@inproceedings{hernandez-bocanegra_effects_2020,
	address   = {New York, NY, USA},
	series    = {{UMAP} '20 {Adjunct}},
	title     = {Effects of {Argumentative} {Explanation} {Types} on the {Perception} of {Review}-{Based} {Recommendations}},
	isbn      = {978-1-4503-7950-2},
	doi       = {10.1145/3386392.3399302},
	booktitle = {Adjunct {Publication} of the 28th {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author    = {Hernandez-Bocanegra, Diana C. and Donkers, Tim and Ziegler, Jürgen},
	year      = {2020},
	note      = {event-place: Genoa, Italy},
	keywords  = {Explanation: Evaluation Method, Explanation: Kind, explanations, recommender systems, user study},
	pages     = {219--225},
	file      = {Hernandez-Bocanegra et al. - 2020 - Effects of Argumentative Explanation Types on the .pdf:/Users/fonok3/Zotero/storage/8TAAMDPI/Hernandez-Bocanegra et al. - 2020 - Effects of Argumentative Explanation Types on the .pdf:application/pdf}
}

@inproceedings{brennen_what_2020,
	address   = {New York, NY, USA},
	series    = {{CHI} {EA} '20},
	title     = {What {Do} {People} {Really} {Want} {When} {They} {Say} {They} {Want} Explainable AI? We {Asked} 60 {Stakeholders}.},
	isbn      = {978-1-4503-6819-3},
	doi       = {10.1145/3334480.3383047},
	booktitle = {Extended {Abstracts} of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author    = {Brennen, Andrea},
	year      = {2020},
	note      = {event-place: Honolulu, HI, USA},
	keywords  = {data science, explainable AI, interface design, machine learning, UI/UX design, user research},
	pages     = {1--7},
	file      = {Brennen - 2020 - What Do People Really Want When They Say They Want.pdf:/Users/fonok3/Zotero/storage/FYBVL2YF/Brennen - 2020 - What Do People Really Want When They Say They Want.pdf:application/pdf}
}

@inproceedings{stange_effects_2021,
	address   = {New York, NY, USA},
	series    = {{HRI} '21 {Companion}},
	title     = {Effects of {Referring} to {Robot} vs. {User} {Needs} in {Self}-{Explanations} of {Undesirable} {Robot} {Behavior}},
	isbn      = {978-1-4503-8290-8},
	doi       = {10.1145/3434074.3447174},
	booktitle = {Companion of the 2021 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {Association for Computing Machinery},
	author    = {Stange, Sonja and Kopp, Stefan},
	year      = {2021},
	note      = {event-place: Boulder, CO, USA},
	keywords  = {empirical study, explainability, human-robot interaction},
	pages     = {271--275},
	file      = {Stange and Kopp - 2021 - Effects of Referring to Robot vs. User Needs in Se.pdf:/Users/fonok3/Zotero/storage/KPYLP8WP/Stange and Kopp - 2021 - Effects of Referring to Robot vs. User Needs in Se.pdf:application/pdf;Stange and Kopp - 2021 - Effects of Referring to Robot vs. User Needs in Se.pdf:/Users/fonok3/Zotero/storage/A4Q45T79/Stange and Kopp - 2021 - Effects of Referring to Robot vs. User Needs in Se.pdf:application/pdf}
}

@inproceedings{kunkel_let_2019,
	address   = {New York, NY, USA},
	series    = {{CHI} '19},
	title     = {Let {Me} {Explain}: {Impact} of {Personal} and {Impersonal} {Explanations} on {Trust} in {Recommender} {Systems}},
	isbn      = {978-1-4503-5970-2},
	doi       = {10.1145/3290605.3300717},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author    = {Kunkel, Johannes and Donkers, Tim and Michael, Lisa and Barbu, Catalin-Mihai and Ziegler, Jürgen},
	year      = {2019},
	note      = {event-place: Glasgow, Scotland Uk},
	keywords  = {counterfactual analysis, explanations, recommender systems, structural equation modelling, trust, user study},
	pages     = {1--12},
	file      = {Kunkel et al. - 2019 - Let Me Explain Impact of Personal and Impersonal .pdf:/Users/fonok3/Zotero/storage/55I9MSJW/Kunkel et al. - 2019 - Let Me Explain Impact of Personal and Impersonal .pdf:application/pdf}
}

@inproceedings{schaffer_i_2019,
	address   = {New York, NY, USA},
	series    = {{IUI} '19},
	title     = {I {Can} {Do} {Better} than {Your} {AI}: {Expertise} and {Explanations}},
	isbn      = {978-1-4503-6272-6},
	doi       = {10.1145/3301275.3302308},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author    = {Schaffer, James and O'Donovan, John and Michaelis, James and Raglin, Adrienne and Höllerer, Tobias},
	year      = {2019},
	note      = {event-place: Marina del Ray, California},
	keywords  = {cognitive modeling, decision support systems, human-computer interaction, information systems, intelligent assistants, user interfaces},
	pages     = {240--251},
	file      = {Schaffer et al. - 2019 - I can do better than your AI expertise and explan.pdf:/Users/fonok3/Zotero/storage/VITFKZEX/Schaffer et al. - 2019 - I can do better than your AI expertise and explan.pdf:application/pdf}
}

@inproceedings{weitz_you_2019,
	address   = {New York, NY, USA},
	series    = {{IVA} '19},
	title     = {Do You Trust Me?: Increasing {User}-{Trust} by {Integrating} {Virtual} {Agents} in {Explainable} {AI} {Interaction} {Design}},
	isbn      = {978-1-4503-6672-4},
	doi       = {10.1145/3308532.3329441},
	booktitle = {Proceedings of the 19th {ACM} {International} {Conference} on {Intelligent} {Virtual} {Agents}},
	publisher = {Association for Computing Machinery},
	author    = {Weitz, Katharina and Schiller, Dominik and Schlagowski, Ruben and Huber, Tobias and André, Elisabeth},
	year      = {2019},
	note      = {event-place: Paris, France},
	keywords  = {deep learning, explainable artificial intelligence, Explanation: Design, human-agent interaction, interpretability, trust, virtual agents},
	pages     = {7--9},
	file      = {Full Text:/Users/fonok3/Zotero/storage/I8D9B48K/Weitz et al. - 2019 - Do You Trust Me Increasing User-Trust by Integ.pdf:application/pdf}
}

@inproceedings{sato_action-triggering_2019,
	title     = {Action-{Triggering} {Recommenders}: {Uplift} {Optimization} and {Persuasive} {Explanation}},
	doi       = {10.1109/ICDMW.2019.00155},
	booktitle = {2019 {International} {Conference} on {Data} {Mining} {Workshops} ({ICDMW})},
	author    = {Sato, Masahiro and Kawai, Shin and Nobuhara, Hajime},
	month     = nov,
	year      = {2019},
	note      = {ISSN: 2375-9259},
	keywords  = {causal inference, context-awareness, explainable recommendation, Explanation: Design, Measurement, Optimization methods, Predictive models, Protocols, recommendation effect, Recommender systems, Training, uplift modeling},
	pages     = {1060--1069},
	file      = {Sato et al. - 2019 - Action-Triggering Recommenders Uplift Optimizatio.pdf:/Users/fonok3/Zotero/storage/6MLRUT55/Sato et al. - 2019 - Action-Triggering Recommenders Uplift Optimizatio.pdf:application/pdf}
}

@inproceedings{li_reasoning_2020,
	title     = {Reasoning about {When} to {Provide} {Explanation} for {Human}-involved {Self}-{Adaptive} {Systems}},
	doi       = {10.1109/ACSOS49614.2020.00042},
	booktitle = {2020 {IEEE} {International} {Conference} on {Autonomic} {Computing} and {Self}-{Organizing} {Systems} ({ACSOS})},
	author    = {Li, Nianyu and Cámara, Javier and Garlan, David and Schmerl, Bradley},
	month     = aug,
	year      = {2020},
	keywords  = {Adaptation models, Adaptive systems, Analytical models, Cognition, Model checking, Probabilistic logic, Time factors},
	pages     = {195--204},
	file      = {Li et al. - 2020 - Reasoning about When to Provide Explanation for Hu.pdf:/Users/fonok3/Zotero/storage/HPUNCK8V/Li et al. - 2020 - Reasoning about When to Provide Explanation for Hu.pdf:application/pdf}
}

@inproceedings{kohl_explainability_2019,
	title     = {Explainability as a {Non}-{Functional} {Requirement}},
	doi       = {10.1109/RE.2019.00046},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author    = {Köhl, Maximilian A. and Baum, Kevin and Langer, Markus and Oster, Daniel and Speith, Timo and Bohlender, Dimitri},
	month     = sep,
	year      = {2019},
	note      = {ISSN: 2332-6441},
	keywords  = {Certification, certified explainability, Decision making, explainable systems, Organizational aspects, Personnel, requirements elicitation, requirements specification, Stakeholders, Standardization, terminology, Terminology},
	pages     = {363--368},
	file      = {Kohl et al. - 2019 - Explainability as a Non-Functional Requirement.pdf:/Users/fonok3/Zotero/storage/IUBTPCIE/Kohl et al. - 2019 - Explainability as a Non-Functional Requirement.pdf:application/pdf}
}

@inproceedings{meteier_workshop_2019,
	address   = {New York, NY, USA},
	series    = {{AutomotiveUI} '19},
	title     = {Workshop on {Explainable} {AI} in {Automated} {Driving}: {A} {User}-{Centered} {Interaction} {Approach}},
	isbn      = {978-1-4503-6920-6},
	doi       = {10.1145/3349263.3350762},
	abstract  = {With the increasing use of automation, users tend to delegate more tasks to the machines. Such complex systems are usually developed with "black box" Artificial Intelligence (AI), which makes these systems difficult to understand for the user. This assumption is particularly true in the field of automated driving since the level of automation is constantly increasing via the use of state-of-the-art AI solutions. We believe it is important to investigate the field of Explainable AI (XAI) in the context of automated driving since interpretability and transparency are key factors for increasing trust and security. In this workshop, we aim at gathering researchers and industry practitioners from different fields to brainstorm about XAI with a special focus on human-vehicle interaction. Questions like "what kind of explanation do we need", "which is the best trade-off between performance and explainability" and "how granular should the explanations be" will be addressed in this workshop.},
	booktitle = {Proceedings of the 11th {International} {Conference} on {Automotive} {User} {Interfaces} and {Interactive} {Vehicular} {Applications}: {Adjunct} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author    = {Meteier, Quentin and Capallera, Marine and Angelini, Leonardo and Mugellini, Elena and Khaled, Omar Abou and Carrino, Stefano and De Salis, Emmanuel and Galland, Stéphane and Boll, Susanne},
	year      = {2019},
	note      = {event-place: Utrecht, Netherlands},
	keywords  = {automated driving, explainable artificial intelligence, trust in automation, user interface design, vehicle environment},
	pages     = {32--37},
	file      = {Meteier et al. - 2019 - Workshop on explainable AI in automated driving a.pdf:/Users/fonok3/Zotero/storage/WKX27Z8M/Meteier et al. - 2019 - Workshop on explainable AI in automated driving a.pdf:application/pdf}
}

@inproceedings{haspiel_explanations_2018,
	address   = {New York, NY, USA},
	series    = {{HRI} '18},
	title     = {Explanations and {Expectations}: {Trust} {Building} in {Automated} {Vehicles}},
	isbn      = {978-1-4503-5615-2},
	doi       = {10.1145/3173386.3177057},
	booktitle = {Companion of the 2018 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {Association for Computing Machinery},
	author    = {Haspiel, Jacob and Du, Na and Meyerson, Jill and Robert Jr., Lionel P. and Tilbury, Dawn and Yang, X. Jessie and Pradhan, Anuj K.},
	year      = {2018},
	note      = {event-place: Chicago, IL, USA},
	keywords  = {human-machine interface, transparency, trust in avs},
	pages     = {119--120},
	file      = {Full Text:/Users/fonok3/Zotero/storage/TRNIKWJ9/Haspiel et al. - 2018 - Explanations and Expectations Trust Building in A.pdf:application/pdf}
}

@inproceedings{de_almeida_investigating_2019,
	address   = {New York, NY, USA},
	series    = {{IHC} '19},
	title     = {Investigating {Google} {Dashboard}'s {Explainability} to {Support} {Individual} {Privacy} {Decision} {Making}},
	isbn      = {978-1-4503-6971-8},
	doi       = {10.1145/3357155.3358438},
	booktitle = {Proceedings of the 18th {Brazilian} {Symposium} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author    = {de Almeida, Maria Clara G. and de Castro Salgado, Luciana C.},
	year      = {2019},
	note      = {event-place: Vitória, Espírito Santo, Brazil},
	keywords  = {data control, decision making, explanation, privacy},
	file      = {de Almeida and de Castro Salgado - 2019 - Investigating Google dashboard's explainability to.pdf:/Users/fonok3/Zotero/storage/6YLUAL5K/de Almeida and de Castro Salgado - 2019 - Investigating Google dashboard's explainability to.pdf:application/pdf}
}

@inproceedings{barik_improving_2015,
	title     = {Improving error notification comprehension in {IDEs} by supporting developer self-explanations},
	doi       = {10.1109/VLHCC.2015.7357233},
	booktitle = {2015 {IEEE} {Symposium} on {Visual} {Languages} and {Human}-{Centric} {Computing} ({VL}/{HCC})},
	author    = {Barik, Titus},
	month     = oct,
	year      = {2015},
	keywords  = {Cognition, Probabilistic logic},
	pages     = {293--294},
	file      = {Barik - 2015 - Improving error notification comprehension in IDEs.pdf:/Users/fonok3/Zotero/storage/YUNRKTW5/Barik - 2015 - Improving error notification comprehension in IDEs.pdf:application/pdf}
}

@inproceedings{kaptein_personalised_2017,
	title     = {Personalised self-explanation by robots: {The} role of goals versus beliefs in robot-action explanation for children and adults},
	doi       = {10.1109/ROMAN.2017.8172376},
	booktitle = {2017 26th {IEEE} {International} {Symposium} on {Robot} and {Human} {Interactive} {Communication} ({RO}-{MAN})},
	author    = {Kaptein, Frank and Broekens, Joost and Hindriks, Koen and Neerincx, Mark},
	month     = aug,
	year      = {2017},
	note      = {ISSN: 1944-9437},
	keywords  = {Aging, Diabetes, Games, Intelligent systems, Pediatrics, Psychology, Robots},
	pages     = {676--682},
	file      = {Kaptein et al. - 2017 - Personalised self-explanation by robots The role .pdf:/Users/fonok3/Zotero/storage/T7G4EPIW/Kaptein et al. - 2017 - Personalised self-explanation by robots The role .pdf:application/pdf}
}

@inproceedings{zolotas_towards_2019,
	title     = {Towards {Explainable} {Shared} {Control} using {Augmented} {Reality}},
	doi       = {10.1109/IROS40897.2019.8968117},
	booktitle = {2019 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author    = {Zolotas, Mark and Demiris, Yiannis},
	month     = nov,
	year      = {2019},
	note      = {ISSN: 2153-0866},
	pages     = {3020--3026},
	file      = {Submitted Version:/Users/fonok3/Zotero/storage/KVMWC2M8/Zolotas and Demiris - 2019 - Towards Explainable Shared Control using Augmented.pdf:application/pdf}
}

@article{riveiro_thats_2021,
	title    = {“{That}'s (not) the output {I} expected!” {On} the role of end user expectations in creating explanations of {AI} systems},
	volume   = {298},
	issn     = {0004-3702},
	url      = {https://www.sciencedirect.com/science/article/pii/S0004370221000588},
	doi      = {10.1016/j.artint.2021.103507},
	journal  = {Artificial Intelligence},
	author   = {Riveiro, Maria and Thill, Serge},
	year     = {2021},
	keywords = {Contrastive, Counterfactual, Expectations, Explainable AI, Explanation: Kind, Explanations, Factual, Human-AI interaction, Machine behaviour, Mental models},
	pages    = {103507},
	file     = {Riveiro and Thill - 2021 - “That's (not) the output I expected!” On the role .pdf:/Users/fonok3/Zotero/storage/67DCW548/Riveiro and Thill - 2021 - “That's (not) the output I expected!” On the role .pdf:application/pdf;Riveiro and Thill - 2021 - “That's (not) the output I expected!” On the role .pdf:/Users/fonok3/Zotero/storage/M2RYKNUG/Riveiro and Thill - 2021 - “That's (not) the output I expected!” On the role .pdf:application/pdf}
}

@article{martin_evaluating_2021,
	title    = {Evaluating {Explainability} {Methods} {Intended} for {Multiple} {Stakeholders}},
	issn     = {1610-1987},
	doi      = {10.1007/s13218-020-00702-6},
	language = {en},
	urldate  = {2021-05-24},
	journal  = {KI - Künstliche Intelligenz},
	author   = {Martin, Kyle and Liret, Anne and Wiratunga, Nirmalie and Owusu, Gilbert and Kern, Mathias},
	month    = feb,
	year     = {2021},
	keywords = {Explanation: Evaluation Method},
	file     = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/DWIJHANY/Martin et al. - 2021 - Evaluating Explainability Methods Intended for Mul.pdf:application/pdf}
}

@inproceedings{martin_developing_2019,
	address   = {Cham},
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {Developing a {Catalogue} of {Explainability} {Methods} to {Support} {Expert} and {Non}-expert {Users}},
	isbn      = {978-3-030-34885-4},
	doi       = {10.1007/978-3-030-34885-4_24},
	language  = {en},
	booktitle = {Artificial {Intelligence} {XXXVI}},
	publisher = {Springer International Publishing},
	author    = {Martin, Kyle and Liret, Anne and Wiratunga, Nirmalie and Owusu, Gilbert and Kern, Mathias},
	editor    = {Bramer, Max and Petridis, Miltos},
	year      = {2019},
	keywords  = {Explainability, Information retrieval, Machine learning, Similarity modeling},
	pages     = {309--324},
	file      = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/D2BLE55F/Martin et al. - 2019 - Developing a Catalogue of Explainability Methods t.pdf:application/pdf}
}

@article{tsai_effects_2020,
	title    = {The effects of controllability and explainability in a social recommender system},
	issn     = {1573-1391},
	doi      = {10.1007/s11257-020-09281-5},
	abstract = {In recent years, researchers in the field of recommender systems have explored a range of advanced interfaces to improve user interactions with recommender systems. Some of the major research ideas explored in this new area include the explainability and controllability of recommendations. Controllability enables end users to participate in the recommendation process by providing various kinds of input. Explainability focuses on making the recommendation process and the reasons behind specific recommendation more clear to the users. While each of these approaches contributes to making traditional “black-box” recommendation more attractive and acceptable to end users, little is known about how these approaches work together. In this paper, we investigate the effects of adding user control and visual explanations in a specific context of an interactive hybrid social recommender system. We present Relevance Tuner+, a hybrid recommender system that allows the users to control the fusion of multiple recommender sources while also offering explanations of both the fusion process and each of the source recommendations. We also report the results of a controlled study (N = 50) that explores the impact of controllability and explainability in this context.},
	language = {en},
	urldate  = {2021-05-24},
	journal  = {User Modeling and User-Adapted Interaction},
	author   = {Tsai, Chun-Hua and Brusilovsky, Peter},
	month    = oct,
	year     = {2020},
	file     = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/QUW5Y23M/Tsai and Brusilovsky - 2020 - The effects of controllability and explainability .pdf:application/pdf}
}

@article{rosenfeld_explainability_2019,
	title    = {Explainability in human–agent systems},
	volume   = {33},
	issn     = {1573-7454},
	doi      = {10.1007/s10458-019-09408-y},
	language = {en},
	number   = {6},
	urldate  = {2021-05-24},
	journal  = {Autonomous Agents and Multi-Agent Systems},
	author   = {Rosenfeld, Avi and Richardson, Ariella},
	month    = nov,
	year     = {2019},
	pages    = {673--705},
	file     = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/F6PLU5CM/Rosenfeld and Richardson - 2019 - Explainability in human–agent systems.pdf:application/pdf}
}

@article{weitz_let_2021,
	title      = {“{Let} me explain!”: exploring the potential of virtual agents in explainable {AI} interaction design},
	volume     = {15},
	issn       = {1783-8738},
	shorttitle = {“{Let} me explain!”},
	doi        = {10.1007/s12193-020-00332-0},
	abstract   = {While the research area of artificial intelligence benefited from increasingly sophisticated machine learning techniques in recent years, the resulting systems suffer from a loss of transparency and comprehensibility, especially for end-users. In this paper, we explore the effects of incorporating virtual agents into explainable artificial intelligence (XAI) designs on the perceived trust of end-users. For this purpose, we conducted a user study based on a simple speech recognition system for keyword classification. As a result of this experiment, we found that the integration of virtual agents leads to increased user trust in the XAI system. Furthermore, we found that the user’s trust significantly depends on the modalities that are used within the user-agent interface design. The results of our study show a linear trend where the visual presence of an agent combined with a voice output resulted in greater trust than the output of text or the voice output alone. Additionally, we analysed the participants’ feedback regarding the presented XAI visualisations. We found that increased human-likeness of and interaction with the virtual agent are the two most common mention points on how to improve the proposed XAI interaction design. Based on these results, we discuss current limitations and interesting topics for further research in the field of XAI. Moreover, we present design recommendations for virtual agents in XAI systems for future projects.},
	language   = {en},
	number     = {2},
	urldate    = {2021-05-24},
	journal    = {Journal on Multimodal User Interfaces},
	author     = {Weitz, Katharina and Schiller, Dominik and Schlagowski, Ruben and Huber, Tobias and André, Elisabeth},
	month      = jun,
	year       = {2021},
	keywords   = {Explanation: Design},
	pages      = {87--98},
	file       = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/BVRC6YPP/Weitz et al. - 2021 - “Let me explain!” exploring the potential of virt.pdf:application/pdf}
}

@article{hacker_explainable_2020,
	title      = {Explainable {AI} under contract and tort law: legal incentives and technical challenges},
	volume     = {28},
	issn       = {1572-8382},
	shorttitle = {Explainable {AI} under contract and tort law},
	doi        = {10.1007/s10506-020-09260-6},
	language   = {en},
	number     = {4},
	urldate    = {2021-05-24},
	journal    = {Artificial Intelligence and Law},
	author     = {Hacker, Philipp and Krestel, Ralf and Grundmann, Stefan and Naumann, Felix},
	month      = dec,
	year       = {2020},
	pages      = {415--439},
	file       = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/I494CPG8/Hacker et al. - 2020 - Explainable AI under contract and tort law legal .pdf:application/pdf}
}

@inproceedings{rjoob_towards_2021,
	address   = {Cham},
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {Towards {Explainable} {Artificial} {Intelligence} and {Explanation} {User} {Interfaces} to {Open} the ‘{Black} {Box}’ of {Automated} {ECG} {Interpretation}},
	isbn      = {978-3-030-68007-7},
	doi       = {10.1007/978-3-030-68007-7_6},
	language  = {en},
	booktitle = {Advanced {Visual} {Interfaces}. {Supporting} {Artificial} {Intelligence} and {Big} {Data} {Applications}},
	publisher = {Springer International Publishing},
	author    = {Rjoob, Khaled and Bond, Raymond and Finlay, Dewar and McGilligan, Victoria and Leslie, Stephen J. and Rababah, Ali and Iftikhar, Aleeha and Guldenring, Daniel and Knoery, Charles and McShane, Anne and Peace, Aaron},
	editor    = {Reis, Thoralf and Bornschlegl, Marco X. and Angelini, Marco and Hemmje, Matthias L.},
	year      = {2021},
	keywords  = {Artificial intelligence (AI), ECG interpretation, Explainable AI (XAI)},
	pages     = {96--108},
	file      = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/M9UI7MV5/Rjoob et al. - 2021 - Towards Explainable Artificial Intelligence and Ex.pdf:application/pdf}
}

@inproceedings{ehsan_human-centered_2020,
	address    = {Cham},
	series     = {Lecture {Notes} in {Computer} {Science}},
	title      = {Human-{Centered} {Explainable} {AI}: {Towards} a {Reflective} {Sociotechnical} {Approach}},
	isbn       = {978-3-030-60117-1},
	shorttitle = {Human-{Centered} {Explainable} {AI}},
	doi        = {10.1007/978-3-030-60117-1_33},
	abstract   = {Explanations—a form of post-hoc interpretability—play an instrumental role in making systems accessible as AI continues to proliferate complex and sensitive sociotechnical systems. In this paper, we introduce Human-centered Explainable AI (HCXAI) as an approach that puts the human at the center of technology design. It develops a holistic understanding of “who” the human is by considering the interplay of values, interpersonal dynamics, and the socially situated nature of AI systems. In particular, we advocate for a reflective sociotechnical approach. We illustrate HCXAI through a case study of an explanation system for non-technical end-users that shows how technical advancements and the understanding of human factors co-evolve. Building on the case study, we lay out open research questions pertaining to further refining our understanding of “who” the human is and extending beyond 1-to-1 human-computer interactions. Finally, we propose that a reflective HCXAI paradigm—mediated through the perspective of Critical Technical Practice and supplemented with strategies from HCI, such as value-sensitive design and participatory design—not only helps us understand our intellectual blind spots, but it can also open up new design and research spaces.},
	language   = {en},
	booktitle  = {{HCI} {International} 2020 - {Late} {Breaking} {Papers}: {Multimodality} and {Intelligence}},
	publisher  = {Springer International Publishing},
	author     = {Ehsan, Upol and Riedl, Mark O.},
	editor     = {Stephanidis, Constantine and Kurosu, Masaaki and Degen, Helmut and Reinerman-Jones, Lauren},
	year       = {2020},
	keywords   = {Artificial intelligence, Critical technical practice, Explainable AI, Human-centered computing, Interpretability, Machine learning, Rationale generation, Sociotechnical, User perception},
	pages      = {449--466},
	file       = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/WPF6T4X6/Ehsan and Riedl - 2020 - Human-Centered Explainable AI Towards a Reflectiv.pdf:application/pdf}
}

@inproceedings{cassens_ambient_2019,
	address    = {Cham},
	series     = {Lecture {Notes} in {Computer} {Science}},
	title      = {Ambient {Explanations}: {Ambient} {Intelligence} and {Explainable} {AI}},
	isbn       = {978-3-030-34255-5},
	shorttitle = {Ambient {Explanations}},
	doi        = {10.1007/978-3-030-34255-5_30},
	language   = {en},
	booktitle  = {Ambient {Intelligence}},
	publisher  = {Springer International Publishing},
	author     = {Cassens, Jörg and Wegener, Rebekah},
	editor     = {Chatzigiannakis, Ioannis and De Ruyter, Boris and Mavrommati, Irene},
	year       = {2019},
	keywords   = {Ambient Intelligence, Context, Explanation: Evaluation Method, Explanation: When?, Explanations, Semiotics},
	pages      = {370--376},
	file       = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/9LQSDE24/Cassens and Wegener - 2019 - Ambient Explanations Ambient Intelligence and Exp.pdf:application/pdf}
}

@inproceedings{thomson_knowledge--information_2020,
	address    = {Cham},
	series     = {Lecture {Notes} in {Computer} {Science}},
	title      = {Knowledge-to-{Information} {Translation} {Training} ({KITT}): {An} {Adaptive} {Approach} to {Explainable} {Artificial} {Intelligence}},
	isbn       = {978-3-030-50788-6},
	shorttitle = {Knowledge-to-{Information} {Translation} {Training} ({KITT})},
	doi        = {10.1007/978-3-030-50788-6_14},
	abstract   = {Modern black-box artificial intelligence algorithms are computationally powerful yet fallible in unpredictable ways. While much research has gone into developing techniques to interpret these algorithms, less have also integrated the requirement to understand the algorithm as a function of their training data. In addition, few have examined the human requirements for explainability, so these interpretations provide the right quantity and quality of information to each user. We argue that Explainable Artificial Intelligence (XAI) frameworks need to account the expertise and goals of the user in order to gain widespread adoptance. We describe the Knowledge-to-Information Translation Training (KITT) framework, an approach to XAI that considers a number of possible explanatory models that can be used to facilitate users’ understanding of artificial intelligence. Following a review of algorithms, we provide a taxonomy of explanation types and outline how adaptive instructional systems can facilitate knowledge translation between developers and users. Finally, we describe limitations of our approach and paths for future research opportunities.},
	language   = {en},
	booktitle  = {Adaptive {Instructional} {Systems}},
	publisher  = {Springer International Publishing},
	author     = {Thomson, Robert and Schoenherr, Jordan Richard},
	editor     = {Sottilare, Robert A. and Schwarz, Jessica},
	year       = {2020},
	keywords   = {Adaptive instructional systems, Explainable AI, Knowledge translation},
	pages      = {187--204},
	file       = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/DD94REWA/Thomson and Schoenherr - 2020 - Knowledge-to-Information Translation Training (KIT.pdf:application/pdf}
}

@inproceedings{cirqueira_scenario-based_2020,
	address   = {Cham},
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {Scenario-{Based} {Requirements} {Elicitation} for {User}-{Centric} {Explainable} {AI}},
	isbn      = {978-3-030-57321-8},
	doi       = {10.1007/978-3-030-57321-8_18},
	abstract  = {Explainable Artificial Intelligence (XAI) develops technical explanation methods and enable interpretability for human stakeholders on why Artificial Intelligence (AI) and machine learning (ML) models provide certain predictions. However, the trust of those stakeholders into AI models and explanations is still an issue, especially domain experts, who are knowledgeable about their domain but not AI inner workings. Social and user-centric XAI research states it is essential to understand the stakeholder’s requirements to provide explanations tailored to their needs, and enhance their trust in working with AI models. Scenario-based design and requirements elicitation can help bridge the gap between social and operational aspects of a stakeholder early before the adoption of information systems and identify its real problem and practices generating user requirements. Nevertheless, it is still rarely explored the adoption of scenarios in XAI, especially in the domain of fraud detection to supporting experts who are about to work with AI models. We demonstrate the usage of scenario-based requirements elicitation for XAI in a fraud detection context, and develop scenarios derived with experts in banking fraud. We discuss how those scenarios can be adopted to identify user or expert requirements for appropriate explanations in his daily operations and to make decisions on reviewing fraudulent cases in banking. The generalizability of the scenarios for further adoption is validated through a systematic literature review in domains of XAI and visual analytics for fraud detection.},
	language  = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Extraction}},
	publisher = {Springer International Publishing},
	author    = {Cirqueira, Douglas and Nedbal, Dietmar and Helfert, Markus and Bezbradica, Marija},
	editor    = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
	year      = {2020},
	keywords  = {Domain expert, Explainable artificial intelligence, Fraud detection, Requirements elicitation},
	pages     = {321--341},
	file      = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/PTVNGSPL/Cirqueira et al. - 2020 - Scenario-Based Requirements Elicitation for User-C.pdf:application/pdf}
}

@article{sokol_one_2020,
	title    = {One {Explanation} {Does} {Not} {Fit} {All}},
	volume   = {34},
	issn     = {1610-1987},
	doi      = {10.1007/s13218-020-00637-y},
	abstract = {The need for transparency of predictive systems based on Machine Learning algorithms arises as a consequence of their ever-increasing proliferation in the industry. Whenever black-box algorithmic predictions influence human affairs, the inner workings of these algorithms should be scrutinised and their decisions explained to the relevant stakeholders, including the system engineers, the system’s operators and the individuals whose case is being decided. While a variety of interpretability and explainability methods is available, none of them is a panacea that can satisfy all diverse expectations and competing objectives that might be required by the parties involved. We address this challenge in this paper by discussing the promises of Interactive Machine Learning for improved transparency of black-box systems using the example of contrastive explanations—a state-of-the-art approach to Interpretable Machine Learning. Specifically, we show how to personalise counterfactual explanations by interactively adjusting their conditional statements and extract additional explanations by asking follow-up “What if?” questions. Our experience in building, deploying and presenting this type of system allowed us to list desired properties as well as potential limitations, which can be used to guide the development of interactive explainers. While customising the medium of interaction, i.e., the user interface comprising of various communication channels, may give an impression of personalisation, we argue that adjusting the explanation itself and its content is more important. To this end, properties such as breadth, scope, context, purpose and target of the explanation have to be considered, in addition to explicitly informing the explainee about its limitations and caveats. Furthermore, we discuss the challenges of mirroring the explainee’s mental model, which is the main building block of intelligible human–machine interactions. We also deliberate on the risks of allowing the explainee to freely manipulate the explanations and thereby extracting information about the underlying predictive model, which might be leveraged by malicious actors to steal or game the model. Finally, building an end-to-end interactive explainability system is a challenging engineering task; unless the main goal is its deployment, we recommend “Wizard of Oz” studies as a proxy for testing and evaluating standalone interactive explainability algorithms.},
	language = {en},
	number   = {2},
	urldate  = {2021-05-24},
	journal  = {KI - Künstliche Intelligenz},
	author   = {Sokol, Kacper and Flach, Peter},
	month    = jun,
	year     = {2020},
	pages    = {235--250},
	file     = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/P72U5J56/Sokol and Flach - 2020 - One Explanation Does Not Fit All.pdf:application/pdf}
}

@inproceedings{lu_study_2017,
	address   = {Cham},
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {A {Study} on {Interactive} {Explanation} {Boards} {Design} and {Evaluation} for {Active} {Aging} {Ecotourism}},
	isbn      = {978-3-319-58530-7},
	doi       = {10.1007/978-3-319-58530-7_11},
	abstract  = {According to the advanced aging society and the change of travelstyle, the advanced age groups are active in joining ecotourism to increase knowledge and experiences that raise their qualities of living. Therefore a solution to enhance ecotourism experiences of the advanced age group would be a major consideration in the future. A tendency of elevating knowledge and experiences of the active aging group with assistance of technology is anticipated. As a consequence the study focus on literature analysis of active aging group, ecotourism, interactive design and related design principles. Through a demand investigation, design and building of principle-based interactive interpretive signs prototype, a revised proposal of the prototype design is come up based on user reports regarding evaluation and review of the interactive prototypes. The study results are as following: (1) A fulfillment of recording, knowledge, safety and convenience of ecotourism is brought to the active aging group through the assistance of technology. (2) From the investigation of technology demand, the study developed consistency, flexibility, efficiency, artistic and simplified design, visibility, feedback, attraction, instruction, sustainability, satisfaction, assistance and directions, user control and unrestrained 11 prototype design characteristics. The design and building of interactive interpretive sign prototypes are based on the above characteristics. (3) In accordance with the user report, an induction of “Hardware Performance Issue” of interactive prototypes are able to increase knowledge of the active aging group, so as to improve the height, layout, narration, icons and information guidance, etc. For the interactive narration time design in “Information Media”, long time consumption of active aging group in a fixed location should be avoided. It may cause inconvenience to other users. The guiding information should be more detailed and screen size should be increased. The “Interactive Operation” is smooth. Manipulating both graphics and words simultaneously, as well as the efficiency of operation system should improve the guidelines. The listening experience in “Sharing their Experience” is one of the most important elements to enhance the interactive narration quality. As for the vision experience, an advancement of the narration screen and the story content is required. The study results expect to enhance the ecotourism experience of the active aging group, also to provide references for related studies operations.},
	language  = {en},
	booktitle = {Human {Aspects} of {IT} for the {Aged} {Population}. {Aging}, {Design} and {User} {Experience}},
	publisher = {Springer International Publishing},
	author    = {Lu, Li-Shu},
	editor    = {Zhou, Jia and Salvendy, Gavriel},
	year      = {2017},
	keywords  = {Active aging, Ecotourism, Explanation boards, Explanation: Design, Interactive design, Usability},
	pages     = {160--172},
	file      = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/E2MS5QPT/Lu - 2017 - A Study on Interactive Explanation Boards Design a.pdf:application/pdf}
}

@inproceedings{chari_explanation_2020,
	address    = {Cham},
	series     = {Lecture {Notes} in {Computer} {Science}},
	title      = {Explanation {Ontology}: {A} {Model} of {Explanations} for {User}-{Centered} {AI}},
	isbn       = {978-3-030-62466-8},
	shorttitle = {Explanation {Ontology}},
	doi        = {10.1007/978-3-030-62466-8_15},
	abstract   = {Explainability has been a goal for Artificial Intelligence (AI) systems since their conception, with the need for explainability growing as more complex AI models are increasingly used in critical, high-stakes settings such as healthcare. Explanations have often added to an AI system in a non-principled, post-hoc manner. With greater adoption of these systems and emphasis on user-centric explainability, there is a need for a structured representation that treats explainability as a primary consideration, mapping end user needs to specific explanation types and the system’s AI capabilities. We design an explanation ontology to model both the role of explanations, accounting for the system and user attributes in the process, and the range of different literature-derived explanation types. We indicate how the ontology can support user requirements for explanations in the domain of healthcare. We evaluate our ontology with a set of competency questions geared towards a system designer who might use our ontology to decide which explanation types to include, given a combination of users’ needs and a system’s capabilities, both in system design settings and in real-time operations. Through the use of this ontology, system designers will be able to make informed choices on which explanations AI systems can and should provide.},
	language   = {en},
	booktitle  = {The {Semantic} {Web} – {ISWC} 2020},
	publisher  = {Springer International Publishing},
	author     = {Chari, Shruthi and Seneviratne, Oshani and Gruen, Daniel M. and Foreman, Morgan A. and Das, Amar K. and McGuinness, Deborah L.},
	editor     = {Pan, Jeff Z. and Tamma, Valentina and d’Amato, Claudia and Janowicz, Krzysztof and Fu, Bo and Polleres, Axel and Seneviratne, Oshani and Kagal, Lalana},
	year       = {2020},
	keywords   = {Explainable AI, Explanation ontology, Modeling of explanations and explanation types, Supporting explainable ai in clinical decision making and decision support},
	pages      = {228--243},
	file       = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/CP533N33/Chari et al. - 2020 - Explanation Ontology A Model of Explanations for .pdf:application/pdf}
}

@article{bharadhwaj_explanations_2018,
	title    = {Explanations for {Temporal} {Recommendations}},
	volume   = {32},
	issn     = {1610-1987},
	doi      = {10.1007/s13218-018-0560-x},
	language = {en},
	number   = {4},
	urldate  = {2021-05-24},
	journal  = {KI - Künstliche Intelligenz},
	author   = {Bharadhwaj, Homanga and Joshi, Shruti},
	month    = nov,
	year     = {2018},
	pages    = {267--272},
	file     = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/PZ7JLP3C/Bharadhwaj and Joshi - 2018 - Explanations for Temporal Recommendations.pdf:application/pdf}
}

@inproceedings{sovrano_modelling_2020,
	address   = {Cham},
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {Modelling {GDPR}-{Compliant} {Explanations} for {Trustworthy} {AI}},
	isbn      = {978-3-030-58957-8},
	doi       = {10.1007/978-3-030-58957-8_16},
	language  = {en},
	booktitle = {Electronic {Government} and the {Information} {Systems} {Perspective}},
	publisher = {Springer International Publishing},
	author    = {Sovrano, Francesco and Vitali, Fabio and Palmirani, Monica},
	editor    = {Kő, Andrea and Francesconi, Enrico and Kotsis, Gabriele and Tjoa, A Min and Khalil, Ismail},
	year      = {2020},
	keywords  = {General Data Protection Regulation, Interactive explanatory tool, Trustworthy artificial intelligence},
	pages     = {219--233},
	file      = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/YCXVGH9Y/Sovrano et al. - 2020 - Modelling GDPR-Compliant Explanations for Trustwor.pdf:application/pdf}
}

@inproceedings{neerincx_using_2018,
	address   = {Cham},
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {Using {Perceptual} and {Cognitive} {Explanations} for {Enhanced} {Human}-{Agent} {Team} {Performance}},
	isbn      = {978-3-319-91122-9},
	doi       = {10.1007/978-3-319-91122-9_18},
	abstract  = {Most explainable AI (XAI) research projects focus on well-delineated topics, such as interpretability of machine learning outcomes, knowledge sharing in a multi-agent system or human trust in agent’s performance. For the development of explanations in human-agent teams, a more integrative approach is needed. This paper proposes a perceptual-cognitive explanation (PeCoX) framework for the development of explanations that address both the perceptual and cognitive foundations of an agent’s behavior, distinguishing between explanation generation, communication and reception. It is a generic framework (i.e., the core is domain-agnostic and the perceptual layer is model-agnostic), and being developed and tested in the domains of transport, health-care and defense. The perceptual level entails the provision of an Intuitive Confidence Measure and the identification of the “foil” in a contrastive explanation. The cognitive level entails the selection of the beliefs, goals and emotions for explanations. Ontology Design Patterns are being constructed for the reasoning and communication, whereas Interaction Design Patterns are being constructed for the shaping of the multimodal communication. First results show (1) positive effects on human’s understanding of the perceptual and cognitive foundation of agent’s behavior, and (2) the need for harmonizing the explanations to the context and human’s information processing capabilities.},
	language  = {en},
	booktitle = {Engineering {Psychology} and {Cognitive} {Ergonomics}},
	publisher = {Springer International Publishing},
	author    = {Neerincx, Mark A. and van der Waa, Jasper and Kaptein, Frank and van Diggelen, Jurriaan},
	editor    = {Harris, Don},
	year      = {2018},
	keywords  = {Cognitive engineering, Design patterns, Explainable AI, Human-agent teamwork, Ontologies},
	pages     = {204--214},
	file      = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/WAPJBCEF/Neerincx et al. - 2018 - Using Perceptual and Cognitive Explanations for En.pdf:application/pdf}
}

@article{nunes_systematic_2017,
	title    = {A systematic review and taxonomy of explanations in decision support and recommender systems},
	volume   = {27},
	issn     = {1573-1391},
	doi      = {10.1007/s11257-017-9195-0},
	abstract = {With the recent advances in the field of artificial intelligence, an increasing number of decision-making tasks are delegated to software systems. A key requirement for the success and adoption of such systems is that users must trust system choices or even fully automated decisions. To achieve this, explanation facilities have been widely investigated as a means of establishing trust in these systems since the early years of expert systems. With today’s increasingly sophisticated machine learning algorithms, new challenges in the context of explanations, accountability, and trust towards such systems constantly arise. In this work, we systematically review the literature on explanations in advice-giving systems. This is a family of systems that includes recommender systems, which is one of the most successful classes of advice-giving software in practice. We investigate the purposes of explanations as well as how they are generated, presented to users, and evaluated. As a result, we derive a novel comprehensive taxonomy of aspects to be considered when designing explanation facilities for current and future decision support systems. The taxonomy includes a variety of different facets, such as explanation objective, responsiveness, content and presentation. Moreover, we identified several challenges that remain unaddressed so far, for example related to fine-grained issues associated with the presentation of explanations and how explanation facilities are evaluated.},
	language = {en},
	number   = {3},
	urldate  = {2021-05-24},
	journal  = {User Modeling and User-Adapted Interaction},
	author   = {Nunes, Ingrid and Jannach, Dietmar},
	month    = dec,
	year     = {2017},
	keywords = {Explanation: Design, Explanation: Purpose},
	pages    = {393--444},
	file     = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/L98FBGB5/Nunes and Jannach - 2017 - A systematic review and taxonomy of explanations i.pdf:application/pdf}
}

@inproceedings{schrills_color_2020,
	address   = {Cham},
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {Color for {Characters} - {Effects} of {Visual} {Explanations} of {AI} on {Trust} and {Observability}},
	isbn      = {978-3-030-50334-5},
	doi       = {10.1007/978-3-030-50334-5_8},
	abstract  = {The present study investigates the effects of prototypical visualization approaches aimed at increasing the explainability of machine learning systems in regard to perceived trustworthiness and observability. As the amount of processes automated by artificial intelligence (AI) increases, so does the need to investigate users’ perception. Previous research on explainable AI (XAI) tends to focus on technological optimization. The limited amount of empirical user research leaves key questions unanswered, such as which XAI designs actually improve perceived trustworthiness and observability. We assessed three different visual explanation approaches, consisting of either only a table with classification scores used for classification, or, additionally, one of two different backtraced visual explanations. In a within-subjects design with N = 83 we examined the effects on trust and observability in an online experiment. While observability benefitted from visual explanations, information-rich explanations also led to decreased trust. Explanations can support human-AI interaction, but differentiated effects on trust and observability have to be expected. The suitability of different explanatory approaches for individual AI applications should be further examined to ensure a high level of trust and observability in e.g. automated image processing.},
	language  = {en},
	booktitle = {Artificial {Intelligence} in {HCI}},
	publisher = {Springer International Publishing},
	author    = {Schrills, Tim and Franke, Thomas},
	editor    = {Degen, Helmut and Reinerman-Jones, Lauren},
	year      = {2020},
	keywords  = {Explainable AI, Explanation: Design, Human-AI interaction, Human-automation interaction, Machine learning, Trust in Automation},
	pages     = {121--135},
	file      = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/TFPHECEI/Schrills and Franke - 2020 - Color for Characters - Effects of Visual Explanati.pdf:application/pdf}
}

@inproceedings{sassoon_explainable_2019,
	address   = {Cham},
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {Explainable {Argumentation} for {Wellness} {Consultation}},
	isbn      = {978-3-030-30391-4},
	doi       = {10.1007/978-3-030-30391-4_11},
	abstract  = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers’ intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
	language  = {en},
	booktitle = {Explainable, {Transparent} {Autonomous} {Agents} and {Multi}-{Agent} {Systems}},
	publisher = {Springer International Publishing},
	author    = {Sassoon, Isabel and Kökciyan, Nadin and Sklar, Elizabeth and Parsons, Simon},
	editor    = {Calvaresi, Davide and Najjar, Amro and Schumacher, Michael and Främling, Kary},
	year      = {2019},
	keywords  = {Explainability, Explainable AI, Explanation, Interpretability, Transparency},
	pages     = {186--202},
	file      = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/VNGF5GRG/Sassoon et al. - 2019 - Explainable Argumentation for Wellness Consultatio.pdf:application/pdf;Springer Full Text PDF:/Users/fonok3/Zotero/storage/9U62JCSC/Sassoon et al. - 2019 - Explainable Argumentation for Wellness Consultatio.pdf:application/pdf}
}

@inproceedings{zhu_effects_2020,
	address   = {Cham},
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {Effects of {Proactive} {Explanations} by {Robots} on {Human}-{Robot} {Trust}},
	isbn      = {978-3-030-62056-1},
	doi       = {10.1007/978-3-030-62056-1_8},
	abstract  = {The performance of human-robot teams depends on human-robot trust, which in turn depends on appropriate robot-to-human transparency. A key way for robots to build trust through transparency is by providing appropriate explanations for their actions. While most previous work on robot explanation generation has focused on robots’ ability to provide post-hoc explanations upon request, in this paper we instead examine proactive explanations generated before actions are taken, and the effect this has on human-robot trust. Our results suggest a positive relationship between proactive explanations and human-robot trust, and reveal fundamental new questions into the effects of proactive explanations on the nature of humans’ mental models and the fundamental nature of human-robot trust.},
	language  = {en},
	booktitle = {Social {Robotics}},
	publisher = {Springer International Publishing},
	author    = {Zhu, Lixiao and Williams, Thomas},
	editor    = {Wagner, Alan R. and Feil-Seifer, David and Haring, Kerstin S. and Rossi, Silvia and Williams, Thomas and He, Hongsheng and Sam Ge, Shuzhi},
	year      = {2020},
	keywords  = {Explanation, Explanation: Kind, Human-robot interaction, Human-robot trust, Transparency},
	pages     = {85--95},
	file      = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/ECI97NTS/Zhu and Williams - 2020 - Effects of Proactive Explanations by Robots on Hum.pdf:application/pdf}
}

@inproceedings{goram_supporting_2019,
	address   = {Cham},
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {Supporting {Privacy} {Control} and {Personalized} {Data} {Usage} {Explanations} in a {Context}-{Based} {Adaptive} {Collaboration} {Environment}},
	isbn      = {978-3-030-34974-5},
	doi       = {10.1007/978-3-030-34974-5_8},
	abstract  = {The General Data Protection Regulation, e.g., provides the “right of access by the data subject” and demands explanations of data usages, i.e. explanations where and for what purpose personal data is being processed. Supporting this kind of privacy control and related personalized explanations of data usage in context-based adaptive collaboration environments are big challenges. Currently, users cannot retrace the usage and the storage of their personal data in context-based adaptive collaboration environments. We address the aforementioned challenges by developing a context-based adaptive collaboration platform, the CONTact platform, that can be linked to or integrated into different kinds of collaboration environments (e.g., meinDorf55+, a novel community support system for elderly). The CONTact platform supports users with privacy control and personalized explanations of data usages. In this paper we present an excerpt of our extended domain model and two sample situations when privacy control and personalized explanations get relevant. We use a sample ontology that is based on our domain model to illustrate the related processes and rules. Using our approach users can control their data usage and are able to get personalized explanations of their data usage in a context-based adaptive collaboration environment. This helps us observing legal regulations, e.g. privacy laws like the GDPR.},
	language  = {en},
	booktitle = {Modeling and {Using} {Context}},
	publisher = {Springer International Publishing},
	author    = {Goram, Mandy and Veiel, Dirk},
	editor    = {Bella, Gábor and Bouquet, Paolo},
	year      = {2019},
	keywords  = {Adaptive, Collaboration environment, Context-based, GDPR, Legal regulations, Personalized explanations, Privacy control},
	pages     = {84--97},
	file      = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/7A2YIFR9/Goram and Veiel - 2019 - Supporting Privacy Control and Personalized Data U.pdf:application/pdf}
}

@inproceedings{wang_is_2018,
	address    = {Cham},
	series     = {Lecture {Notes} in {Computer} {Science}},
	title      = {Is {It} {My} {Looks}? {Or} {Something} {I} {Said}? {The} {Impact} of {Explanations}, {Embodiment}, and {Expectations} on {Trust} and {Performance} in {Human}-{Robot} {Teams}},
	isbn       = {978-3-319-78978-1},
	shorttitle = {Is {It} {My} {Looks}?},
	doi        = {10.1007/978-3-319-78978-1_5},
	language   = {en},
	booktitle  = {Persuasive {Technology}},
	publisher  = {Springer International Publishing},
	author     = {Wang, Ning and Pynadath, David V. and Rovira, Ericka and Barnes, Michael J. and Hill, Susan G.},
	editor     = {Ham, Jaap and Karapanos, Evangelos and Morita, Plinio P. and Burns, Catherine M.},
	year       = {2018},
	pages      = {56--69},
	file       = {Springer Full Text PDF:/Users/fonok3/Zotero/storage/4LU3PHEF/Wang et al. - 2018 - Is It My Looks Or Something I Said The Impact of.pdf:application/pdf}
}

@inproceedings{vivacqua_explanations_2019,
	address   = {Panama City Panama},
	title     = {Explanations and sensemaking with {AI} and {HCI}},
	isbn      = {978-1-4503-7679-2},
	doi       = {10.1145/3358961.3359004},
	language  = {en},
	urldate   = {2021-05-24},
	booktitle = {Proceedings of the {IX} {Latin} {American} {Conference} on {Human} {Computer} {Interaction}},
	publisher = {ACM},
	author    = {Vivacqua, Adriana S and Stelling, Roberto and Garcia, Ana Cristina B and Gouvea, Livia C},
	month     = sep,
	year      = {2019},
	keywords  = {artificial intelligence, explainability, Explanation: Design, human computer interaction, human-centered AI, information visualization, machine learning},
	pages     = {1--4},
	file      = {Vivacqua et al. - 2019 - Explanations and sensemaking with AI and HCI.pdf:/Users/fonok3/Zotero/storage/LUJJAAYT/Vivacqua et al. - 2019 - Explanations and sensemaking with AI and HCI.pdf:application/pdf}
}

@inproceedings{wiegand_id_2020,
	address   = {Oldenburg Germany},
	title     = {“{I}’d like an {Explanation} for {That}!”{Exploring} {Reactions} to {Unexpected} {Autonomous} {Driving}},
	isbn      = {978-1-4503-7516-0},
	doi       = {10.1145/3379503.3403554},
	abstract  = {Autonomous vehicles are complex systems that may behave in unexpected ways. From the drivers’ perspective, this can cause stress and lower trust and acceptance of autonomous driving. Prior work has shown that explanation of system behavior can mitigate these negative effects. Nevertheless, it remains unclear in which situations drivers actually need an explanation and what kind of interaction is relevant to them. Using thematic analysis of real-world experience reports, we first identified 17 situations in which a vehicle behaved unexpectedly. We then conducted a think-aloud study (N = 26) in a driving simulator to validate these situations and enrich them with qualitative insights about drivers’ need for explanation. We identified six categories to describe the main concerns and topics during unexpected driving behavior (emotion and evaluation, interpretation and reason, vehicle capability, interaction, future driving prediction and explanation request times). Based on these categories, we suggest design implications for autonomous vehicles, in particular related to collaboration insights, user mental models and explanation requests.},
	language  = {en},
	urldate   = {2021-05-24},
	booktitle = {22nd {International} {Conference} on {Human}-{Computer} {Interaction} with {Mobile} {Devices} and {Services}},
	publisher = {ACM},
	author    = {Wiegand, Gesa and Eiband, Malin and Haubelt, Maximilian and Hussmann, Heinrich},
	month     = oct,
	year      = {2020},
	keywords  = {Autonomous driving., Explainable AI, Explanation: Interaction?, Explanation: When?, Unexpected driving behavior},
	pages     = {1--11},
	file      = {Wiegand et al. - 2020 - “I’d like an Explanation for That!”Exploring React.pdf:/Users/fonok3/Zotero/storage/L88J6WLD/Wiegand et al. - 2020 - “I’d like an Explanation for That!”Exploring React.pdf:application/pdf}
}

@inproceedings{zahedi_towards_2019,
	address   = {Daegu, Korea (South)},
	title     = {Towards {Understanding} {User} {Preferences} for {Explanation} {Types} in {Model} {Reconciliation}},
	isbn      = {978-1-5386-8555-6},
	doi       = {10.1109/HRI.2019.8673097},
	abstract  = {Recent work has formalized the explanation process in the context of automated planning as one of model reconciliation – i.e. a process by which the planning agent can bring the explainee’s (possibly faulty) model of a planning problem closer to its understanding of the ground truth until both agree that its plan is the best possible. The content of explanations can thus range from misunderstandings about the agent’s beliefs (state), desires (goals) and capabilities (action model). Though existing literature has considered different kinds of these model differences to be equivalent, literature on the explanations in social sciences has suggested that explanations with similar logical properties may often be perceived differently by humans. In this brief report, we explore to what extent humans attribute importance to different kinds of model differences that have been traditionally considered equivalent in the model reconciliation setting. Our results suggest that people prefer the explanations which are related to the effects of actions.},
	language  = {en},
	urldate   = {2021-05-24},
	booktitle = {2019 14th {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction} ({HRI})},
	publisher = {IEEE},
	author    = {Zahedi, Zahra and Olmo, Alberto and Chakraborti, Tathagata and Sreedharan, Sarath and Kambhampati, Subbarao},
	month     = mar,
	year      = {2019},
	keywords  = {Artificial intelligence, Cognitive science, Computational modeling, Context modeling, Logistics, Planning},
	pages     = {648--649},
	file      = {Zahedi et al. - 2019 - Towards Understanding User Preferences for Explana.pdf:/Users/fonok3/Zotero/storage/X97ZZ7S2/Zahedi et al. - 2019 - Towards Understanding User Preferences for Explana.pdf:application/pdf}
}

@inproceedings{yamada_evaluating_2016,
	address   = {Kumamoto, Japan},
	title     = {Evaluating {Explanation} {Function} in {Railway} {Crew} {Rescheduling} {System} by {Think}-{Aloud} {Test}},
	isbn      = {978-1-4673-8985-3},
	doi       = {10.1109/IIAI-AAI.2016.93},
	abstract  = {A previously developed interactive system with an explanation function generates crew schedules automatically by using a rule base in an ‘if-then-because’ format. Delivering ‘because’ information to the user is helpful for decision making because it visualizes the computing process. We evaluated the effectiveness of the explanation function by think-aloud testing in which the verbal comments of each participant were recorded while the participant was solving a pre-specified problem of crew rescheduling in our system. A cognitive model for using the system was obtained in a user state transition form by protocol analysis of the comments. The results revealed that users spent 28\% of their time for interpreting ‘because’ information and this information played an important role in the decision making. Interactive processing with the help of the explanation function is practical for real-time railway crew rescheduling.},
	language  = {en},
	urldate   = {2021-05-24},
	booktitle = {2016 5th {IIAI} {International} {Congress} on {Advanced} {Applied} {Informatics} ({IIAI}-{AAI})},
	publisher = {IEEE},
	author    = {Yamada, Takaaki and Sato, Tatsuhiro and Tomiyama, Tomoe and Ueki, Nobutaka},
	month     = jul,
	year      = {2016},
	keywords  = {Decision making, explanation function, interaction, Interactive systems, Rail transportation, railway crew, Railway engineering, Real-time systems, Schedules, scheduling, Testing, think-aloud},
	pages     = {991--994},
	file      = {Yamada et al. - 2016 - Evaluating Explanation Function in Railway Crew Re.pdf:/Users/fonok3/Zotero/storage/QMK7PELY/Yamada et al. - 2016 - Evaluating Explanation Function in Railway Crew Re.pdf:application/pdf}
}

@article{chi_three_nodate,
	title    = {Three {Types} of {Conceptual} {Change}: {Belief} {Revision}, {Mental} {Model} {Transformation}, and {Categorical} {Shift}},
	language = {en},
	author   = {Chi, Michelene T H},
	pages    = {22},
	file     = {Chi - Three Types of Conceptual Change Belief Revision,.pdf:/Users/fonok3/Zotero/storage/YKYZB7L4/Chi - Three Types of Conceptual Change Belief Revision,.pdf:application/pdf}
}

@inproceedings{ribera2019can,
	title     = {Can we do better explanations? A proposal of user-centered explainable AI.},
	author    = {Ribera, Mireia and Lapedriza, Agata},
	booktitle = {IUI Workshops},
	year      = {2019}
}

@inproceedings{wiegand2019drive,
	title     = {I drive-you trust: Explaining driving behavior of autonomous cars},
	author    = {Wiegand, Gesa and Schmidmaier, Matthias and Weber, Thomas and Liu, Yuanting and Hussmann, Heinrich},
	booktitle = {Extended abstracts of the 2019 chi conference on human factors in computing systems},
	pages     = {1--6},
	year      = {2019}
}

@article{miller2017explainable,
	title   = {Explainable AI: Beware of inmates running the asylum or: How I learnt to stop worrying and love the social and behavioural sciences},
	author  = {Miller, Tim and Howe, Piers and Sonenberg, Liz},
	journal = {arXiv preprint arXiv:1712.00547},
	year    = {2017}
}

@article{jaimes2007guest,
	title     = {HumanCentered Computing: Toward a Human Revolution},
	author    = {Jaimes, Alejandro and Gatica-Perez, Daniel and Sebe, Nicu and Huang, Thomas S},
	journal   = {Computer},
	volume    = {40},
	number    = {5},
	pages     = {30--34},
	year      = {2007},
	publisher = {IEEE}
}

@inproceedings{bilgic2005explaining,
	title     = {Explaining recommendations: Satisfaction vs. promotion},
	author    = {Bilgic, Mustafa and Mooney, Raymond J},
	booktitle = {Beyond Personalization Workshop, IUI},
	volume    = {5},
	pages     = {153},
	year      = {2005}
}

@article{chazette_knowledge_nodate,
	title    = {A {Knowledge} {Catalogue} for {Explainability}: {Deﬁnitions}, {Impacts}, and {Dimensions}},
	abstract = {The growing complexity of software systems and the inﬂuence of software-supported decisions in our society awoke the need for software that is transparent, accountable, and trustable. Explainability has been identiﬁed as a means to achieve these qualities. It is recognized as an emerging non-functional requirement (NFR) that has a signiﬁcant impact on system quality. However, in order to incorporate this NFR into systems, we need to understand what explainability means from a software engineering perspective and how it impacts other quality aspects in a system. This allows for an early analysis of the beneﬁts and possible design issues that arise from interrelationships between different quality aspects. Nevertheless, explainability is currently under-researched in the domain of requirements engineering and there is a lack of conceptual models and knowledge catalogues that support the requirements engineering process and system design. In this work, we bridge this gap by proposing a deﬁnition, a model, and a catalogue for explainability. They illustrate how explainability interacts with other quality aspects and how it may impact various system dimensions. For this purpose, we have conducted an interdisciplinary Systematic Literature Review and validated our ﬁndings with experts in workshops.},
	language = {en},
	year     = {2021},
	author   = {Chazette, Larissa and Brunotte, Wasja and Speith, Timo},
	keywords = {Explainability, Explanations, Interpretability, Non-Functional Requirements, Quality Aspects, Requirements Synergy, Software Transparency},
	pages    = {12}
}

@article{gunning2019darpa,
	title   = {DARPA’s explainable artificial intelligence (XAI) program},
	author  = {Gunning, David and Aha, David},
	journal = {AI Magazine},
	volume  = {40},
	number  = {2},
	pages   = {44--58},
	year    = {2019}
}

@article{doshi2017towards,
	title   = {Towards a rigorous science of interpretable machine learning},
	author  = {Doshi-Velez, Finale and Kim, Been},
	journal = {arXiv preprint arXiv:1702.08608},
	year    = {2017}
}

@article{miller2019explanation,
	title     = {Explanation in artificial intelligence: Insights from the social sciences},
	author    = {Miller, Tim},
	journal   = {Artificial intelligence},
	volume    = {267},
	pages     = {1--38},
	year      = {2019},
	publisher = {Elsevier}
}

@article{Pearl_causal_2009,
	author    = {Judea Pearl},
	title     = {{Causal inference in statistics: An overview}},
	volume    = {3},
	journal   = {Statistics Surveys},
	number    = {none},
	publisher = {Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the Statist. Soc. Canada},
	pages     = {96 -- 146},
	keywords  = {causal effects, causes of effects, confounding, counterfactuals, graphical methods, mediation, policy evaluation, potential-outcome, structural equation models},
	year      = {2009},
	doi       = {10.1214/09-SS057}
}

@inproceedings{lim_2009_assessing,
	title     = {Assessing demand for intelligibility in context-aware applications},
	author    = {Lim, Brian Y and Dey, Anind K},
	booktitle = {Proceedings of the 11th international conference on Ubiquitous computing},
	pages     = {195--204},
	year      = {2009}
}

@article{hoffman2018metrics,
	title   = {Metrics for explainable AI: Challenges and prospects},
	author  = {Hoffman, Robert R and Mueller, Shane T and Klein, Gary and Litman, Jordan},
	journal = {arXiv preprint arXiv:1812.04608},
	year    = {2018}
}

@inproceedings{cheng2019explaining,
	title     = {Explaining decision-making algorithms through UI: Strategies to help non-expert stakeholders},
	author    = {Cheng, Hao-Fei and Wang, Ruotong and Zhang, Zheng and O'Connell, Fiona and Gray, Terrance and Harper, F Maxwell and Zhu, Haiyi},
	booktitle = {Proceedings of the 2019 chi conference on human factors in computing systems},
	pages     = {1--12},
	year      = {2019}
}

@inproceedings{biran2014justification,
	title     = {Justification narratives for individual classifications},
	author    = {Biran, Or and McKeown, Kathleen},
	booktitle = {Proceedings of the AutoML workshop at ICML},
	volume    = {2014},
	pages     = {1--7},
	year      = {2014}
}

@article{chakraborti2017visualizations,
	title   = {Visualizations for an explainable planning agent},
	author  = {Chakraborti, Tathagata and Fadnis, Kshitij P and Talamadupula, Kartik and Dholakia, Mishal and Srivastava, Biplav and Kephart, Jeffrey O and Bellamy, Rachel KE},
	journal = {arXiv preprint arXiv:1709.04517},
	year    = {2017}
}

@article{koo_understanding_2016,
	title    = {Understanding driver responses to voice alerts of autonomous car operations},
	volume   = {70},
	issn     = {0143-3369, 1741-5314},
	doi      = {10.1504/IJVD.2016.076740},
	abstract = {This study explores, in the context of automated braking, how a voice alert accompanying the car’s autonomous action affects the driver’s attitude and driving behaviour. To examine the research question we designed an experimental setup in a simulator environment that (1) enabled automatic braking to perform as a vehicle’s autonomous longitudinal behaviour and (2) enacted a voice alert system in a timely way to notify the driver of pending brake actions. Subjective driving experience and driver responses towards the car were strongly affected by the voice alerts when the car made automated decisions. These results have important implications for the design of vehicle–user interfaces, suggesting that, rather than simply developing a car that executes autonomous decisions, car makers should also focus on the human–machine interaction, i.e., on how the car announces its ‘intentions’ to act.},
	language = {en},
	number   = {4},
	urldate  = {2021-05-26},
	journal  = {International Journal of Vehicle Design},
	author   = {Koo, Jeamin and Shin, Dongjun and Steinert, Martin and Leifer, Larry},
	year     = {2016},
	pages    = {377},
	file     = {Koo et al. - 2016 - Understanding driver responses to voice alerts of .pdf:/Users/fonok3/Zotero/storage/253NSTLP/Koo et al. - 2016 - Understanding driver responses to voice alerts of .pdf:application/pdf}
}

@article{koo_why_2015,
	title      = {Why did my car just do that? {Explaining} semi-autonomous driving actions to improve driver understanding, trust, and performance},
	volume     = {9},
	issn       = {1955-2513, 1955-2505},
	shorttitle = {Why did my car just do that?},
	doi        = {10.1007/s12008-014-0227-2},
	abstract   = {This study explores, in the context of semiautonomous driving, how the content of the verbalized message accompanying the car’s autonomous action affects the driver’s attitude and safety performance. Using a driving simulator with an auto-braking function, we tested different messages that provided advance explanation of the car’s imminent autonomous action. Messages providing only “how” information describing actions (e.g., “The car is braking”) led to poor driving performance, whereas “why” information describing reasoning for actions (e.g., “Obstacle ahead”) was preferred by drivers and led to better driving performance. Providing both “how and why” resulted in the safest driving performance but increased negative feelings in drivers. These results suggest that, to increase overall safety, car makers need to attend not only to the design of autonomous actions but also to the right way to explain these actions to the drivers.},
	language   = {en},
	number     = {4},
	urldate    = {2021-05-26},
	journal    = {International Journal on Interactive Design and Manufacturing (IJIDeM)},
	author     = {Koo, Jeamin and Kwac, Jungsuk and Ju, Wendy and Steinert, Martin and Leifer, Larry and Nass, Clifford},
	month      = nov,
	year       = {2015},
	pages      = {269--275},
	file       = {Koo et al. - 2015 - Why did my car just do that Explaining semi-auton.pdf:/Users/fonok3/Zotero/storage/QJF4TM4F/Koo et al. - 2015 - Why did my car just do that Explaining semi-auton.pdf:application/pdf}
}

@inproceedings{eiband2018bringing,
	title     = {Bringing transparency design into practice},
	author    = {Eiband, Malin and Schneider, Hanna and Bilandzic, Mark and Fazekas-Con, Julian and Haug, Mareike and Hussmann, Heinrich},
	booktitle = {23rd international conference on intelligent user interfaces},
	pages     = {211--223},
	year      = {2018}
}


@article{hoffman_metrics_nodate,
	title    = {Metrics for {Explainable} {AI}: {Challenges} and {Prospects}},
	language = {en},
	author   = {Hoffman, Robert R and Mueller, Shane T and Klein, Gary and Litman, Jordan},
	pages    = {50},
	file     = {Hoffman et al. - Metrics for Explainable AI Challenges and Prospec.pdf:/Users/fonok3/Zotero/storage/AIKTD4ZQ/Hoffman et al. - Metrics for Explainable AI Challenges and Prospec.pdf:application/pdf}
}

@article{tintarev_designing_nodate,
	title    = {Designing and {Evaluating} {Explanations} for {Recommender} {Systems}},
	abstract = {This chapter gives an overview of the area of explanations in recommender systems. We approach the literature from the angle of evaluation: that is, we are interested in what makes an explanation “good”, and suggest guidelines as how to best evaluate this. We identify seven beneﬁts that explanations may contribute to a recommender system, and relate them to criteria used in evaluations of explanations in existing systems, and how these relate to evaluations with live recommender systems. We also discuss how explanations can be affected by how recommendations are presented, and the role the interaction with the recommender system plays w.r.t. explanations. Finally, we describe a number of explanation styles, and how they may be related to the underlying algorithms. Examples of explanations in existing systems are mentioned throughout.},
	language = {en},
	author   = {Tintarev, Nava and Masthoff, Judith},
	pages    = {32},
	file     = {Designing and Evaluating Explanations for Recommender Systems.pdf:/Users/fonok3/Zotero/storage/24TQCDRK/Designing and Evaluating Explanations for Recommender Systems.pdf:application/pdf},
	year     = {2010}
}


@article{sato_context_nodate,
	title  = {Context {Style} {Explanation} for {Recommender} {Systems}},
	doi    = {10.2197/ipsjjip.27.720},
	author = {Sato, Masahiro and Nagatani, Koki and Sonoda, Takashi and Zhang, Qian and Ohkuma, Tomoko},
	file   = {Context Style Explanation for Recommender Systems.pdf:/Users/fonok3/Zotero/storage/4DRRL2YT/Context Style Explanation for Recommender Systems.pdf:application/pdf}
}

@inproceedings{tintarev2007survey,
	title        = {A survey of explanations in recommender systems},
	author       = {Tintarev, Nava and Masthoff, Judith},
	booktitle    = {2007 IEEE 23rd international conference on data engineering workshop},
	pages        = {801--810},
	year         = {2007},
	organization = {IEEE}
}

@book{wohlin2012experimentation,
	title     = {Experimentation in software engineering},
	author    = {Wohlin, Claes and Runeson, Per and H{\"o}st, Martin and Ohlsson, Magnus C and Regnell, Bj{\"o}rn and Wessl{\'e}n, Anders},
	year      = {2012},
	publisher = {Springer Science \& Business Media}
}

@inproceedings{ghazi2016exploratory,
	title        = {An exploratory study on user interaction challenges when handling interconnected requirements artifacts of various sizes},
	author       = {Ghazi, Parisa and Glinz, Martin},
	booktitle    = {2016 IEEE 24th International Requirements Engineering Conference (RE)},
	pages        = {76--85},
	year         = {2016},
	organization = {IEEE}
}

@inproceedings{carvalho2020developers,
	title        = {How developers believe Invisibility impacts NFRs related to User Interaction},
	author       = {Carvalho, Rainara Maia and Andrade, Rossana MC and Oliveira, K{\'a}thia M},
	booktitle    = {2020 IEEE 28th International Requirements Engineering Conference (RE)},
	pages        = {102--112},
	year         = {2020},
	organization = {IEEE}
}

@article{do2010software,
	title     = {Software transparency},
	author    = {do Prado Leite, Julio Cesar Sampaio and Cappelli, Claudia},
	journal   = {Business \& Information Systems Engineering},
	volume    = {2},
	number    = {3},
	pages     = {127--139},
	year      = {2010},
	publisher = {Springer}
}

@misc{international2011iso,
	title     = {ISO/IEC 25010: Systems and software engineering-systems and Software Quality Requirements and Evaluation (SQuaRE)},
	author    = {INTERNATIONAL ORGANIZATION FOR STANDARDIZATION},
	year      = {2011},
	publisher = {System and software quality models Geneva}
}

@article{chazette2020explainability,
	title     = {Explainability as a non-functional requirement: challenges and recommendations},
	author    = {Chazette, Larissa and Schneider, Kurt},
	journal   = {Requirements Engineering},
	volume    = {25},
	number    = {4},
	pages     = {493--514},
	year      = {2020},
	publisher = {Springer}
}

@phdthesis{wang_integration_2020,
	type   = {Bachelor},
	title  = {Integration and {Evaluation} of {Explanations} in the {Context} of a {Navigation} {App}},
	school = {Leibniz University Hanover},
	author = {Wang, Zhongpin},
	year   = {2020}
}

@book{schneider2012abenteuer,
	title     = {Abenteuer Softwarequalit{\"a}t: Grundlagen und Verfahren f{\"u}r Qualit{\"a}tssicherung und Qualit{\"a}tsmanagement},
	author    = {Schneider, Kurt},
	year      = {2012},
	publisher = {dpunkt. verlag}
}

@article{briand1995goal,
	title  = {Goal-driven definition of product metrics based on properties},
	author = {Briand, Lionel and Morasca, Sandro and Basili, Victor R},
	year   = {1995}
}

@inproceedings{anjomshoae2019explainable,
	title        = {Explainable agents and robots: Results from a systematic literature review},
	author       = {Anjomshoae, Sule and Najjar, Amro and Calvaresi, Davide and Fr{\"a}mling, Kary},
	booktitle    = {18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019), Montreal, Canada, May 13--17, 2019},
	pages        = {1078--1088},
	year         = {2019},
	organization = {International Foundation for Autonomous Agents and Multiagent Systems}
}

@book{golledge1999wayfinding,
	title     = {Wayfinding behavior: Cognitive mapping and other spatial processes},
	author    = {Golledge, Reginald G and others},
	year      = {1999},
	publisher = {JHU press}
}

@book{bovy2012route,
	title     = {Route Choice: Wayfinding in Transport Networks: Wayfinding in Transport Networks},
	author    = {Bovy, Piet H and Stern, Eliahu},
	volume    = {9},
	year      = {2012},
	publisher = {Springer Science \& Business Media}
}

@article{rajnish2010quality,
	author  = {Rajnish, Ranjana and Dev, Prof and Rajnish, Vyas},
	year    = {2010},
	month   = {06},
	pages   = {},
	title   = {Writing Quality Requirements (SRS): An Approach To Manage Requirements Volatility},
	volume  = {1},
	journal = {Indian Journal of Computer Science and Engineering}
}

@book{alexander2002writing,
	title     = {Writing better requirements},
	author    = {Alexander, Ian F and Stevens, Richard},
	year      = {2002},
	publisher = {Pearson Education}
}

@article{wiegers1999writing,
	title     = {Writing quality requirements},
	author    = {Wiegers, Karl E},
	journal   = {Software Development},
	volume    = {7},
	number    = {5},
	pages     = {44--48},
	year      = {1999},
	publisher = {{Miller Freeman, Inc. Lawrence, KS, USA}}
}

@incollection{tintarev2015explaining,
	title     = {Explaining recommendations: Design and evaluation},
	author    = {Tintarev, Nava and Masthoff, Judith},
	booktitle = {Recommender systems handbook},
	pages     = {353--382},
	year      = {2015},
	publisher = {Springer}
}

@article{knijnenburg2012explaining,
	title     = {Explaining the user experience of recommender systems},
	author    = {Knijnenburg, Bart P and Willemsen, Martijn C and Gantner, Zeno and Soncu, Hakan and Newell, Chris},
	journal   = {User Modeling and User-Adapted Interaction},
	volume    = {22},
	number    = {4},
	pages     = {441--504},
	year      = {2012},
	publisher = {Springer}
}

@incollection{chung2009non,
	title     = {On non-functional requirements in software engineering},
	author    = {Chung, Lawrence and do Prado Leite, Julio Cesar Sampaio},
	booktitle = {Conceptual modeling: Foundations and applications},
	pages     = {363--379},
	year      = {2009},
	publisher = {Springer}
}

@book{hleg2019policy,
	author    = {{High-Level Expert Group on Artificial Intelligence}},
	title     = {Policy and investment recommendations for trustworthy AI},
	subtitle  = {A subtitle (optional)},
	year      = {2019},
	publisher = {The European Commission}
}

@article{mayer1999effect,
	title     = {The effect of the performance appraisal system on trust for management: A field quasi-experiment.},
	author    = {Mayer, Roger C and Davis, James H},
	journal   = {Journal of applied psychology},
	volume    = {84},
	number    = {1},
	pages     = {123},
	year      = {1999},
	publisher = {American Psychological Association}
}

@phdthesis{schaefer2013perception,
	title  = {The perception and measurement of human-robot trust},
	author = {Schaefer, Kristin},
	school = {University of Central Florida},
	type   = {Doctor of Philosophy},
	year   = {2013}
}

@book{norman1988psychology,
	title     = {The psychology of everyday things.},
	author    = {Norman, Donald A},
	year      = {1988},
	publisher = {Basic books}
}

@incollection{cypko2017guide,
	title     = {A guide for constructing bayesian network graphs of cancer treatment decisions},
	author    = {Cypko, Mario A and Stoehr, Matthaeus and Oeltze-Jafra, Steffen and Dietz, Andreas and Lemke, Heinz U},
	booktitle = {MEDINFO 2017: Precision Healthcare through Informatics},
	pages     = {1355--1355},
	year      = {2017},
	publisher = {IOS Press}
}

@inproceedings{cawsey1991generating,
	title        = {Generating Interactive Explanations.},
	author       = {Cawsey, Alison},
	booktitle    = {AAAI},
	pages        = {86--91},
	year         = {1991},
	organization = {Citeseer}
}

@incollection{byrne1991construction,
	title     = {The construction of explanations},
	author    = {Byrne, Ruth MJ},
	booktitle = {AI and Cognitive Science’90},
	pages     = {337--351},
	year      = {1991},
	publisher = {Springer}
}


@inproceedings{gilpin_explaining_2018,
	title     = {Explaining {Explanations}: {An} {Overview} of {Interpretability} of {Machine} {Learning}},
	doi       = {10.1109/DSAA.2018.00018},
	booktitle = {2018 {IEEE} 5th {International} {Conference} on {Data} {Science} and {Advanced} {Analytics} ({DSAA})},
	author    = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
	month     = oct,
	year      = {2018},
	keywords  = {Artificial intelligence, Taxonomy, Biological neural networks, Computational modeling, Decision trees, Complexity theory, Deep learning and deep analytics, Fairness and transparency in data science, Machine learning theories, Models and systems},
	pages     = {80--89},
	file      = {Gilpin et al. - 2018 - Explaining Explanations An Overview of Interpreta.pdf:/Users/fonok3/Zotero/storage/PVC4TBJS/Gilpin et al. - 2018 - Explaining Explanations An Overview of Interpreta.pdf:application/pdf}
}

@inproceedings{fong_interpretable_2017,
	title     = {Interpretable {Explanations} of {Black} {Boxes} by {Meaningful} {Perturbation}},
	doi       = {10.1109/ICCV.2017.371},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author    = {Fong, Ruth C. and Vedaldi, Andrea},
	month     = oct,
	year      = {2017},
	note      = {ISSN: 2380-7504},
	keywords  = {Neural networks, Visualization, Prediction algorithms, Perturbation methods, Machine learning algorithms, Gradient methods, Backpropagation},
	pages     = {3449--3457}
}

@incollection{samek_towards_2019,
	series    = {Lecture {Notes} in {Computer} {Science}},
	title     = {Towards {Explainable} {Artificial} {Intelligence}},
	isbn      = {978-3-030-28954-6},
	url       = {10.1007/978-3-030-28954-6_1},
	language  = {en},
	urldate   = {2021-05-24},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author    = {Samek, Wojciech and Müller, Klaus-Robert},
	year      = {2019},
	doi       = {10.1007/978-3-030-28954-6_1},
	pages     = {5--22}
}

@online{eu_verordnung_2016,
	chapter    = {KAPITEL 1 - Allgemeine bestimmungen},
	title      = {{Verordnung} ({EU}) 2016/679 {des} {europäischen} {Parlaments} {und} {des} {Rates} zum {Schutz} natürlicher {Personen} bei der {Verarbeitung} personenbezogener {Daten}, zum freien {Datenverkehr} und zur {Aufhebung} der {Richtlinie} 95/46/{EG} ({Datenschutz}-{Grundverordnung})},
	shorttitle = {{Datenschutz}-{Grundverordnung}},
	url        = {https://eur-lex.europa.eu/legal-content/DE/TXT/HTML/?uri=CELEX:32016R0679&qid=1629269806673&from=EN#d1e40-1-1},
	urldate    = {2021-08-18},
	author     = {{Europäisches Parlament} and {Rat der euopäischen Union}},
	month      = may,
	year       = {2016}
}

@misc{mosseri_shedding_2021,
	title   = {Shedding {More} {Light} on {How} {Instagram} {Works}},
	url     = {https://about.instagram.com/blog/announcements/shedding-more-light-on-how-instagram-works},
	urldate = {2021-08-18},
	author  = {Mosseri, Adam},
	month   = jun,
	year    = {2021}
}


@misc{tiktok_technology_limited_how_2021,
	title   = {How {TikTok} recommends videos \#{ForYou}},
	url     = {https://newsroom.tiktok.com/en-us/how-tiktok-recommends-videos-for-you/},
	urldate = {2021-08-18},
	author  = {{TikTok Technology Limited}},
	month   = jun,
	year    = {2021}
}

@online{cfpb_regulation_2018,
	title      = {12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B)},
	shorttitle = {{Datenschutz}-{Grundverordnung}},
	url        = {https://www.consumerfinance.gov/rules-policy/regulations/1002/9/},
	urldate    = {2021-08-18},
	author     = {Consumer Financial Protection Bureau},
	month      = jan,
	year       = {2018}
}

@article{dunn1964multiple,
	title     = {Multiple comparisons using rank sums},
	author    = {Dunn, Olive Jean},
	journal   = {Technometrics},
	volume    = {6},
	number    = {3},
	pages     = {241--252},
	year      = {1964},
	publisher = {Taylor \& Francis}
}

@article{BAHR2011776,
	title    = {How and why pop-ups don’t work: Pop-up prompted eye movements, user affect and decision making},
	journal  = {Computers in Human Behavior},
	volume   = {27},
	number   = {2},
	pages    = {776-783},
	year     = {2011},
	note     = {Web 2.0 in Travel and Tourism: Empowering and Changing the Role of Travelers},
	issn     = {0747-5632},
	doi      = {10.1016/j.chb.2010.10.030},
	url      = {https://www.sciencedirect.com/science/article/pii/S0747563210003286},
	author   = {G. Susanne Bahr and Richard A. Ford},
	keywords = {Pop-ups, Affect, Eye tracking, Designing for security, Polite interaction, Design guidelines}
}

@article{basili1988tame,
	author  = {Basili, V.R. and Rombach, H.D.},
	journal = {IEEE Transactions on Software Engineering},
	title   = {The TAME project: towards improvement-oriented software environments},
	year    = {1988},
	volume  = {14},
	number  = {6},
	pages   = {758-773},
	doi     = {10.1109/32.6156}
}

@article{kitchenham2004procedures,
	title   = {Procedures for performing systematic reviews},
	author  = {Kitchenham, Barbara},
	journal = {Keele, UK, Keele University},
	volume  = {33},
	number  = {2004},
	pages   = {1--26},
	year    = {2004},
	doi     = {10.1016/j.infsof.2008.09.009}
}

@article{carvalho2017quality,
	title     = {Quality characteristics and measures for human--computer interaction evaluation in ubiquitous systems},
	author    = {Carvalho, Rainara Maia and de Castro Andrade, Rossana Maria and de Oliveira, K{\'a}thia Mar{\c{c}}al and de Sousa Santos, Ismayle and Bezerra, Carla Ilane Moreira},
	journal   = {Software Quality Journal},
	volume    = {25},
	number    = {3},
	pages     = {743--795},
	year      = {2017},
	publisher = {Springer},
	doi       = {10.1007/s11219-016-9320-z}
}

@inproceedings{mahoney2019framework,
	title        = {A Framework for Explainable Text Classification in Legal Document Review},
	author       = {Mahoney, Christian J and Zhang, Jianping and Huber-Fliflet, Nathaniel and Gronvall, Peter and Zhao, Haozhen},
	booktitle    = {2019 IEEE International Conference on Big Data (Big Data)},
	pages        = {1858--1867},
	year         = {2019},
	organization = {IEEE},
	doi          = {10.1109/BigData47090.2019.9005659}
}

@inproceedings{salgado_cultural_2015,
	author    = {Salgado, Luciana and Pereira, Roberto and Gasparini, Isabela},
	editor    = {Kurosu, Masaaki},
	title     = {Cultural Issues in HCI: Challenges and Opportunities},
	booktitle = {Human-Computer Interaction: Design and Evaluation},
	year      = {2015},
	publisher = {Springer International Publishing},
	address   = {Cham},
	pages     = {60--70},
	isbn      = {978-3-319-20901-2},
	doi       = {10.1007/978-3-319-20901-2_6}
}
