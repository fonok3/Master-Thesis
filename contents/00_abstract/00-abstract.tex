\chapter*{Zusammenfassung}

\subsection*{Konzeption und Evaluation eines Modells zur Unterstützung des Designs von Erklärungen in erklärbaren Systemen}

Mit der zunehmenden Komplexität von Softwaresystemen und deren tiefgreifender Integration in den Alltag der Nutzer wächst der Bedarf an Transparenz, Nachvollziehbarkeit und Vertrauenswürdigkeit. Ein signifikanter Einfluss von Erklärbarkeit als nicht-funktionale Anforderung (NFR) auf diese Qualitätsaspekte sowie auf die Gesamtqualität von Softwaresystemen wurde bereits nachgewiesen.

Da es sich jedoch um eine relativ neue NFR handelt, gibt es noch keine Artefakte wie Richtlinien oder Modelle, die Fachleute bei der Identifizierung und Operationalisierung von Anforderungen im Zusammenhang mit Erklärbarkeit unterstützen. Daher ist es wichtig, dass diese Artefakte vorhanden sind, um den Requirements-Engineering-Prozess für Erklärbarkeit und seine Umsetzung zu erleichtern.

Ein Überblick über die verschiedenen Möglichkeiten zur Integration von Erklärungen in Softwaresysteme kann die Gestaltung von Erklärungen in erklärbaren Systemen unterstützen. Zu diesem Zweck habe ich eine Literaturrecherche durchgeführt, um die externen Abhängigkeiten, Eigenschaften und Bewertungsmethoden von Erklärungen zu identifizieren. Ein exemplarischer Einsatz in der Praxis diente schließlich dazu, die Anwendbarkeit eines entwickelten Leitfadens zu evaluieren, was zu vielversprechenden Ergebnissen hinsichtlich der evaluierten Qualität der resultierenden Erklärungen und der subjektiven Nützlichkeit des Leitfadens führte. 

In dieser Arbeit wird ein Leitfaden vorgestellt, der Vorschläge für die Entwicklung von Erklärungen, einen Katalog von Zusammenhängen zwischen Merkmalen von Erklärungen und Softwarequalitätsaspekten sowie Heuristiken für die Erklärungsgestaltung enthält. Als zukünftiger Beitrag soll der Leitfaden in weiteren Kontexten evaluiert werden und eine weitere Iteration der entwickelten Erklärungen auf Basis der Evaluationsergebnisse entwickelt werden.

Übersetzt mit www.DeepL.com/Translator (kostenlose Version)

\clearpage

\chapter*{Abstract}

\subsection*{Conception and evaluation of a model to support the design of explanations in explainable systems}

With the growing complexity of software systems and their profound integration into users' daily lives, the need for transparent, traceable, and trustworthy awakens. A significant impact of explainability, as a non-functional requirement (NFR), on these quality attributes as well as the overall quality of software systems has already been shown.

However, because it is a relatively new NFR, artifacts such as guidelines or models do not yet exist to assist professionals in identifying and operationalizing requirements related to explainability. Therefore, it is important that these artifacts are in place to facilitate the requirements engineering process for explainability and its implementation.

An overview of the various possibilities for the integration of explanations into software systems may support the design of explanations in explainable systems. For this purpose, I conducted a literature review to identify the external dependencies, characteristics, and evaluation methods of explanations. Finally, an exemplary use in practice served to evaluate the applicability of a developed guideline, which lead to promising results concerning the evaluated quality of the resulting explanations and subjective usefulness of the guideline. 

This thesis presents a guideline containing proposals for the development of explanations, together with a catalog of correlations between characteristics of explanations and software quality aspects, as well as heuristics for explanation design. As a future contribution, the guidelines have to be evaluated in more contexts and another iteration of the developed explanations based on the evaluation results should be developed.

\clearpage