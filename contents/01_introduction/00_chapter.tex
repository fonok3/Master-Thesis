\chapter{Einleitung}

\section{Motivation}

Die Integration von Softwaresystemen in den Alltag der meisten Menschen wird heutzutage immer tiefer und umfassender \cite{carvalho2020developers}.  Gleichzeitig steigt die Größe und Komplexität dieser Systeme. Als Lösung für eine wachsende Zahl an Aufgaben reicht die Nutzung von Software dabei von der Routenberechnung in Navigationssystemen, über das Vorschlagen von Produkten bis zu klinischen Unterstützungssystemen \cite{chazette2020explainability, tintarev2015explaining, cypko2017guide}. Aus diesem Grund basieren auch immer mehr getroffene Entscheidungen auf Algorithmen oder den zugrunde liegenden Daten. Vermehrt werden dabei vor allem für den Endnutzer schwer nachvollziehbare Algorithmen beispielsweise aus dem Bereich des maschinellen Lernens (ML) verwendet. Nutzer stoßen so öfter auf das Problem, dass die eigene Annahme über die Funktionsweise eines Systems nicht mit der wirklichen System-Funktionsweise übereinstimmt oder unvollständig ist \cite{chazette_knowledge_nodate}.

Folglich wächst der Bedarf an Software, die transparent, nachvollziehbar und vertrauenswürdig ist, um nicht nur das Verständnis der Nutzer für die Funktionsweise des Systems zu erhöhen, sondern auch Diskriminierung durch Software vorzubeugen. Neben bereits bekannten Anforderungen an die Softwarequalität \cite{international2011iso} entsteht demnach ein zunehmender Bedarf an Erklärungen innerhalb von Softwaresystemen \cite{chazette_end-users_nodate}.

Bereits heute existieren Bereiche, in denen für Nutzer ein \glqq Recht auf Erklärung\grqq{} gesetzlich verankert ist. Auf europäischer Ebene regelt die \citetitle{eu_verordnung_2016} (DSGVO) \cite{eu_verordnung_2016} im Rahmen der Transparenzverpflichtung ein Recht auf die Aufklärung über die Art der Verarbeitung von personenbezogenen Daten und die Auswirkungen einer automatisierten, daraus resultierenden Entscheidung. Eine vergleichbare Verordnung für das Finanzwesen findet sich auch im amerikanischen Recht \cite{cfpb_regulation_2018}. So soll sichergestellt werden, dass Unterstützungssysteme, wie sie beispielsweise bei der Bewertung der Kreditwürdigkeit von Personen oder in der Versicherungsbranche angewendet werden, objektive und diskriminierungsfreie Empfehlungen geben. Unter die Verarbeitung von personenbezogenen Daten fallen aber auch die Sortierungs- und Empfehlungsalgorithmen sozialer Netzwerke. Erste Umsetzungen solcher Erklärungen zeigen Instagram\footnote{\url{https://about.instagram.com/blog/announcements/sheddingmore-light-on-how-instagram-works}, besucht: 01.10.21} für die Priorisierung von Beiträgen und Tiktok\footnote{\url{https://newsroom.tiktok.com/en-us/how-tiktok-recommends-videos-for-you/}, besucht: 18.08.2021} für die Empfehlung von Videos \cite{mosseri_shedding_2021,tiktok_technology_limited_how_2021}. Instagram erklärt beispielsweise, welche Daten in den Algorithmus einfließen, der die Reihenfolge der Beiträge im Feed von Nutzern festlegt. Hier wird von Instagram vor allem im Rahmen des Datenschutzes erläutert, welche Daten nicht verwendet und wie verwendete Daten geschützt werden.

Aus dem wachsenden Bedarf und Einsatz von Erklärungen entsteht auch die Notwendigkeit der Formalisierung der Betrachtung dieser. Frühe Ansätze für erklärbare, intelligente Systeme (\textit{Explainable Artificial Intelligence}: XAI) stammen bereits von \citeauthor{byrne1991construction} und \citeauthor{cawsey1991generating}, welche die Erklärungsinhalte \cite{byrne1991construction} und die Interaktion mit Erklärungen untersuchen \cite{cawsey1991generating}. Aktuelle Studien auf dem Gebiet befassen sich i.d.R. entweder mit dem technischen Verständnis komplexer Algorithmen zum Beispiel aus dem Bereich des maschinellen Lernens \cite{gilpin_explaining_2018, fong_interpretable_2017, samek_towards_2019} oder dem Einfluss von Erklärungen auf die vom Nutzer wahrgenommene Qualität (externe Qualität \cite{international2011iso}) \cite{nunes_systematic_2017,kouki_user_2017,chazette_end-users_nodate}. Letztere wird in dieser Arbeit näher betrachtet.

Bei der Betrachtung von Erklärungen wird Erklärbarkeit (\textit{Explainability}) als neue Nicht-Funktionale Anforderung (\textit{Non-Functional-Requirement}: NFR) interpretiert, um Abhängigkeiten mit anderen Qualitätsanforderungen zu untersuchen \cite{kohl_explainability_2019, chazette2020explainability}. Erste Ergebnisse auf diesem Gebiet zeigen, dass Erklärbarkeit eine starke Auswirkung auf die Gesamtqualität von Softwaresystemen hat. Auch kann bereits gesagt werden, dass die Integration von Erklärungen in der Regel mit Kompromissen einhergeht, da \textit{Explainability} sowohl positive als auch negative Auswirkungen auf andere Qualitätsaspekte haben kann. Folglich müssen Qualitätsziele gegeneinander abgewogen werden.

Bis dato existieren vor allem Studien, die die Auswirkung einzelner Eigenschaften von Erklärungen auf bestimmte Qualitätsaspekte untersuchen oder einen Überblick über die verschiedenen Einsätze von Erklärungen innerhalb eines bestimmten Kontextes (z.B. Empfehlungssysteme \cite{nunes_systematic_2017}) geben.

Da es sich bei \textit{Erklärbarkeit} um eine neue NFR handelt, fehlen zum aktuellen Zeitpunkt Artefakte, die bei der Anforderungserhebung (Requirements Engineering) für Erklärungen und dessen Operationalisierung unterstützen. Um die Umsetzung im Requirements-Engineering-Prozess zu erleichtern, werden Richtlinien und Modelle benötigt, welche einen Überblick über Vorgehensweisen zur Entwicklung und über Aspekte von Erklärungen geben. 

\section{Lösungsansatz}

Auf der Basis der Definitionen für \textit{Erklärbarkeit} von \citeauthor[]{chazette_knowledge_nodate} wird in dieser Arbeit ein Leitfaden vorgestellt, welcher einen Überblick über die Aspekte von Erklärungen in Softwaresystemen gibt. In einer Literaturrecherche wurden dabei (i) die äußeren Einflüsse auf Erklärungen, (ii) der Bedarf und die Granularität von Erklärungen, sowie (iii) die Evaluation von Erklärungen herausgearbeitet. Diese sind in einem Modell für Erklärungen gebündelt. Auf der Basis werden außerdem die in vorangegangenen Arbeiten gezeigten Zusammenhänge zwischen den einzelnen Aspekten von Erklärungen, sowie dessen Auswirkungen auf die Softwarequalität zusammengefasst.

Dabei konzentriert sich diese Arbeit auf die Qualitätsaspekte, anhand welcher die Qualität von Erklärungen in vorangegangenen Arbeiten bereits indirekt gemessen wurde. Dies hilft unter anderem dabei, die Kompromisse bei der Integration von Erklärungen abzuschätzen. Abschließend werden  Design-Empfehlungen für Erklärungen abgeleitet. Die Ergebnisse aus dem Modell, den Zusammenhängen der einzelnen Aspekte sowie den Design-Empfehlungen werden in einem Leitfaden zusammengefasst.

Im zweiten Teil der Arbeit werden die Resultate hinsichtlich der Anwendbarkeit in der Wirtschaft untersucht. Im Zuge dessen werden Erklärungen anhand des in dieser Arbeit entwickelten Leitfadens in ein bestehendes Navigationssystem integriert und evaluiert. Zunächst wurden auf Grundlage der Ergebnisse einer Nutzer-Feedback-Analyse in einem Workshop Probleme identifiziert, welche durch die Integration von Erklärungen gelöst werden sollen. Darauf aufbauend sind Ziele und Evaluationsmöglichkeiten der Erklärungen sowie Ideen zur Umsetzung zusammengetragen worden. Anhand essen sind konkrete Anforderungen an Erklärungen formuliert, und in dem Navigationssystem umgesetzt worden. Abschließend wurden die integrierten Erklärungen in einer Case-Study sowie einem Quasi-Experiment evaluiert.
% Die Ergebnisse der Anwendung des Leitfadens sind in den in dieser Arbeit entwickelten Leitfaden mit eingeflossen.

\section{Struktur der Arbeit}

Zusammenfassend bietet diese Arbeit einen Leitfaden, welcher die Integration von Erklärungen in bestehende Software erleichtert und als Einstiegspunkt für die Entwicklung detaillierter Modelle für verschiedene Kontexte gesehen werden kann. Dieser Leitfaden unterstützt mit dem enthaltenen Modell erstens die Anforderungserhebung für Erklärungen. Darüber hinaus geben die existierenden Ergebnisse zur Integration Hilfestellung bei der Umsetzung von Erklärungen sowie deren Evaluation.

Im nächsten Kapitel (Kapitel 2) werden die Grundlagen von Erklärbarkeit und NFRs im Allgemeinen sowie verwandte Arbeiten vorgestellt. Kapitel 3 definiert das Forschungsziel und beschreibt die verwendeten Methoden. Die Anwendung dessen folgt in den Kapiteln 4 - 6. Wobei Kapitel 4 die durchgeführte Literaturrecherche, Kapitel 5 den resultierenden Leitfaden bestehend aus Modell und gezeigten Zusammenhängen und Kapitel 6 die Evaluation des Leitfadens mithilfe von Nutzern darstellt. Abschließend werden alle Ergebnisse in den Kapitel 7 und 8 diskutiert, auf ihre Allgemeingültigkeit hin untersucht und zusammengefasst.