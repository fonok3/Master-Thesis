\chapter{Einleitung}

Die Integration von Softwaresystemen in den Alltag der meisten Menschen wird heutzutage immer tiefer und umfassender \cite{carvalho2020developers}.  Gleichzeitig steigt die Größe und Komplexität dieser Systeme. Als Lösung für viele Aufgaben reicht die Nutzung von Software dabei von der Routenberechnung in Navigationssystemen, über das Vorschlagen von Produkten bis zu klinischen Unterstützungssystemen \cite{chazette2020explainability, tintarev2015explaining, cypko2017guide}. Dadurch basieren auch immer mehr getroffene Entscheidungen auf Algorithmen oder den zugrunde liegenden Daten. Immer häufiger werden dabei vor allem für den Endnutzer schwer nachvollziehbare Algorithmen beispielsweise aus dem Bereich des maschinellen Lernens (ML) verwendet. Nutzer stoßen so öfter auf das Problem, dass das eigene mentale Modell nicht mehr mit dem Systemmodell übereinstimmt oder unvollständig ist.

Folglich wächst der Bedarf an Software, die transparent, nachvollziehbar und vertrauenswürdig ist, um nicht nur das Verständnis der Nutzer für die Funktionsweise des Systems zu erhöhen, sondern auch Diskriminierung durch Software vorzubeugen. Neben bekannten Anforderungen an die Softwarequalität \cite{international2011iso} entsteht also zunehmend der Bedarf für Erklärungen innerhalb von Softwaresystemen \cite{chazette_end-users_nodate}.

Bereits heute existieren Bereiche, in denen für die Nutzer ein \glqq Recht auf Erklärung\grqq{} gesetzlich verankert ist. Auf europäischer Ebene regelt die \citetitle{eu_verordnung_2016} (DSGVO) \cite{eu_verordnung_2016} im Rahmen der Transparenzverpflichtung ein Recht auf die Aufklärung über die Art der Verarbeitung von personenbezogenen Daten und die Auswirkungen einer automatisierten, daraus resultierenden Entscheidung. So soll beispielsweise sichergestellt werden, dass Unterstützungssysteme, wie sie beispielsweise bei der Bewertung der Kreditwürdigkeit von Personen oder in der Versicherungsbranche angewendet werden, objektiv und diskriminierungsfreie Empfehlungen geben. Unter die Verarbeitung von personenbezogenen Daten fallen aber auch die Sortierungs- und Empfehlungsalgorithmen sozialer Netzwerke. Erste Umsetzungen solcher Erklärungen zeigen Instagram für die Priorisierung von Beiträgen und Tiktok für die Empfehlung von Videos \cite{mosseri_shedding_2021,tiktok_technology_limited_how_2021}. Eine vergleichbare Verordnung für das Finanzwesen findet sich im amerikanischen Recht \cite{cfpb_regulation_2018}.

Aus diesem wachsenden Bedarf und Einsatz von Erklärungen entsteht auch die Notwendigkeit der Formalisierung der Betrachtung dieser. Frühe Ansätze für erklärbare, intelligente Systeme (XAI) stammen bereits von \citeauthor{byrne1991construction} und \citeauthor{cawsey1991generating}, welche die Erklärungsinhalte \cite{byrne1991construction} und die Interaktion mit Erklärungen untersuchen \cite{cawsey1991generating}. Aktuelle Studien auf dem Gebiet befassen sich i.d.R. entweder mit dem technischen Verständnis von vorwiegend ML-Algorithmen (\textit{Interpretability}) \cite{gilpin_explaining_2018, fong_interpretable_2017, samek_towards_2019} oder dem Einfluss von Erklärungen auf die vom Nutzer wahrgenommene Qualität (externe Qualität \cite{international2011iso}) \cite{nunes_systematic_2017,kouki_user_2017,chazette_end-users_nodate}. Letztere wird in dieser Arbeit näher untersucht.

Bei der Betrachtung von Erklärungen wird \textit{Erklärbarkeit} als neue Nicht-Funktionale Anforderung (NFR) interpretiert, um Abhängigkeiten mit anderen Qualitätsanforderungen zu untersuchen \cite{kohl_explainability_2019, chazette2020explainability}. Erste Ergebnisse auf diesem Gebiet zeigen, dass Erklärbarkeit eine starke Auswirkung auf die Gesamtqualität von Softwaresystemen hat. Bis dato existieren allerdings vor allem Studien, die die Auswirkung einzelner Eigenschaften von Erklärungen auf bestimmte Qualitätsaspekte untersuchen oder einen Überblick über die verschiedenen Einsätze von Erklärungen innerhalb eines bestimmten Kontextes (z.B. Empfehlungssysteme \cite{nunes_systematic_2017}) geben.

Da es sich bei \textit{Erklärbarkeit} um eine neue NFR handelt, fehlt es zum aktuellen Zeitpunkt an Artefakten, die bei der Anforderungserhebung (Requirements Engineering) für Erklärungen und dessen Operationalisierung unterstützen. Um folglich die Umsetzung im Requirements-Engineering-Prozess zu erleichtern, werden Richtlinien und Modelle benötigt, welche einen Überblick über Vorgehensweisen und Aspekte von Erklärungen geben. 

Auf der Basis der Definitionen für \textit{Erklärbarkeit} von \citeauthor[]{chazette_knowledge_nodate} und \citeauthor[]{kohl_explainability_2019} wird in dieser Arbeit ein Modell vorgestellt, welches einen Überblick über die Aspekte von Erklärungen gibt. Innerhalb einer Literaturrecherche wurden dabei (i) die äußeren Einflüsse auf Erklärungen, (ii) die Eigenschaften und der Bedarf von Erklärungen, sowie (iii) die Evaluation derer untersucht. Auf Basis dieser Übersicht fasst diese Arbeit außerdem die in vorangegangenen Arbeiten gezeigten Zusammenhänge zwischen den einzelnen Aspekten sowie dessen Auswirkungen auf die Softwarequalität zusammen. Darüber hinaus werden schließlich Design-Empfehlungen für Erklärungen abgeleitet.

Dieses Modell, bestehend aus dem Überblick über die Aspekte, den Zusammenhängen und Design-Empfehlungen, wird im zweiten Teil der Arbeit auf dessen Anwendbarkeit in der Wirtschaft untersucht. Dabei werden anhand des Modells Erklärungen in ein bestehendes Navigationssystem integriert und evaluiert. Zunächst wurden dafür auf Grundlage der Ergebnisse einer Nutzer-Feedback-Analyse in einem Workshop Probleme identifiziert, welche durch die Integration von Erklärungen gelöst werden sollen. Darauf aufbauend wurden Ziele und Evaluationsmöglichkeiten der Erklärungen sowie Ideen zur Umsetzung zusammengetragen. Final wurden die integrierten Erklärungen in einer Feld-Studie analysiert. Die Ergebnisse aus dieser Anwendung des Modells wurden abschließend in die in dieser Arbeit vorgestellten Version des Modells überführt.

Zusammenfassend bietet diese Arbeit ein Modell, welches die Integration von Erklärungen in bestehende Software erleichtert und als Einstiegspunkt für die Entwicklung detaillierter Modelle für verschiedene Kontexte gesehen werden kann. Darüber hinaus kann es als Basis für die Entwicklung eines Prozesses für die Beachtung von \textit{Erklärbarkeit} im Requirements-Engineering- und Entwicklungs-Prozess dienen.

Im nächsten Kapitel (Kapitel 2) werden die Grundlagen von Erklärbarkeit und NFRs im Allgemeinen sowie verwandte Arbeiten vorgestellt. Kapitel 3 definiert das Forschungsziel und beschreibt die verwendeten Methoden. Die Anwendung dessen folgt in den Kapitel 4 - 6. Wobei Kapitel 4 die durchgeführte Literaturrecherche, Kapitel 5 das resultierende Modell und Kapitel 6 die Evaluation des Modells mithilfe von Nutzern darstellt. Abschließend werden alle Ergebnisse in den Kapitel 7 - 8 zunächst diskutiert, dann auf ihre Allgemeingültigkeit hin untersucht und zum Schluss zusammengefasst.