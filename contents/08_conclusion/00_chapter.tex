\chapter{Fazit und Ausblick}

\section{Fazit}

Ziel dieser Arbeit war es, ein Modell zur Unterstützung des Designs von Erklärungen in erklärbaren Systemen zu konzipieren und im Anschluss zu evaluieren. Als Ergebnis dieser Arbeit ist ein Modell zur Unterstützung der Integration von Erklärungen entstanden, welches in einen Leitfaden integriert ist. Der Leitfaden enthält darüber hinaus einen Katalog über bestehende und verallgemeinerbare Zusammenhänge zwischen den äußeren Abhängigkeiten für Erklärungen, den Eigenschaften und Einflüssen auf die Softwarequalität. Abschließend werden im Leitfaden Heuristiken für das Design von Erklärungen zusammengefasst. Der Leitfaden ist im Rahmen einer Literaturrecherche entstanden, in welcher die bestehenden Ergebnisse für das Design von Erklärungen in erklärbaren Systemen analysiert wurden. Das im Leitfaden für die Integration von Erklärungen enthaltene Modell beinhaltet die folgenden Teile:

\paragraph{RQ1} Unter \textit{External Dependencies} sind alle Rahmenbedingungen zusammengefasst, die einen direkten Einfluss auf die Anforderungen an Erklärungen aufweisen. Als relevante Aspekte sind verschiedene Ausprägungen des \textit{Contexts} von erklärbaren Systemen, sowie die \textit{Objectives} für die Integration von Erklärungen auf verschiedenen Abstraktionsebenen in dem Modell enthalten. Somit unterstützt dieser Modellteil die Anforderungserhebung für Erklärungen.

\paragraph{RQ2} Die Umsetzung der Anforderungen wird in dem vorgestellten Modell durch in der Forschung evaluierte \textit{Characteristics} unterstützt. Dabei enthält das Modell Eigenschaften von Erklärungen für die Umsetzung des Bedarfs für Erklärungen (\textit{Demand}), die transportierten Informationen (\textit{Content}) und die Art der Informationsvermittlung an \textit{End User} (\textit{Presentation}). Die im Modell vorgestellten Ausprägungen bieten unterschiedliche Möglichkeiten, um Erklärungen für verschiedene Kontexte zu gestalten. Aufgeführt sind lediglich Eigenschaften, für die ein Effekt auf die Qualität von Softwaresystemen bereits gezeigt werden konnte.

\paragraph{RQ3} Über die Entwicklung von Erklärungen hinaus bietet das Modell Hilfestellungen für die Evaluation von Erklärungen in einem System. Grundsätzlich werden die im \textit{Software Engineering} üblichen Studienformen vorgeschlagen, welche je nach Ziel einer Evaluation Anwendung finden können \cite[vgl.][]{wohlin2012experimentation}. Außerdem wird zwischen der direkten Messung der Qualität von Erklärungen sowie der Messung der Einflüsse von integrierten Erklärungen auf externe Qualitätsaspekte eines Systems unterschieden.

Um einen ganzheitlichen Überblick über die Qualität von Erklärungen zu erhalten, empfiehlt der Leitfaden, in den das Modell integriert ist, eine Kombination der verschiedenen Evaluationsmöglichkeiten. Die Erklärungen können sowohl direkt evaluiert werden als auch deren Einflüsse auf andere Qualitätsaspekte gemessen werden. Für die Bewertung der Qualität sind außerdem sowohl quantitative als auch qualitative Metriken notwendig, um die Performanz der \textit{End User} wie auch deren subjektive Wahrnehmung zu betrachten.

\paragraph{RQ4} Neben dem Modell für Erklärungen, enthält der Leitfaden einen Katalog der Erklärungen, welcher die Zusammenhänge der Eigenschaften von Erklärungen auf abhängige Qualitätsaspekte zusammenfasst. Die beantwortet zu einen die Frage, unter welchen Bedinungen das Präsentieren von Erklärungen, welche Einflüsse auf ausgewählte externer Softwarequalitätsaspekte hat. Zum anderen werden ebenfalls die Auswirkungen der Granulariät von Erklärungen auf selbige Qualitätsaspekte zusammengefasst.

\bigskip

Abgeleitet aus den ersten Teilen des Leitfadens werden des Weiteren Heuristiken für die Gestaltung von Erklärungen. Dabei werden allgemein anwendbare Zusammenhänge vorgeschlagen, welche im Regelfall für die Integration von Erklärungen beachtet werden sollten.

\bigskip

Ein weiteres Resultat dieser Arbeit ist die erfolgreiche Anwendung des vorgestellten Leitfadens in der Wirtschaft. Mithilfe des Leitfadens konnten im Rahmen eines Workshops zusammen mit der Firma \textit{Graphmasters GmbH} aus Hannover Erklärungen in ein Navigationssystem integriert werden. Die Rohanforderungen, Umsetzungsideen und Ansätze zur Evaluation, welche das Ergebnis des Workshops waren, wurden dazu mithilfe eines Qualitätsmodells in konkrete Anforderungen überführt. Final wurden diese in eine Produktivanwendung integriert und mit über 4~000 \textit{End Usern} der Smartphone-Anwendung evaluiert. Zusammen mit einem anschließenden Quasi-Experiment mit vier Teilnehmern konnte geschlussfolgert werden, dass der Leitfaden bei der Integration von Erklärungen in ein bestehendes System zum einen die Entwicklung unterstützt hat und zum anderen Erklärungen positive Auswirkungen im Rahmen der aufgestellten Ziele erreichen konnte. Darüber hinaus konnte anhand der qualitativen Evaluation Verbesserungspotential der Erklärungen für weitere Iterationen aufgedeckt werden.

\bigskip

Zusammenfassend liefert diese Arbeit einen Leitfaden zur Integration von Erklärungen, welcher die Möglichkeit bietet Erklärungen in ein erklärbares System zu integrieren. Der Leitfaden hat während seiner Anwendung im Unternehmen bei allen wichtige Schritten der Integration von Erklärungen geholfen (Anforderungserhebung, Umsetzung und Evaluation). Mit der Verwendung des Leitfadens konnten die Erklärungen erfolgreich zum Erreichen von aufgestellten Qualitätszielen genutzt werden. Damit ist der Leitfaden ein erster Schritt bei der Entwicklung von Artefakten zur Vereinheitlichung der Entwicklung und Evaluation von Erklärungen in Wirtschaft und Wissenschaft \cite{kohl_explainability_2019,lim_2009_assessing,sokol_explainability_2020}.

Final erfüllt der in dieser Arbeit entwickelt Leitfaden folglich die Anforderung, eine Unterstützung bei der Gestaltung von Erklärungen in erklärbaren Systemen zu bieten.

\section{Ausblick}

Mit der Anwendung des entwickelten Leitfadens in der Wirtschaft konnte gezeigt werden, dass dieser das Ziel der Arbeit erfüllt. Zunächst ist die qualitative Evaluation lediglich mit vier Teilehmern erfolgt, welche bereits verwendbare Rückmeldungen gegeben haben. Eine Ausweitung des Quasi-Experiment würde also vermutlich zu besseren und ggf. statistisch signifikanten Ergebnissen führen.

Außerdem der einzelne Einsatz des Leitfadens nicht ausreichend belegen, dass der Leitfaden allgemeingültig anwendbar ist. Um eine erfolgreiche Verwendung in der Wirtschaft zu gewährleisten, sollte folglich im Anschluss an diese Arbeit ein strukturierter Technologietransfer wie zum Beispiel von \citeauthor{4012630} vorgestellt, durchgeführt werden \cite{4012630}. Insbesondere kann dieser weitere Verständnisprobleme des Leitfadens aufdecken. Je nach Anwendungsfall sollte außerdem in ein finales Artefakt mit dem Leitfaden eine Einführung in das Thema Erklärbarkeit integriert werden.

Zusätzlich sollte die Vollständigkeit des Modells für Erklärungen evaluiert und durch weitere unabhängige Arbeiten zu dem Thema bestätigt werden.

Ein Thema, welches im entwickelten Leitfaden wenig behandelt wird, sind mögliche negative Einflüsse, durch die Integration von Erklärungen. Es gibt einige Autoren, die beispielsweise die Gefahr der Beeinträchtigung der \textit{Usability} von Systemen durch Erklärungen diskutieren \cite{chazette_knowledge_nodate,koo_understanding_2016,kohl_explainability_2019}. Eine Evaluation erfolgt aber in der Regel in Bezug auf positive Einflüsse von Erklärungen. 
% In diesem Rahmen verweisen \citeauthor{lim_2009_assessing} darauf, dass vor allem ein Augenmerk auf die Aufdringlichkeit von Erklärungen gelegt werden sollte (\textit{Obtrusivness}) \cite{lim_2009_assessing}.
Folglich sollten in zukünftigen Arbeiten negative Einflüsse näher  betrachtet werden.

In diesem Zusammenhang fehlt außerdem ein Katalog über die genauen Einflüsse zwischen den verschiedenen Qualitätsaspekten, welche mit \textit{Explainability} in Verbindung stehen. \citeauthor{chazette_knowledge_nodate} haben dafür bereits eine Grundlage mit einem Katalog der Aspekte, welche beieinflusst werden vorgestellt \cite{chazette_knowledge_nodate}. Auf Basis dessen sollte ein \textit{Explainability Softgoal Interdependence Graph (SIG)} entwickelt werden. Dieser sollte über \textit{Explainability} einen ähnlichen Überblick liefern wie bereits existierende SIGs über andere Qualitätsaspekte \cite[vgl.][]{do2010software, carvalho2020developers}.

Folglich müssen in Zukunft weitere Artefakte entwickelt werden, welche die Möglichkeit bieten, die neue NFR \textit{Explainability} in der Wirtschaft einfach anwendbar zu machen, um die Betrachtung in der Wissenschaft zu vereinheitlichen\cite{sokol_explainability_2020}. Außerdem wird ein Prozess benötigt, der die entwickelten Methoden und Artefakte in bestehende Software-Entwicklungszyklen integriert \cite{kohl_explainability_2019, cassens_ambient_2019}.
