\chapter{Fazit und Ausblick}

\section{Fazit}

Ziel dieser Arbeit war es, ein Modell zur Unterstützung des Designs von Erklärungen in erklärbaren Systemen zu konzipieren und im Anschluss zu evaluieren. Als Ergebnis dieser Arbeit ist unter anderem einen ein Modell zur Unterstützung der Integration von Erklärungen entstanden, welches in einen Leitfaden integriert ist. Der Leitfaden enthält darüber hinaus einen Katalog über bestehende und verallgemeinerbare Zusammenhänge zwischen den äußeren Abhängigkeiten für Erklärungen, den Eigenschaften und Einflüssen auf die Softwarequalität. Abschließend werden im Leitfaden außerdem einige wichtige Heuristiken für das Design von Erklärungen zusammengefasst. Dieser ist im Rahmen einer Literaturrecherche entstanden, welche die bestehenden Ergebnisse für das Design von Erklärungen in erklärbaren Systemen analysiert hat. Der im Leitfaden für die Integration von Erklärungen enthaltene Modell beinhaltet dabei die folgenden Teile:

\paragraph{RQ1} Unter \textit{External Dependencies} sind alle Rahmenbedingungen zusammengefasst, die einen direkten Einfluss auf die Anforderungen an Erklärungen aufweisen. Als relevante Aspekte sind verschiedene Ausprägungen des \textit{Contexts} von erklärbaren Systemen, sowie \textit{Objectives} auf verschiedenen Abstraktionsebenen für die Integration von Erklärungen in dem Modell enthalten. Somit unterstützt dieser Modellteil die Anforderungserhebung für Erklärungen.

\paragraph{RQ2} Die Umsetzung der Anforderungen wird in dem vorgestellten Modell durch in der Forschung evaluierte \textit{Characteristics} unterstützt. Dabei enthält das Modell Eigenschaften von Erklärungen für die Umsetzung des Bedarfs für Erklärungen (\textit{Demand}), die transportierten Informationen \textit{Content} und die Art der Informationsvermittlung an \textit{End User} (\textit{Presentation}). Die im Modell vorgestellten Ausprägungen geben dabei viele Möglichkeiten, um Erklärungen für verschiedene Kontexte zu gestalten. Aufgeführt sind lediglich Eigenschaften, für die ein Effekt auf die Qualität von Softwaresystemen bereits gezeigt werden konnte.

\paragraph{RQ3} Über die Entwicklung von Erklärungen hinaus bietet das Modell des weiteren Hilfestellungen für die Evaluation von Erklärungen in einem System. Grundsätzlich werden die im \textit{Software Engineering} üblichen Studienformen vorgeschlagen, welche je nach Ziel einer Evaluation Anwendung finden können \cite[vgl.][]{wohlin2012experimentation}. Außerdem wird zwischen der direkten Messung der Qualität von Erklärungen sowie der Messung der Einflüsse von integrierten Erklärungen auf externe Qualitätsaspekte eines Systems.

% Folgenden Absatz ggf. aus Platzgründen streichen
Um einen ganzheitlichen Überblick über die Qualität von Erklärungen zu erhalten, empfiehlt der Leitfaden, in den das Modell integriert ist, auf Basis existierender Literatur eine Kombination der verschiedenen Evaluationsmöglichkeiten. Einerseits sollten sowohl die Erklärungen sowohl direkt als auch deren Einflüsse auf andere Qualitätsaspekte evaluiert werden. Andererseits sind für Bewertung der Qualität sowohl quantitative als auch qualitative Metriken notwendig, um sowohl die Performanz der \textit{End User} als auch deren subjektive Wahrnehmung zu betrachten.

\paragraph{RQ4} Über das Modell für Erklärungen hinaus, enthält der Leitfaden einen Katalog der Erklärungen, welcher die Zusammenhänge der Eigenschaften auf abhängige Qualitätsaspeke zusammenfasst. Die beantwortet zu einen die Frage, unter welchen Bedinungen das Präsentieren von Erklärungen, welche Einflüsse auf ausgewählte externe Softwarequalitätsaspekte hat. Zum anderen werden ebenfalls die Auswirkungen der Granulariät von Erklärungen auf selbige Qualitätsaspekte zusammengefasst.

\bigskip

Abgeleitet aus den ersten Teilen des Leitfadens wurden in dieser Arbeit des Weiteren Heuristiken für die Gestaltung von Erklärungen. Dabei sind allgemein anwendbare Zusammenhänge, welche im Regelfall für die Integration von Erklärungen beachtet werden sollten, in zehn Empfehlungen zusammengefasst worden.

\bigskip

Ein weiteres Resultat dieser Arbeit ist die erfolgreiche Anwendung des vorgestellten Leitfadens in der Wirtschaft. Mithilfe des Leitfadens konnten im Rahmen eines Workshops zusammen mit der Firma Graphmasters GmbH aus Hannover Erklärungen in ein Navigationssystem integriert werden. Die Rohanforderungen, Umsetzungsideen und Ansätze zur Evaluation, welche das Ergebnis des Workshops waren, wurden dazu mithilfe eines Qualitätsmodells in konkrete Anforderungen überführt. Final wurden diese in eine Produktivanwendung integriert und mit über 4~000 \textit{End Usern} der Smartphone-Anwendung evaluiert. Zusammen mit einem anschließenden Quasi-Experiment mit vier Teilnehmern konnte schließlich geschlussfolgert werden, dass der Leitfaden bei der Integration von Erklärungen in ein bestehendes System zum einen geholfen hat und zum anderen Erklärungen mit positiven Auswirkungen im Rahmen der aufgestellten Ziele erreichen konnte. Darüber hinaus konnte anhand der im Leitfaden vorgeschlagenen qualitativen Evaluation Verbesserungspotential der Erklärungen für weitere Iterationen aufgedeckt werden.

\newpage

Zusammenfassend liefert diese Arbeit einen Leitfaden zur Integration von Erklärungen, welcher die Möglichkeit bietet Erklärungen in ein bestehendes System zu integrieren. Der Leitfaden hat während seiner Anwendung im Unternehmen bei allen wichtige Schritten der Integration von Erklärungen (Anforderungserhebung, Umsetzung und Evaluation) geholfen. Mit der Verwendung des Leitfadens konnten die Erklärungen erfolgreich zum Erreichen von aufgestellten Qualitätszielen genutzt werden. Darüber hinaus sind unter Anwendung des Leitfadens positive Einflüsse auf die zuvor gesetzten Ziele erzielt worden. Damit ist der Leitfaden ein erster Schritt bei der Entwicklung von Artefakten, wie sie zur Vereinheitlichung der Entwicklung und Evaluation von Erklärungen in der Literatur mehrfach gefordert wurden \cite{kohl_explainability_2019,lim_2009_assessing,sokol_explainability_2020}.

Final erfüllt der in dieser Arbeit entwickelt Leitfaden folglich die Anforderung, eine Unterstützung bei der Gestaltung von Erklärungen in erklärbaren Systemen zu bieten.

\section{Ausblick}

Mit der Anwendung des entwickelten Leitfadens in der Wirtschaft konnte gezeigt werden, dass dieser das Ziel der Arbeit erfüllt. Dieser einzelne Einsatz kann jedoch nicht ausreichend belegen, dass der Leitfaden allgemeingültig anwendbar ist. Auch wurde der Leitfaden an sich lediglich wissenschaftlich auf Basis von existierenden Ergebnissen erstellt. Um eine gute Nutzung in der Wirtschaft gewährleisten sollte folglich im Anschluss an diese Arbeit ein strukturierter Technologietransfer wie zum Beispiel von \citeauthor{4012630} vorgestellt, durchgeführt werden \cite{4012630}. Insbesondere kann dies weitere Verständnisprobleme im Leitfaden aufdecken. Je nach Anwendungsfall sollte außerdem in ein finales Artefakt mit dem Leitfaden eine Einführung in das Thema Erklärbarkeit integriert werden.

Zusätzlich sollte die Vollständigkeit des Modells für Erklärungen evaluiert werden und durch weitere unabhängige Arbeiten zu dem Thema bestätigt werden.

Ein Thema, welches im entwickelten Leitfaden wenig behandelt wird, sind mögliche negative Einflüsse, die die Integration von Erklärungen mit sich bringen kann. Es gibt zwar einige Autoren, die beispielsweise die Gefahr der Beeinträchtigung der \textit{Usability} von Systemen durch Erklärungen diskutieren, evaluiert werden in der Literatur aber in der Regel die positiven Einflüsse von Erklärungen \cite{}. In diesem Rahmen verweisen \citeauthor{lim_2009_assessing} darauf, dass vor allem ein Augenmerk auf die Aufdringlichkeit von Erklärungen gelegt werden sollte (\textit{Obtrusivness}) \cite{lim_2009_assessing}. Folglich sollten in zukünftigen Arbeiten negative Einflüsse näher  betrachtet werden.

In diesem Zusammenhang fehlt ein Katalog über die genauen Einflüsse zwischen den verschiedenen Qualitätsaspekten, welche mit \textit{Explainability} in Verbindung stehen. \citeauthor{chazette_knowledge_nodate} haben dafür bereits eine Grundlage mit einem Katalog der Aspekte, welche beieinflusst werden gelegt \cite{chazette_knowledge_nodate}. Auf Basis dessen sollte ein \textit{Explainability Softgoal Interpendency Graph (SIG)} erstellt werden. Dieser muss über \textit{Explainability} einen ähnlichen Überblick liefern wie dies andere SIGs zum Beispiel für  \textit{Transparency} oder \textit{Invisibility} bieten \cite{do2010software, carvalho2020developers}.

Folglich müssen in Zukunft mehr Artefakte entwickelt werden, welche die Möglichkeit bieten die neue NFR \textit{Explainability} in der Wirtschaft einfach andwendbar zu machen \cite{sokol_explainability_2020}. Außerdem wird ein Prozess benötigt der entwickelte Methoden und Artefakte in bestehende Software-Entwicklungszyklen integriert \cite{kohl_explainability_2019, cassens_ambient_2019}.