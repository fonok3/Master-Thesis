\section{Aspekte von Erklärungen}
\label{sec:model_explanation_aspects}

Literatur, die einen Überblick über Erklärbarkeit im Allgemeinen oder in einem bestimmten Anwendungsfeld gibt, betrachtet in der Regel fünf Aspekte von Erklärbarkeit \cite{rosenfeld_explainability_2019, nunes_systematic_2017,chazette_knowledge_nodate}. Dies sind der Kontext der Erklärung, die Zielsetzung dieser, welche Erklärung angezeigt werden soll und wann diese angezeigt werden soll. In einigen Arbeiten wird bei der angezeigten Erklärung außerdem zwischen dem Inhalt und der Darstellung unterschieden bzw. diese Eigenschaften einzeln untersucht \cite{nunes_systematic_2017,abdulrahman_belief-based_2019}. Darüber hinaus wird in allen hier betrachteten Arbeiten auch die Evaluation von Erklärungen thematisiert (Siehe \autoref{sec:literature_review}).

Für die zuvor genannten Aspekte werden in der Literatur verschiedene Unterkategorien konkret benannt oder Synonyme verwendet. \autoref{tab:model_explaination_aspects} fasst die verwendeten Synonyme aus den Veröffentlichungen, welche den Aspekt explizit erwähnt haben, unter der final gewählten Benennung zusammen. Neben den dort aufgezeigten Begriffen haben mehrere Autoren (z.~B. \cite{rosenfeld_explainability_2019, chazette2020explainability}) die verschiedenen Aspekte von Erklärbarkeit zusätzlich mit Fragewörtern verknüpft. Die vollständigen Fragen dahinter verweisen allerdings auf verschiedene Unterpunkte, weswegen das vorgestellte Modell auf Fragewörter verzichtet, um Verwechselungen vorzubeugen. (Beispiel: \glqq \textbf{Wie} kann die Erklärung evaluiert werden?\grqq{} (\textit{Evaluation}) \cite[vgl.][]{rosenfeld_explainability_2019} und \glqq \textbf{Wie} viele Informationen sollte jede Erklärung enthalten?\grqq{} (\textit{Content}) \cite[vgl.][]{kouki_user_2017}).

\bigskip

Im Folgenden werden die genannten Oberkategorien erläutert bzw. definiert.

\begin{table}
    \begin{tabular}{|p{.2\textwidth}|p{.4\textwidth}|p{.3\textwidth}|}
        \hline
        \textbf{Aspekt} & \textbf{Synonyme}         & \textbf{Quellen} \\ \hline
        1. Context      & (Experimental) Context    & \cite{chazette_knowledge_nodate} \cite{chazette_end-users_nodate}
                                                    \cite{sato_context_nodate} \cite{waa_evaluating_2021} 
                                                    \cite{kohl_explainability_2019} \cite{neerincx_using_2018} 
                                                    \cite{sovrano_modelling_2020} \cite{doshi2017towards} \\
                        & (Explanation) Scope       & \cite{wohlin2012experimentation} \cite{eiband_impact_2019}
                                                    \cite{doshi2017towards} \\
                        & Use Case                  & \cite{waa_evaluating_2021} \\
                        & Stakeholder               & \cite{rosenfeld_explainability_2019} \\
        \hline
        2. Objectives   & Objectives                & \cite{nunes_systematic_2017} \\
                        & Construct                 & \cite{waa_evaluating_2021} \\
                        & Purpose                   & \cite{nunes_systematic_2017} \cite{wohlin2012experimentation} \\
                        & (Stakeholder) Goals       & \cite{cirqueira_scenario-based_2020}
                                                    \cite{sovrano_modelling_2020} \cite{ribera2019can} \\
                        & Main Drive                & \cite{anjomshoae2019explainable} \\
                        & Intended Effect           & \cite{balog_measuring_2020} \\
        \hline
        3. Demand       & Demand                    & \cite{chazette_knowledge_nodate} \\
        \hline
        4. Content      & User Interface Component(s) & \cite{nunes_systematic_2017}
                                                    \cite{rosenfeld_explainability_2019} \\
                        & Content                   & \cite{ribera2019can} \\
                        & Granularity               & \cite{chazette_knowledge_nodate}
                                                    \cite{kohl_explainability_2019} \\
        \hline
        5. Presentation & Presentation              & \cite{rosenfeld_explainability_2019} \cite{kouki_user_2017} \\
                        & (Explanation) Type        & \cite{ribera2019can} \cite{rosenfeld_explainability_2019} \\
        \hline
        6. Evaluation   & Evaluation                & \cite{kohl_explainability_2019} \cite{doshi2017towards} \\
                        & Measurements              & \cite{waa_evaluating_2021} \cite{balog_measuring_2020} \\
                        & Metrics                   & \cite{nunes_systematic_2017} \cite{anjomshoae2019explainable}
                                                    \cite{chari_explanation_2020} \cite{waa_evaluating_2021}\\
        \hline
    \end{tabular}
\caption{Übergeordnete Aspekte von Erklärungen in der Literatur}
\label{tab:model_explaination_aspects}
\end{table}

\paragraph{Context} Der \textit{Context} einer Erklärung wird durch die Situation gegeben, welche durch die Interaktion eines Nutzers, seiner Aufgabe, dem System und der Umgebung entsteht. (vgl. \cite{chazette_knowledge_nodate, kohl_explainability_2019}).

\paragraph{Objectives} \textit{Objectives} sind die Qualitätsziele, welche für eine Erklärung gelten oder aufgrund derer Erklärungen in ein System integriert werden sollen.

\paragraph{Demand} Der \textit{Demand} für eine Erklärung ist der Bedarf für Erklärungen durch den Nutzer eines Systems. Das heißt der \textit{Demand} beschreibt die Notwendigkeit, zu einemen bestimmten Zeitpunkt für einen Systemteil Erklärungen bereitzustellen. Das beinhaltet auch, ob die Initiative vom Nutzer ausgeht oder das System selbstständig eine Erklärung anzeigt.

\paragraph{Content} Der \textit{Content} einer bei bestehendem Bedarf dem Nutzer bereitgestellten Erklärung ist durch die Informationen und die Informationsdichte definiert.

\paragraph{Presentation} Der Inhalt einer Erklärung kann Nutzern auf verschiedenen Wegen zugänglich gemacht werden. Die \textit{Presentation} einer Erklärung ist die Art und Weise, auf die dem Nutzer die Informationen durch die Erklörung bereitgestellt werden.

\paragraph{Evaluation} Die \textit{Evaluation} ist die Bewertung der Qualität von Erklärungen. Dies enthält die grundsätzlichen verschiedenen Evaluationsmöglihckeiten und- Metriken mit denen die Qualität gemessen werden kann.

\input{contents/05_model_description/21_overview}

\input{contents/05_model_description/22_external_dependencies}

\input{contents/05_model_description/23_characteristics}

\input{contents/05_model_description/24_evaluation}