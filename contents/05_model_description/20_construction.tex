\section{Aspekte von Erklärungen}
\label{sec:model_explanation_aspects}

Literatur, die einen Überblick über Erklärbarkeit im Allgemeinen oder in einem bestimmten Anwendungsfeld gibt, betrachtet in der Regel fünf Aspekte von Erklärbarkeit \cite{rosenfeld_explainability_2019, nunes_systematic_2017,chazette_knowledge_nodate}. Dies sind der Kontext der Erklärung, die Zielsetzung dieser, welche Erklärung angezeigt werden soll und wann diese angezeigt werden soll. In einigen Arbeiten wird bei der angezeigten Erklärung außerdem zwischen dem Inhalt und der Darstellung unterschieden bzw. diese Eigenschaften einzeln untersucht \cite{nunes_systematic_2017,abdulrahman_belief-based_2019}. Außerdem wird in allen hier betrachteten Arbeiten auch die Evaluation von Erklärungen thematisiert (Siehe \autoref{sec:literature_review}).

Für die zuvor genannten Aspekte werden in der Literatur verschiedene Unterkategorien konkret benannt oder Synonyme verwendet. \autoref{tab:model_explaination_aspects} fasst die verwendeten Synonyme aus den Veröffentlichungen, welche diese explizit erwähnt haben, unter der final gewählten Benennung des Aspekts zusammen. Neben den dort aufgezeigten Begriffen haben mehrere Autoren (z.B. \cite{rosenfeld_explainability_2019, chazette2020explainability}) die verschiedenen Aspekte von Erklärbarkeit zusätzlich mit Fragewörtern verknüpft. Die vollständigen Fragen dahinter verweisen allerdings auf verschiedene Unterpunkte, weswegen das vorgestellte Modell auf Fragewörter verzichtet, um Verwechselungen vorzubeugen. (Beispiel: \glqq \textbf{Wie} kann die Erklärung evaluiert werden?\grqq (\textit{Evaluation})\cite[vgl.][]{rosenfeld_explainability_2019} und \glqq \textbf{Wie} viele Informationen sollte jede Erklärung enthalten?\grqq (\textit{Content}) \cite[vgl.][]{kouki_user_2017}). Folgend werden die Aspekte näher erläutert bzw. definiert.

\begin{table}
    \begin{tabular}{|p{.2\textwidth}|p{.4\textwidth}|p{.3\textwidth}|}
        \hline
        \textbf{Aspekt}          & \textbf{Synonyme} & \textbf{Quellen} \\ \hline
        1. Context      & (Experimental) Context & \cite{chazette_knowledge_nodate} \cite{chazette_end-users_nodate} \cite{sato_context_nodate} \cite{waa_evaluating_2021} \cite{kohl_explainability_2019} \cite{neerincx_using_2018} \cite{sovrano_modelling_2020} \cite{doshi2017towards} \\
                        & (Explanation) Scope & \cite{wohlin2012experimentation} \cite{eiband_impact_2019} \cite{doshi2017towards} \\
                        & Use Case & \cite{waa_evaluating_2021} \\
                        & Stakeholder & \cite{rosenfeld_explainability_2019} \\
        \hline
        2. Objectives   & Objectives & \cite{nunes_systematic_2017} \\
                        & Construct & \cite{waa_evaluating_2021} \\
                        & Purpose & \cite{nunes_systematic_2017} \cite{wohlin2012experimentation} \\
                        & (Stakeholder) Goals & \cite{cirqueira_scenario-based_2020} \cite{sovrano_modelling_2020} \cite{ribera2019can} \\
                        & Main Drive & \cite{anjomshoae2019explainable} \\
                        & Intended Effect & \cite{balog_measuring_2020} \\
        \hline
        3. Demand          & Demand            & \cite{chazette_knowledge_nodate} \\
        \hline
        4. Content         & User Interface Component(s) & \cite{nunes_systematic_2017}
                                                        \cite{rosenfeld_explainability_2019} \\
                        & Content               & \cite{ribera2019can} \\
                        & Granularity           & \cite{chazette_knowledge_nodate}
                                                  \cite{kohl_explainability_2019} \\
        \hline
        5. Presentation    & Presentation          & \cite{rosenfeld_explainability_2019,kouki_user_2017} \\
                        & (Explanation) Type    & \cite{ribera2019can} \cite{rosenfeld_explainability_2019} \\
        \hline
        6. Evaluation      & Evaluation            & \cite{kohl_explainability_2019} \cite{doshi2017towards} \\
                        & Measurements          & \cite{waa_evaluating_2021} \cite{balog_measuring_2020} \\
                        & Metrics               & \cite{nunes_systematic_2017} \cite{anjomshoae2019explainable}
                                                  \cite{chari_explanation_2020} \cite{waa_evaluating_2021}\\
        \hline
    \end{tabular}
\caption{Übergeordnete Aspekte von Erklärungen in der Literatur}
\label{tab:model_explaination_aspects}
\end{table}

\paragraph{Context} Der \textit{Context} einer Erklärung wird durch die Situation gegeben, welche durch die Interaktion eines Nutzers, seiner Aufgabe, dem System und der Umgebung entsteht. (vgl. \cite{chazette_knowledge_nodate, kohl_explainability_2019}).

\paragraph{Objectives} Unter \textit{Objectives} werden die Ziele verstanden, welche eine Erklärung spezifisch erreichen soll und solche, aufgrund derer Erklärungen in ein System integriert werden sollen.

\paragraph{Demand} Unter dem Punkt \textit{Demand} muss in der Entwicklung entschieden werden, wann die Nutzer des Systems überhaupt eine Erklärung benötigen. Dies bezieht sich nicht nur auf den Zeitpunkt beziehungsweise den Systemteil, welcher erklärt werden soll, sondern auch ob die Initiative vom Nutzer ausgeht oder das System selbständig eine Erklärung anzeigt.

\paragraph{Content} Wenn der Bedarf einer Erklärung geklärt ist, muss festgelegt werden, welche und viele Informationen dem Nutzer eines erklärbaren Systems angezeigt werden sollen.

\paragraph{Presentation} Der Inhalt einer Erklärung kann Nutzern auf verschiedenen Wegen zugänglich gemacht werden. Welche Möglichkeiten bereits in der Literatur verwendet wurden, ist unter \textit{Presentation} zusammengefasst.

\paragraph{Evaluation} Unter \textit{Evaluation} werden Möglichkeiten zusammengetragen, wie gemessen werden kann, ob die in ein System integrierten Erklärungen das ursprüngliche Ziel erreichen. Die \textit{Evaluation} dient also vor allem der Bewertung der Qualität von Erklärungen.

\input{contents/05_model_description/21_overview}

\input{contents/05_model_description/22_external_dependencies}

\input{contents/05_model_description/23_characteristics}

\input{contents/05_model_description/24_evaluation}