\subsection{Externe Abhängigkeiten}
\label{sec:model_external_dependencies}

Unter \textit{External Dependencies} sind die Aspekte zusammengefasst, die eine Auswirkung auf die Erklärungen in einem System haben. Außerdem können von den hier aufgeführten Punkten Anforderungen abgeleitet und später zusammen mit Metriken Hypothesen aufgestellt werden. Daraus kann dann auch abgeleitet werden, welche Funktionen des Systems einer Erklärung bedürfen \cite{kohl_explainability_2019}. Im folgenden werden die beiden zusammenhängenden Unteraspekte \textit{Context} des Systems und \textit{Objectives} erläutert.

\subsubsection{Context}

Der \textit{Context} einer Erklärung beschreibt die äußeren Einflüsse, die unmittelbar auf das erklärbare System wirken und somit Anforderungen an die Eigenschaften von Erklärungen stellen.

Dies beinhaltet die Aktivität, die der Endbenutzer in einer bestimmten Umgebung durchführt. Aus den Eigenschaften der drei Aspekte (Aktivität, Endbenutzer und Umgebung) leiten sich dabei direkte Einflüsse auf den Bedarf, den Inhalt und die Darstellung einer Erklärung ab. \autoref{tab:impact_of_context_on_explanation} stellt wie bereits zuvor die verwendeten Synonyme für die verschiedenen Facetten des \textit{Context} eines Systems dar. Insbesondere der Begriff Stakeholder wurde in der Literatur verschieden eingesetzt. \citeauthor{cirqueira_scenario-based_2020} nutzen den Begriff für den Nutzer einer Software \cite{cirqueira_scenario-based_2020} während \citeauthor{nunes_systematic_2017} diesen als Oberbegriff für Personengruppen, die ein Interesse an einem System haben, verwenden, den Nutzer allerdings ausschließen \cite{nunes_systematic_2017}. Diese Arbeit verwendet den Begriff, um alle Personengruppen mit Interesse am System inklusive aller Nutzer zu beschreiben (vgl. \cite{schneider2012abenteuer,chazette_knowledge_nodate}). Folgend werden nun die drei Aspekte sowie typische Ausprägungen oder Charakteristiken näher erläutert.

\begin{table}[bht!]
    \begin{tabular}{|p{.2\textwidth}|p{.5\textwidth}|p{.2\textwidth}|}
        \hline
        \textbf{Aspekt} & \textbf{Synonyme} & \textbf{Quellen} \\ \hline
        End User        &  (Targt / End)  User & \cite{chazette2020explainability} \cite{kaptein_personalised_2017} \cite{sokol_one_2020} \cite{wiegand_id_2020} \\
                        & Stakeholder & \cite{chazette_knowledge_nodate} \\
                        & Consumer & \cite{ehsan_human-centered_2020} \\
                        & Explainee & \cite{chazette_knowledge_nodate} \cite{kohl_explainability_2019} \\
                        & Explanation Audience & \cite{sokol_explainability_2020} \\
        \hline
        Task            & Task & \cite{chazette_knowledge_nodate} \cite{sokol_explainability_2020} \cite{gunning2019darpa} \\
                        & Activity & \cite{wohlin2012experimentation} \\
        \hline
        Environment     & Environment & \cite{chazette_knowledge_nodate} \cite{wiegand_id_2020} \cite{wiegand2019drive} \\
                        & Application Area & \cite{sokol_explainability_2020} \cite{wiegand2019drive} \cite{wiegand_id_2020} \\
        \hline
    \end{tabular}
    \caption{Relevante Aspekte des Kontextes eines erklärbaren Systems zu Integration von Erklärungen.}
    \label{tab:impact_of_context_on_explanation}
\end{table}

\paragraph{End User} Der \textit{End User} ist diejenige Person, die mit dem System interagiert und auf welchen somit die Erklärungen zugeschnitten sein müssen. Dieser entspricht in den Definitionen von Erklärbarkeit von \citeauthor{chazette_knowledge_nodate} und \citeauthor{kohl_explainability_2019} dem \textit{Explainee} \cite{chazette_knowledge_nodate,kohl_explainability_2019}.

Generell kann ein System verschiedene Nutzer(-typen) haben, die sich auch in ihrem Bedarf für Erklärungen unterscheiden. Im Folgenden werden die drei am häufigsten erwähnten Eigenschaften vorgestellt (unter anderem in \cite{chazette_knowledge_nodate,tintarev_designing_nodate,yamada_evaluating_2016}).

Sowohl beim generellen technischen Verständnis (\textit{Technical Background}) als auch beim Domänen-Wissen (\textit{Domain Expertise}) \cite{yamada_evaluating_2016} können \textit{End User} verschieden viel Hintergrundwissen vorweisen. Somit können sich in einem System mit unterschiedlichen Eigenschaftzen der  Nutzer auch verschiedene Anforderungen an Erklärungen ergeben.

Der \textit{Cultural Background} fasst darüber hinaus die kulturellen Hintergründe des Nutzers zusammen, die sich zum Beispiel auf die Verwendung von Metaphern in Software bzw. Erklärungen auswirken können \cite{salgado_cultural_2015}.

\paragraph{Task} Der \textit{Task} definiert die Aufgabe(n), welche durch den \textit{End User} mithilfe des Systems durchgeführt werden sollen. Auch hier spielen verschiedene Eigenschaften eine Rolle. Genannt werden in der Literatur beispielsweise die Zeitabhängigkeit (\textit{Time Dependency}), die Komplexität (\textit{Complexity}) und die Dauer der Aufgabe (\textit{Length}). Die drei genannten Ausprägungen werden als wichtige Betrachtungsgegenstände für die Eigenschaften von Erklärungen beschrieben. Folglich sind auch diese Aspekte für die Anforderungserhebung im Kontext der Erklärbarkeit relevant \cite{sokol_explainability_2020}.

\paragraph{Environment} Sehr eng mit der zu erledigenden Aufgabe hängt auch dessen Umgebung zusammen. Das \textit{Environment} ist durch die äußeren Umstände des Systems definiert. Dies beinhaltet den generellen Anwendungsbereich des Systems (\textit{Area of Application}), welcher unter anderem die Kritikalität des Systems definiert. Aber auch die Art Nutzerinteraktion fällt in diesen Bereich (\textit{Interaction Type}) und hat eine Auswirkung auf die Anforderungen an Erklärungen \cite{wiegand_id_2020}. 

\bigskip

Zusammenfassend kann als Zwischenergebnis für \textbf{RQ1} festgehalten werden, dass der \textit{Context} eines Systems einer der zu betrachtenden Rahmenbedingungen mit einem Einfluss auf die Anforderungen an Erklärungen ist.

\subsubsection{Zielsetzung}
\label{subsec:model_objective}

Als zweiter Aspekt neben dem \textit{Context} werden in der Literatur die \textit{Objectives} hinter der Integration von Erklärungen mit einem Einfluss auf die Anforderungen an Erklärungen genannt \cite{rosenfeld_explainability_2019, nunes_systematic_2017}.

\citeauthor{chazette_knowledge_nodate} haben in ihrem Modell für Erklärbarkeit die Qualitätsaspekte zusammengefasst, die mit Erklärbarkeit in einem Zusammenhang stehen und somit auch als \textit{Objectives} für die Integration von Erklärungen gesehen werden können. Zusammen mit weiteren Ergebnissen der Literaturrecherche haben sich vor allem acht Qualitätsaspekte herausgestellt, welche auf die Auswirkungen durch Erklärungen untersucht wurden \cite{nunes_systematic_2017, tintarev2007survey}. Die Qualitätsaspekte sind in den meisten Fällen dafür genutzt worden, über das Messen dieser Aspekte die Qualität der integrierten Erklärungen indirekt zu bestimmten. \autoref{tab:quality_aspects_of_explanation} enthält eine nach der in der Literatur vorkommenden Häufigkeit sortierte Liste zusammen mit den Definitionen der Qualitätsaspekte. Diese enthält alle Arbeiten, welche den jeweiligen Aspekt explizit untersuchen oder Ergebnisse von Untersuchen dazu zusammenfassen.

\begin{table}[htb!]
    \begin{center}
        \begin{tabular}{|p{.24\textwidth}|p{.5\textwidth}|p{.2\textwidth}|}
            \hline
            \textbf{Qualitätsziel}    & \textbf{Beschreibung} & \textbf{Quellen} \\ \hline
            Trust                       & Das Vertrauen des Nutzers in das System erhöhen \cite[vgl.][]{balog_measuring_2020}
                                        & \cite{nunes_systematic_2017} \cite{chazette_knowledge_nodate} \cite{tintarev_designing_nodate} \cite{balog_measuring_2020} \cite{eiband_impact_2019} \cite{tintarev2015explaining} \cite{hernandez-bocanegra_effects_2020} \cite{stange_effects_2021} \cite{weitz_you_2019} \cite{yamada_evaluating_2016} \cite{haspiel_explanations_2018} \cite{martin_developing_2019} \cite{martin_evaluating_2021} \cite{tsai_effects_2020}  \cite{sokol_one_2020}  \cite{wang_is_2018} \cite{koo_understanding_2016} \cite{wiegand2019drive} \cite{gunning2019darpa} \cite{lim_2009_assessing} \cite{tintarev2007survey} \cite{kunkel_let_2019} \\ \hline
            Satisfaction                & Die Benutzerfreundlichkeit und generelle Zufriedenheit von Nutzern mit dem System erhöhen. \cite[vgl.][]{balog_measuring_2020}
                                        & \cite{nunes_systematic_2017} \cite{chazette_knowledge_nodate} \cite{tintarev_designing_nodate} \cite{balog_measuring_2020} \cite{tsai_evaluating_2019} \cite{tintarev2015explaining} \cite{riveiro_thats_2021} \cite{martin_developing_2019} \cite{martin_evaluating_2021} \cite{tsai_effects_2020} \cite{ehsan_human-centered_2020} \cite{sovrano_modelling_2020} \cite{koo_understanding_2016} \cite{ribera2019can} \cite{gunning2019darpa} \cite{lim_2009_assessing}  \cite{tintarev2007survey} \cite{sato_context_nodate} \\ \hline
            Transparency                & Erklären, wie das System funktioniert. \cite[vgl.][]{balog_measuring_2020}
                                        & \cite{nunes_systematic_2017} \cite{chazette_knowledge_nodate} \cite{tintarev_designing_nodate} \cite{chazette_end-users_nodate} \cite{balog_measuring_2020} \cite{chazette2020explainability} \cite{tintarev2015explaining} \cite{hernandez-bocanegra_effects_2020} \cite{tsai_effects_2020} \cite{rjoob_towards_2021}  \cite{sokol_one_2020} \cite{wang_is_2018} \cite{koo_understanding_2016} \cite{tintarev2007survey}\\ \hline
            Understandability           & Das Verständnis von Nutzern über das System erhöhen \cite[vgl.][]{chazette_end-users_nodate}
                                        & \cite{chazette_knowledge_nodate} \cite{chazette_end-users_nodate} \cite{martin_evaluating_2021}  \cite{ehsan_human-centered_2020} \cite{rjoob_towards_2021}  \cite{sokol_one_2020} \cite{cheng2019explaining} \\ \hline
            Scrutability                & Nutzern die Möglichkeit geben, dem System einen Fehler mitzuteilen \cite[vgl.][]{balog_measuring_2020}
                                        & \cite{nunes_systematic_2017} \cite{chazette_knowledge_nodate} \cite{tintarev_designing_nodate} \cite{balog_measuring_2020} \cite{tintarev2015explaining} \cite{martin_developing_2019} \cite{gunning2019darpa}  \cite{tintarev2007survey} \cite{martin_evaluating_2021} \\ \hline
            Efficiency                  & Nutzern helfen ihre Aufagaben schneller zu erledigen \cite[vgl.][]{balog_measuring_2020} 
                                        & \cite{nunes_systematic_2017} \cite{chazette_knowledge_nodate} \cite{tintarev_designing_nodate} \cite{balog_measuring_2020} \cite{tsai_evaluating_2019} \cite{tintarev2015explaining} \cite{hernandez-bocanegra_effects_2020} \cite{tintarev2007survey}\\ \hline
            Effectiveness               & Die Qualität der Aufgaben von Nutzern erhöhen \cite[vgl.][]{balog_measuring_2020}
                                        & \cite{nunes_systematic_2017} \cite{chazette_knowledge_nodate} \cite{tintarev_designing_nodate} \cite{balog_measuring_2020} \cite{tintarev2015explaining} \cite{zolotas_towards_2019} \cite{hernandez-bocanegra_effects_2020} \cite{martin_evaluating_2021} \cite{rjoob_towards_2021} \cite{tintarev2007survey} \\ \hline
            Persuasiveness              & Die akzeptanz der Entscheidungen des Systems durch die Nutzer erhöhen \cite[vgl.][]{chazette_knowledge_nodate}
                                        & \cite{nunes_systematic_2017} \cite{tintarev_designing_nodate} \cite{balog_measuring_2020} \cite{sato_context_nodate} \cite{sato_context_nodate} \cite{abdulrahman_belief-based_2019} \cite{tintarev2015explaining} \cite{sato_action-triggering_2019} \cite{tintarev2007survey} \\ \hline
        \end{tabular}
    \end{center}
    \caption{Qualitätsziele für Erklärungen – sortiert nach der Anzahl der Veröffentlichungen in der Literaturrecherche, die den zugehörigen Qualitätsaspekt untersucht haben.}
    \label{tab:quality_aspects_of_explanation}
\end{table}

\begin{table}[htb!]
    \begin{center}
        \begin{tabular}{|p{.2\textwidth}|p{.5\textwidth}|p{.2\textwidth}|}
            \hline
            \textbf{Aspekt}     & \textbf{Synonym} & \textbf{Quellen} \\ \hline
            Business Goals      & Stakeholder Goals & \cite{nunes_systematic_2017} \\
                                & (Intended) Purpose & \cite{waa_evaluating_2021} \\
                                & Higher-level Goals & \cite{nunes_systematic_2017} \\
                                & Application Level & \cite{sokol_explainability_2020} \\
            \hline
            Users'              & User Perceived Quality Factors & \cite{nunes_systematic_2017} \\
            Perception          & (Consumer) Needs & \cite{ehsan_human-centered_2020} \cite{chazette_end-users_nodate} \\
                                & User Goals & \cite{ehsan_human-centered_2020} \\
                                & Intermediate Requirements & \cite{waa_evaluating_2021} \\
                                & Human Level & \cite{sokol_explainability_2020} \\
            \hline
            Explanation         & (Explanation) Purpose & \cite{nunes_systematic_2017} \\
            Purpose             & Explanatory Goal & \cite{tintarev_designing_nodate} \cite{balog_measuring_2020} \\
                                & Function Level & \cite{sokol_explainability_2020} \\
            \hline
        \end{tabular}
    \end{center}
    \caption{Abstraktionsebenen der Zielsetzung bei der Integration von Erklärungen in ein System.}
    \label{tab:impact_of_objective_on_explanation}
\end{table}

In ihrer Definition von Erklärbarkeit sehen \citeauthor{chazette_knowledge_nodate} vor allem \textit{Transparency} und \textit{Understandability} als zentrale Ziele von Erklärbarkeit \cite{chazette_end-users_nodate}. Sie schreiben, dass diese unmittelbar durch die Integration von Erklärungen erreicht werden können. Wie in \autoref{tab:quality_aspects_of_explanation} zu sehen ist, werden Untersuchungen aber häufiger zu anderen Aspekten durchgeführt. Dies ist ein Indiz dafür, dass diese Aspekte nicht in einer flachen Hierarchie zueinander stehen. Eine hierarchische Anordnung der Qualitätsziele findest sich bei auch bei weiteren Autoren \cite{nunes_systematic_2017,tintarev2007survey}. Folglich werden die \textit{Objectives} in drei Abstraktionsebenen gegliedert: \textit{Business Goals}, \textit{Users' Perception} und \textit{Explanation Purpose}. Diese Ebenen, die von \citeauthor{nunes_systematic_2017} sowie \citeauthor{tintarev2007survey} vorgeschlagen werden, sind in diesem Modell als konkrete Abstraktionslevel für Qualitätsmodelle (vgl. \cite{schneider2012abenteuer}) zu interpretieren. Die Zuordnung der Qualitätsaspekte zu den Kategorien ist in der Gesamtübersicht des Modells abgebildet (\autoref{fig:model_overview_complete}). \autoref{tab:impact_of_objective_on_explanation} fasst die Erwähnungen der drei Zielebenen in der Literatur mit den jeweiligen Synonymen zusammen.

\paragraph{Business Goals} \textit{Business Goals} sind die Ziele, für das System im Ganzen gelten. \citeauthor{schneider2012abenteuer} nennt sie im Kontext von Qualitätsmodellen \glqq Allgemeine Qualitätsziele\grqq \cite{schneider2012abenteuer}.

\paragraph{User Perception Goals} \textit{User Perception Goals} sind jene Ziele, die direkt durch den \textit{End User} des Sytems wahrgenommen werden sollen. Die trägt der Erreichung der allgemeinen Ziele auf der höheren Ebene bei. Diese Ziele sind als Zwischenziele hin zu einem konkreten Ziel für zu integrierende Erklärungen zu verstehen.

\paragraph{Explanation Goals} Die \textit{Explanation Goals} sind die \glqq konkreten Qualitätsziele\grqq{} im Rahmen von Erklärbarkeit (vgl. \cite{schneider2012abenteuer}). Diese sind allerdings noch nicht konkrete Ziele für eine bestimmte Situation, in der die Erklärungen eingesetzt werden sollen. Hierfür müssen diese weiter verfeinert werden bis zu ihrer Messbarkeit, um daraus Anforderungen zu entwickeln (Siehe \autoref{sec:model_evaluation_description}).

\bigskip

Die hier vorgestellten Ziele bei der Integration von Erklärungen stellen keine abgeschlossene oder vollständige Liste dar, sondern dienen im Rahmen des Leitfadens als Überblick über in der Literatur häufig betrachtete Ziele, für deren Erreichung Erklärungen erfolgreich eingesetzt wurden.

\smallskip

\noindent\fbox{
    \parbox{0.964\textwidth}{
        \smallskip
        \textbf{RQ1} Welche Rahmenbedingungen haben einen Einfluss auf die Anforderungen für Erklärungen?
        \smallskip
    }
}

\smallskip

Mit dem \textit{Context} des Systems und den \textit{Objectives} für die Integration von Erklärungen wurden im Rahmen des ersten Modellteils die Rahmenbedingungen, welche einen direkten Einfluss auf Anforderungen an Erklärungen haben, vorgestellt. Die unter \textit{External dependencies} zusammengefassten Aspekte und Ausprägungen sind folglich die Antwort auf die erste Forschungsfrage.

Als Hilfsmittel kann dieser erste Teil des Modells vor allem bei der Anforderungserhebung (\textit{Requirements Elicitation}) und -Analyse (Requirements Analysis) helfen \cite{schneider2012abenteuer}. Auf dieser Basis können Anforderungen formuliert und die Grundlagen für Hypothesen gelegt werden. Folglich wird durch diesen ersten Modellteil auch die erste Modellanforderung erfüllt ([MR1]).

\smallskip

Im folgenden Abschnitt werden die Eigenschaften von Erklärungen vorgestellt, welche in der Literatur bereits verwendet wurden, um die Anforderungen umzusetzen.
