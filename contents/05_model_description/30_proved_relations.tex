\section{Abhängigkeiten}
\label{sec:model_proved_relations}

Zusätzlich zum Modell über die in der Literatur bereits betrachteten Aspekte von Erklärungen sind im Folgenden die Ergebnisse der Auswirkungen dieser Aspekte auf die externe Qualität von Erklärungen zusammengefasst. Enthalten sind dabei jene Resultate, die entweder bereits bewiesen sind oder die Autoren eine starke Vermutung für eine These haben, die allerdings noch einer Überprüfung bedarf. Diese sind dementsprechend gekennzeichnet. Auch sind nicht alle Ergebnisse vorangegangener Arbeiten Anwendungs-unabhängig gezeigt worden. Somit können einige Ergebnisse nur mit Vorsicht auf andere Kontexte übertragen werden. Daher sind einige Empfehlungen mit Einschränkungen verbunden. Außerdem werden ziehen die Ergebnisse in der Regel Vergleiche zu einem alternativen Erklärungstypen oder keiner Erklärung. Daher werden die hier aufgezeigten Einflüsse immer in der Regel als Vergleich zu einer Alternative dargestellt.

Generell ist einerseits das Ziel dieser Zusammenfassung von Abhängigkeiten, den Anwendern dieses Leitfadens eine Unterstützung bei der Ableitung von konkreten Anforderungen an Erklärungen aus den \textit{Explanation Purposes} zu geben. Darüber hinaus soll dieser Überblick über zuvor geleistete Ergebnisse aber auch bei der Umsetzung der Anforderungen im System unterstützen. Folglich dient dieser Teil des Leitfadens der Beantwortung der Forschungsfragen \textbf{RQ4.1} und \textbf{RQ4.2}, welche explizit die Zusammenhänge zwischen verschiedenen Aspekten des Modells abdecken und gibt damit zusätzliche Informationen zu den Einflüssen, die einzelne Eigenschaften von Erklärungen unter bestimmten Rahmenbedingungen auf externe Qualitätsaspekte haben. Je nach \textit{Context} können dabei auch Konflikte zwischen den externen Qualitätsaspekten entstehen, auf die Erklärbarkeit Auswirkungen hat. Die Erkennung dieser soll durch den Katalog der Zusammenhänge auch unterstützt werden.

Wie auch schon das Modell der Aspekte von Erklärungen hat auch dieser Katalog mit Abhängigkeiten keinen Anspruch auf Vollständigkeit, sondern soll einen guten Überblick über bestehende Ergebnisse schaffen, die sich auf viele Anwendungsfälle übertragen lassen.

Die Formulierung der Abhängigkeiten orientiert sich an einer in \citetitle*{international2011iso} definierten Formulierungsvorlage \cite{international2011iso}. Genutzt wird diese beispielsweise auch von \citeauthor{carvalho2020developers} bei der generellen Untersuchung verschiedener NFRs \cite{carvalho2020developers}. Das Konzept beschreibt, dass einzelne Eigenschaften von Softwaresystemen einen positiven, negativen oder neutralen Effekt auf Qualitätsaspekte haben können. Bedarf ein Zusammenhang einer Bedingung, so wird diese nachgestellt.

Gegliedert ist der Katalog der Abhängigkeiten in zwei Abschnitte. Ersterer behandelt die Auswirkungen, die die Rahmenbedingungen auf den \textit{Demand} für Erklärungen haben. Im zweiten Teil werden die Einflüsse der Rahmenbedingungen eines Systems auf die Granularität von Erklärungen, also den \textit{Content} und die \textit{Presentation} vorgestellt. Außerdem sind die verschiedenen Einflüsse in positive, negative und neutrale Einflüsse gegliedert.

\newpage

\subsection*{Demand}

Im Folgenden sind die Einflüsse aufgeführt, die sich darauf beziehen, ob und wann Erklärungen benötigt werden.

\subsubsection*{Positive Zusammenh}

\begin{itemize}
    \item Erklärungen erhöhen im Allgemeinen die \textit{Perceived Transparency}, \textit{wenn End User geringes Vertrauen in Technologie haben} \cite{tsai_effects_2020}. 
    \item Erklärungen erhöhen im Allgemeinen die \textit{Perceived Transparency}, \textit{wenn End User Datenschutzbedenken haben} \cite{tsai_effects_2020}.
    \item Das proaktive präsentieren von Erklärungen (\textit{System-Initiative}) wirkt sich positiv auf die \textit{Usability} eines Systems aus, \textit{wenn der Inhalt der Erklärung gering ist} \cite{???}.
    \item Das Anzeigen von Erklärungen in \textit{End Usern} unbekannten Situationen hat einen positiven Einfluss auf \textit{Trust} \cite{haspiel_explanations_2018}.
    \item Das Präsentieren einer Erklärung vor einer unbekannten Situation (\textit{before}) hat einen positiveren Einfluss auf \textit{Trust}, als Präsentation danach \cite{haspiel_explanations_2018}.
    \item Das proaktive präsentieren von Erklärungen (\textit{System-Initiative}) in unbekannten Situationen hat einen positiven Einfluss auf \textit{Trust} \cite{zhu_effects_2020}.
    \item Das Präsentieren von Erklärungen für sicherheitsrelevante Systemfunktionen hat einen positiven Einfluss auf \textit{Trust}  \cite{wiegand2019drive}.
    \item Das Präsentieren von Erklärungen für sicherheitsrelevante Systemfunktionen hat einen positiven Einfluss auf \textit{Satisfaction}  \cite{wiegand2019drive}.
    \item Die Möglichkeit der Anforderung von Erklärungen im Allgemeinen \textit{User-initiative} hat einen positiven Einfluss auf \textit{Satisfaction} \cite{chazette_end-users_nodate}.
\end{itemize}

\subsubsection*{Negative Zusammenhänge}

\begin{itemize}
    \item Das Präsentieren von Erklärungen kann einen negativen Einfluss auf die \textit{Effectivity} von \textit{End Usern} haben, \textit{wenn diese sehr selbstbewusst sind} \cite{schaffer_i_2019}.
    \item Das Präsentieren von Erklärungen kann einen negativen Einfluss auf die \textit{Satisfaction} haben, \textit{wenn End User hohen Trust in das System haben} \cite{rosenfeld_explainability_2019, doshi2017towards}.
\end{itemize}

\subsubsection*{Weitere Zusammenhänge}

\begin{itemize}
    \item Der Zusammenhang zwischen dem Grad der Systembeeinflussung durch \textit{End User} und dem Erklärungsbedarf ist antiproportional. Umso mehr Beeinflussungsmöglichkeiten \textit{End User} in einem System haben, umso weniger Erklärungsbedarf haben diese \cite{rosenfeld_explainability_2019}.
\end{itemize}

\newpage

\subsection*{Granularität}

Wie der \textit{Content} und die \textit{Presentation} durch Rahmenbedingungen wie dem \textit{Context} und den \textit{Objectives} für Erklärungen beeinflusst werden, ist im Folgenden dargestellt.

\subsubsection*{Positiv}

Eine Erklärung, die darstellt, warum eine Alternative nicht genommen wurde, wird besser verstanden als eine, welche die aktuelle Entscheidung erklärt. \cite{schrills_color_2020}

Eine Erklärung die den Kontext des Systemverhaltens wiederspiegelt wirkt sich positiv auf die Persuasiveness und die Usefullness einer Erklärung aus. Bewiesen ist dies nur für Recommender Systeme \cite{sato_action-triggering_2019, abdulrahman_belief-based_2019}

Hybride Stile (Typ + Inhalt) haben den größten positiven Einfluss auf die Usability und Persuasiveness eines Recommender Systems im Gegensatz zu einzeln ausgewählten Stilen. \cite{sato_action-triggering_2019, kunkel_let_2019, sato_action-triggering_2019, schrills_color_2020, lim_2009_assessing}

Das Präsentieren von Erklärungen durch Agenten hat einen positiven Einfluss auf Trust in intelligenten System \cite{weitz_you_2019}.

Das Geben von Kontextinformationen beeinflusst Usefullness und Persuasivness am positivsten im Vergleich zu keinen oder Inhaltsbasierten erklärungen \cite{sato_action-triggering_2019}

Non-experts prefer belief-based explanations and experts perefer goal-based \cite{kaptein_personalised_2017}, hypotthesis (The letter is proved by \cite{martin_evaluating_2021})

Counterfactual (Why not) ist better bei Alternativen \cite{martin_evaluating_2021, schrills_color_2020}  \cite{neerincx_using_2018} \cite{schrills_color_2020} (Small significant effect), \cite{lim_2009_assessing} sagt, dass das nicht trivial ist.

Personalisierte Erklärungen erhöhen die Transparenz \cite{sokol_one_2020, wiegand2019drive}

Wenn der User mit dem Output des Systems übereinstimmt, dann erhöht dies den Trust in das System. \cite{schrills_color_2020}

Einfache Erklärungen führen zu einer höheren Akzeptanz der Erklärungen und zu höherer Nutzerzufrienenheit \cite{hleg2019policy, sovrano_modelling_2020}

Interactive Erklären erhöhen das Verständnis \cite{cheng2019explaining}

Context und Causality sind die am meisten angefragten Informationen \cite{chazette_end-users_nodate} -> Satisfaction


\subsubsection*{Negativ}

(2) Rational explanations are only effective on users that report being very unfamiliar with a task – regardless of their actual competency level. \cite{schaffer_i_2019}

Wenn User nicht mit der Ausgabe des Systems übereinstimmen, dann 

lack of completeness \cite{chazette_end-users_nodate} -> Kann durch interactivity umgangen werden

Wenn ein Assistent, der für Erklärungen genutzt wird zu lächerlich gestaltet ist, kann dies zu einem vertrauensverlust führen. \cite{wang_is_2018}

Behaviour information might have a negativ effect on satisfaction due to redundance with the system behaviour it self und kann sogar zu schlechterer Nutzer Performance führen. \cite{koo_why_2015}

Recommender Systems Repetetive Erklärungen \glqq langweilig\grqq{}

Wenn der Nutzer durch eine Erklärung zu sehr davon Überzeugt ist, das System verstanden haben, kann dies einen negativen Impact auf die Effectivity haben, da er Fehler des Systems nicht erkennt. (Cognitive bias) \cite{kohl_explainability_2019}

why information is good for transparency \cite{chazette2020explainability}

\subsubsection*{Neutral}

Die Anzahl der Paper bestätigt das Ergebnis von \cite{chazette_end-users_nodate}, dass die Reihenfolge der Häufigkeiten mit der die Inhalte untersucht wurden so ist. (How ist unnötig.)

Die Richtigkeit einer Erklärung hat keinen Einfluss auf das Vertrauen des Endnutzers in das System, wenn das Verhalten des Systems mit dem Mentalen Modells des Nutzers übereinstimmt. \cite{eiband_impact_2019, riveiro_thats_2021}

Es gibt keinen Zusammen zwischen dem Ziel mit dem die Erklärung verfasst wurde und dem Effekt, der sich auf die verschiedenen Ziele messen lässt, bei einer Formulierung durch den Menschen. \cite{balog_measuring_2020}.

Bei der Inhaltlichen Abfrage steht \cite{zahedi_towards_2019} gegen chazeete (Genaues paper suchen). Folglich scheint der Inhaltstyp keinen großen Einfluss auf die Erklärung zu haben. Wenn allerdings die 

Transparency might not increase the Trust solely. The explanation has to fit it's goal \cite{wiegand2019drive}

User haben höhere Anfroderungen and why erklärungen, weswegen diese öfter als nicht hilfreich gekennzwichnet werden als How-Fragen. \cite{lim_2009_assessing}

There ist no prove up to now that interactions increase trust \cite{cheng2019explaining}
-------------------------------------------------------------------

Scrutability and Trust are related: Zu viel Trust -> Der User erkennt nicht mehr, wenn das System falsch ist. \cite{gunning2019darpa}

Für die verschiedenen Systemkontexte gibt es weitere Zusammenhänge, die eine Rolle spielen. Als Beispiele wäre hier zum Beispiel zu nennen, dass User Erklärungen im AI-Kontext... Im Recommender System Kontext wäre das...

Da das Ziel dieser Arbeit allerdings ein allgemeiner Überblick über Erklärbarkeit mit seinen Zusammenhängen und Eigenschaften von Erklärungen geben soll, werden diese an dieser Stelle ausgeklammert.

\cite{martin_evaluating_2021}: For engineers, it is about whether the explanation follows their reasoning, while desk-based agents are more concerned with whether it supports their work.

Eine Übersicht mit Arbeiten vor 2015, die im Kontext von Empfehlungssystemen einen Effekte auf eines der in \autoref{tab:quality_aspects_of_explanation} aufgelisteten Ziele hat, kann in der Arbeit von \citeauthor{nunes_systematic_2017} gefunden werden \cite{nunes_systematic_2017}.

Usability beschreibt die Qualität einer Erklärung im Bezug auf die Interaktion und die Darstellung der Inhalte \cite{chazette_end-users_nodate}. Informativeness ist dierekt bezogen auf den Inhalt der Erklärung \cite{chazette_end-users_nodate}. Direkt messbar an der Erklärung selbst.