\section{Abhängigkeiten}
\label{sec:model_proved_relations}

\cite{international2011iso} definiert ein Konzept, um Zusammenhänge zwischen Charakteristiken eines Systems und Qualitätsaspekten zu definieren. Anwendung findet dies unter Anderem bei \cite{carvalho2020developers}. Das Konzept beschreibt, dass einzelne Eigenschaften von Softwaresystemen einen positiven (\textcolor{green}{helps}), negativen (\textcolor{red}{hurts}) oder neutralen Effekt auf Qualitätsaspekte haben kann. Diese können entweder generell gelten oder unter bestimmten Bedingungen (Kontexten) zutreffen.

\subsection*{Demand}

\subsubsection*{Positiv}

Eine Erklärung die den Kontext des Systemverhaltens wiederspiegelt wirkt sich positiv auf die Persuasiveness und die Usefullness einer Erklärung aus. Bewiesen ist dies nur für Recommender Systeme \cite{sato_action-triggering_2019, abdulrahman_belief-based_2019}

Eine Erklärung, die darstellt, warum eine Alternative nicht genommen wurde, wird besser verstanden als eine, welche die aktuelle Entscheidung erklärt. \cite{schrills_color_2020}

(2) Rational explanations are only effective on users that report being very unfamiliar with a task – regardless of their actual competency level. \cite{schaffer_i_2019}

Das proaktive Präsentieren von Erklärungen, wenn diese einen geringen Inhalt haben wirkt sich positiv auf die Usability aus.

Das Anzeigen von Erklärungen vor einem Event hat einen positiveren Einfluss auf Trust als das Anzeigen danach. \cite{haspiel_explanations_2018}

Proaktive Erklärungen in unvorhersehbaren Situation, erhöhen das Vertrauen \cite{zhu_effects_2020}

Das anzeigen von Erklärungen bei sicherheitsrelevanten Systemfunktionen hat einen positiven Einfluss auf satisfaction und trust. \cite{wiegand2019drive}

\subsubsection*{Negativ}

(3) There is a danger in showing explanations to self-confident users in that situation awareness might be negatively impacted – this can be mitigated by requiring interaction with an agent. \cite{schaffer_i_2019}

\subsubsection*{Beides}

Der Zusammenhang zwischen dem Grad der Systembeeinflussung durch den Nutzer und dem Erklärungsbedarf ist antiproportional. Umso mehr mehr Kontrolle der Nutzer über ein System erhält, umso weniger Erklärungsbedarf hat dieser \cite{rosenfeld_explainability_2019}.

Wenn der Nutzer generell ein hohes Vertrauen in ein System hat, dann benötigt dieser keine Erklärungen, um dieses herzustellen. \cite{rosenfeld_explainability_2019, doshi2017towards}

Die User Satisfaction kann leiden, wenn die Nutzer das System bereits gut kennen, wenn eine Erklärung angezeigt wird.

\subsection*{Granularität}

\subsubsection*{Positiv}

Hybride Stile (Typ + Inhalt) haben den größten positiven Einfluss auf die Usability und Persuasiveness eines Recommender Systems im Gegensatz zu einzeln ausgewählten Stilen. \cite{sato_action-triggering_2019, kunkel_let_2019, sato_action-triggering_2019, schrills_color_2020, lim_2009_assessing}

Das Präsentieren von Erklärungen durch Agenten hat einen positiven Einfluss auf Trust in intelligenten System \cite{weitz_you_2019}.

Das Geben von Kontextinformationen beeinflusst Usefullness und Persuasivness am positivsten im Vergleich zu keinen oder Inhaltsbasierten erklärungen \cite{sato_action-triggering_2019}

Non-experts prefer belief-based explanations and experts perefer goal-based \cite{kaptein_personalised_2017}, hypotthesis (The letter is proved by \cite{martin_evaluating_2021})

Counterfactual (Why not) ist better bei Alternativen \cite{martin_evaluating_2021}  \cite{neerincx_using_2018} \cite{schrills_color_2020} (Small significant effect), \cite{lim_2009_assessing} sagt, dass das nicht trivial ist.

Bessere Transparenz durch erklärungen, wenn der Nutzer generell ein geringes Vertrauen in Technologie oder höhere Privacy concerns im Allgemeinen hat. \cite{tsai_effects_2020}

Personalisierte Erklärungen erhöhen die Transparenz \cite{sokol_one_2020, wiegand2019drive}

Wenn der User mit dem Output des Systems übereinstimmt, dann erhöht dies den Trust in das System. \cite{schrills_color_2020}

Einfache Erklärungen führen zu einer höheren Akzeptanz der Erklärungen und zu höherer Nutzerzufrienenheit \cite{hleg2019policy, sovrano_modelling_2020}

Interactive Erklären erhöhen das Verständnis \cite{cheng2019explaining}

\subsubsection*{Negativ}

Wenn User nicht mit der Ausgabe des Systems übereinstimmen, dann 

lack of completeness \cite{chazette_end-users_nodate} -> Kann durch interactivity umgangen werden

Wenn ein Assistent, der für Erklärungen genutzt wird zu lächerlich gestaltet ist, kann dies zu einem vertrauensverlust führen. \cite{wang_is_2018}

Behaviour information might have a negativ effect on satisfaction due to redundance with the system behaviour it self und kann sogar zu schlechterer Nutzer Performance führen. \cite{koo_why_2015}

Recommender Systems Repetetive Erklärungen \glqq langweilig\grqq{}

\subsubsection*{Neutral}

Die Richtigkeit einer Erklärung hat keinen Einfluss auf das Vertrauen des Endnutzers in das System, wenn das Verhalten des Systems mit dem Mentalen Modells des Nutzers übereinstimmt. \cite{eiband_impact_2019, riveiro_thats_2021}

Es gibt keinen Zusammen zwischen dem Ziel mit dem die Erklärung verfasst wurde und dem Effekt, der sich auf die verschiedenen Ziele messen lässt, bei einer Formulierung durch den Menschen. \cite{balog_measuring_2020}.

Bei der Inhaltlichen Abfrage steht \cite{zahedi_towards_2019} gegen chazeete (Genaues paper suchen). Folglich scheint der Inhaltstyp keinen großen Einfluss auf die Erklärung zu haben. Wenn allerdings die 

Transparency might not increase the Trust solely. The explanation has to fit it's goal \cite{wiegand2019drive}

User haben höhere Anfroderungen and why erklärungen, weswegen diese öfter als nicht hilfreich gekennzwichnet werden als How-Fragen. \cite{lim_2009_assessing}

There ist no prove up to now that interactions increase trust \cite{cheng2019explaining}
-------------------------------------------------------------------

Scrutability and Trust are related: Zu viel Trust -> Der User erkennt nicht mehr, wenn das System falsch ist. \cite{gunning2019darpa}

Für die verschiedenen Systemkontexte gibt es weitere Zusammenhänge, die eine Rolle spielen. Als Beispiele wäre hier zum Beispiel zu nennen, dass User Erklärungen im AI-Kontext... Im Recommender System Kontext wäre das...

Da das Ziel dieser Arbeit allerdings ein allgemeiner Überblick über Erklärbarkeit mit seinen Zusammenhängen und Eigenschaften von Erklärungen geben soll, werden diese an dieser Stelle ausgeklammert.

\cite{martin_evaluating_2021}: For engineers, it is about whether the explanation follows their reasoning, while desk-based agents are more concerned with whether it supports their work.